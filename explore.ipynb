{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export CUDA_VISIBLE_DEVICES=1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lixy/anaconda3/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-24 11:21:58,740\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "from baichuan_generation_utils import build_chat_input\n",
    "from vllm_model import Baichuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"pre_train_model/Baichuan2-7B-Chat\"\n",
    "llm = Baichuan(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-24 11:22:01 config.py:1354] Casting torch.bfloat16 to torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-24 11:22:03 config.py:698] Defaulting to use mp for distributed inference\n",
      "INFO 11-24 11:22:03 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='pre_train_model/Baichuan2-7B-Chat', speculative_config=None, tokenizer='pre_train_model/Baichuan2-7B-Chat', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=pre_train_model/Baichuan2-7B-Chat, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "WARNING 11-24 11:22:03 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 11-24 11:22:04 selector.py:153] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-24 11:22:04 selector.py:53] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:04 selector.py:153] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:04 selector.py:53] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:05 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:05 utils.py:741] Found nccl from library libnccl.so.2\n",
      "INFO 11-24 11:22:05 utils.py:741] Found nccl from library libnccl.so.2\n",
      "INFO 11-24 11:22:05 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:05 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-24 11:22:05 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /home/lixy/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:05 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /home/lixy/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 11-24 11:22:06 selector.py:153] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-24 11:22:06 selector.py:53] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:06 selector.py:153] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:06 selector.py:53] Using XFormers backend.\n",
      "INFO 11-24 11:22:19 model_runner.py:255] Loading model weights took 7.0249 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:19 model_runner.py:255] Loading model weights took 7.0249 GB\n",
      "INFO 11-24 11:22:21 distributed_gpu_executor.py:56] # GPU blocks: 4316, # CPU blocks: 1024\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:23 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:23 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-24 11:22:23 model_runner.py:924] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-24 11:22:23 model_runner.py:928] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-24 11:22:37 custom_all_reduce.py:219] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:37 model_runner.py:1117] Graph capturing finished in 14 secs.\n",
      "INFO 11-24 11:22:37 custom_all_reduce.py:219] Registering 2275 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=1725146)\u001b[0;0m INFO 11-24 11:22:37 model_runner.py:1117] Graph capturing finished in 14 secs.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "                        model_path,\n",
    "                        trust_remote_code=True\n",
    "                    )\n",
    "model = LLM(model=model_path,\n",
    "                            tokenizer=model_path,\n",
    "                            tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n",
    "                            trust_remote_code=True,\n",
    "                            gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n",
    "                            dtype='half')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LLM' object has no attribute 'generation_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m      3\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m         {\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: q},\n\u001b[1;32m      9\u001b[0m     ]\n\u001b[0;32m---> 10\u001b[0m     chat, _ \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_chat_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     tokenized_chat \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     batch_text\u001b[38;5;241m.\u001b[39mappend(tokenized_chat)\n",
      "File \u001b[0;32m~/RAG/baichuan_generation_utils.py:20\u001b[0m, in \u001b[0;36mbuild_chat_input\u001b[0;34m(model, tokenizer, messages, max_new_tokens)\u001b[0m\n\u001b[1;32m     18\u001b[0m         rounds\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mround\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m system, rounds\n\u001b[0;32m---> 20\u001b[0m max_new_tokens \u001b[38;5;241m=\u001b[39m max_new_tokens \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241m.\u001b[39mmax_new_tokens\n\u001b[1;32m     21\u001b[0m max_input_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_max_length \u001b[38;5;241m-\u001b[39m max_new_tokens\n\u001b[1;32m     22\u001b[0m system, rounds \u001b[38;5;241m=\u001b[39m _parse_messages(messages, split_role\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LLM' object has no attribute 'generation_config'"
     ]
    }
   ],
   "source": [
    "batch_text = []\n",
    "for q in prompts:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": q},\n",
    "    ]\n",
    "    chat, _ = build_chat_input(model, tokenizer, messages)\n",
    "    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    batch_text.append(tokenized_chat)\n",
    "# batch_input = torch.cat(batch_text, dim=0)\n",
    "outputs = model.generate(batch_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RequestOutput(request_id=1, prompt='[gMASK]sop<|system|>\\n You are a helpful assistant.<|user|>\\n 吉利汽车座椅按摩<|assistant|>', prompt_token_ids=[64790, 64792, 64790, 64792, 906, 31007, 13361, 31007, 30994, 13, 809, 383, 260, 6483, 9319, 30930, 64795, 30910, 13, 30910, 40457, 32031, 38555, 37972, 64796], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' \\n 吉利汽车座椅按摩功能可以带给您更加舒适的乘坐体验。', token_ids=(30910, 13, 30910, 40457, 32031, 38555, 37972, 31877, 31628, 39220, 55353, 31923, 47108, 34895, 32300, 31155), cumulative_logprob=-7.034255657988979, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1732412855.962891, last_token_time=1732412855.962891, first_scheduled_time=1732412855.9705887, first_token_time=1732412856.0352516, time_in_queue=0.007697582244873047, finished_time=1732412856.5612748), lora_request=None),\n",
       " RequestOutput(request_id=2, prompt='[gMASK]sop<|system|>\\n You are a helpful assistant.<|user|>\\n 吉利汽车语音组手唤醒<|assistant|>', prompt_token_ids=[64790, 64792, 64790, 64792, 906, 31007, 13361, 31007, 30994, 13, 809, 383, 260, 6483, 9319, 30930, 64795, 30910, 13, 30910, 40457, 32031, 38128, 54786, 54718, 43256, 64796], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' \\n 很抱歉，我不太明白您的意思。能否提供更多的信息', token_ids=(30910, 13, 30910, 54657, 52992, 31123, 34045, 54948, 32855, 33379, 32662, 31155, 35276, 31692, 32854, 31707), cumulative_logprob=-14.498186274580803, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1732412855.9642894, last_token_time=1732412855.9642894, first_scheduled_time=1732412855.9705887, first_token_time=1732412856.0352516, time_in_queue=0.006299257278442383, finished_time=1732412856.5612872), lora_request=None),\n",
       " RequestOutput(request_id=3, prompt='[gMASK]sop<|system|>\\n You are a helpful assistant.<|user|>\\n 自动驾驶功能介绍<|assistant|>', prompt_token_ids=[64790, 64792, 64790, 64792, 906, 31007, 13361, 31007, 30994, 13, 809, 383, 260, 6483, 9319, 30930, 64795, 30910, 13, 30910, 41882, 31877, 32025, 64796], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' \\n 自动驾驶，即Autonomous driving，是指汽车在没有人类驾驶员', token_ids=(30910, 13, 30910, 41882, 31123, 54996, 14784, 5978, 527, 5007, 31123, 33662, 32031, 45887, 32381, 39315), cumulative_logprob=-7.849432225319447, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1732412855.9648647, last_token_time=1732412855.9648647, first_scheduled_time=1732412855.9705887, first_token_time=1732412856.0352516, time_in_queue=0.0057239532470703125, finished_time=1732412856.5612948), lora_request=None)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

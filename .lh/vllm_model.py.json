{
    "sourceFile": "vllm_model.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 45,
            "patches": [
                {
                    "date": 1732076018952,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1732077224335,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,9 +52,9 @@\n \n        # 加载vLLM大模型\n        self.model = LLM(model=model_path,\n                             tokenizer=model_path,\n-                            tensor_parallel_size=1,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n+                            tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n                             gpu_memory_utilization=0.6, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n                             # dtype=\"bfloat16\")\n"
                },
                {
                    "date": 1732084862406,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,8 +38,10 @@\n \n class ChatLLM(object):\n \n     def __init__(self, model_path):\n+            torch.multiprocessing.set_start_method('spawn')\n+\n        self.tokenizer = AutoTokenizer.from_pretrained(\n                         model_path,\n                         pad_token='<|extra_0|>',\n                         eos_token='<|endoftext|>',\n@@ -54,9 +56,9 @@\n        self.model = LLM(model=model_path,\n                             tokenizer=model_path,\n                             tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n-                            gpu_memory_utilization=0.6, # 可以根据gpu的利用率自己调整这个比例\n+                            gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n                             # dtype=\"bfloat16\")\n        for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n             self.stop_words_ids.extend(stop_id)\n"
                },
                {
                    "date": 1732084870267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,10 +38,9 @@\n \n class ChatLLM(object):\n \n     def __init__(self, model_path):\n-            torch.multiprocessing.set_start_method('spawn')\n-\n+       torch.multiprocessing.set_start_method('spawn')\n        self.tokenizer = AutoTokenizer.from_pretrained(\n                         model_path,\n                         pad_token='<|extra_0|>',\n                         eos_token='<|endoftext|>',\n"
                },
                {
                    "date": 1732084976019,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,9 +38,8 @@\n \n class ChatLLM(object):\n \n     def __init__(self, model_path):\n-       torch.multiprocessing.set_start_method('spawn')\n        self.tokenizer = AutoTokenizer.from_pretrained(\n                         model_path,\n                         pad_token='<|extra_0|>',\n                         eos_token='<|endoftext|>',\n"
                },
                {
                    "date": 1732085295377,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,9 +52,9 @@\n \n        # 加载vLLM大模型\n        self.model = LLM(model=model_path,\n                             tokenizer=model_path,\n-                            tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n+                            tensor_parallel_size=1,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n                             gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n                             # dtype=\"bfloat16\")\n"
                },
                {
                    "date": 1732086203961,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,9 +52,9 @@\n \n        # 加载vLLM大模型\n        self.model = LLM(model=model_path,\n                             tokenizer=model_path,\n-                            tensor_parallel_size=1,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n+                            tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n                             gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n                             # dtype=\"bfloat16\")\n"
                },
                {
                    "date": 1732086384032,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,9 +64,9 @@\n \n        # LLM的采样参数\n        sampling_kwargs = {\n             \"stop_token_ids\": self.stop_words_ids,\n-            \"early_stopping\": False,\n+            # \"early_stopping\": False,\n             \"top_p\": 1.0,\n             \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n             \"temperature\": 0.0,\n             \"max_tokens\": 2000,\n"
                },
                {
                    "date": 1732086577567,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,9 +72,9 @@\n             \"max_tokens\": 2000,\n             \"repetition_penalty\": self.generation_config.repetition_penalty,\n             \"n\":1,\n             \"best_of\":2,\n-            \"use_beam_search\":True\n+            # \"use_beam_search\":True\n        }\n        self.sampling_params = SamplingParams(**sampling_kwargs)\n \n     # 批量推理，输入一个batch，返回一个batch的答案\n"
                },
                {
                    "date": 1732087061690,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,9 +70,9 @@\n             \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n             \"temperature\": 0.0,\n             \"max_tokens\": 2000,\n             \"repetition_penalty\": self.generation_config.repetition_penalty,\n-            \"n\":1,\n+            \"n\":2,\n             \"best_of\":2,\n             # \"use_beam_search\":True\n        }\n        self.sampling_params = SamplingParams(**sampling_kwargs)\n"
                },
                {
                    "date": 1732087131527,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,10 +70,10 @@\n             \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n             \"temperature\": 0.0,\n             \"max_tokens\": 2000,\n             \"repetition_penalty\": self.generation_config.repetition_penalty,\n-            \"n\":2,\n-            \"best_of\":2,\n+            \"n\":1,\n+            \"best_of\":1,\n             # \"use_beam_search\":True\n        }\n        self.sampling_params = SamplingParams(**sampling_kwargs)\n \n"
                },
                {
                    "date": 1732087425707,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,10 +71,10 @@\n             \"temperature\": 0.0,\n             \"max_tokens\": 2000,\n             \"repetition_penalty\": self.generation_config.repetition_penalty,\n             \"n\":1,\n-            \"best_of\":1,\n-            # \"use_beam_search\":True\n+            \"best_of\":2,\n+            \"use_beam_search\":True\n        }\n        self.sampling_params = SamplingParams(**sampling_kwargs)\n \n     # 批量推理，输入一个batch，返回一个batch的答案\n"
                },
                {
                    "date": 1732344169440,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,9 +64,9 @@\n \n        # LLM的采样参数\n        sampling_kwargs = {\n             \"stop_token_ids\": self.stop_words_ids,\n-            # \"early_stopping\": False,\n+            \"early_stopping\": False,\n             \"top_p\": 1.0,\n             \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n             \"temperature\": 0.0,\n             \"max_tokens\": 2000,\n@@ -103,9 +103,9 @@\n        torch_gc()\n        return batch_response\n \n if __name__ == \"__main__\":\n-    qwen7 = \"/root/autodl-tmp/codes/pre_train_model/Qwen-7B-Chat\"\n+    qwen7 = \"pre_train_model/Qwen-7B-Chat\"\n     start = time.time()\n     llm = ChatLLM(qwen7)\n     test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n     generated_text = llm.infer(test)\n"
                },
                {
                    "date": 1732347713908,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,9 +103,9 @@\n        torch_gc()\n        return batch_response\n \n if __name__ == \"__main__\":\n-    qwen7 = \"pre_train_model/Qwen-7B-Chat\"\n+    qwen7 = \"pre_train_model/chatglm3-6b\"\n     start = time.time()\n     llm = ChatLLM(qwen7)\n     test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n     generated_text = llm.infer(test)\n"
                },
                {
                    "date": 1732347919821,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,9 +103,9 @@\n        torch_gc()\n        return batch_response\n \n if __name__ == \"__main__\":\n-    qwen7 = \"pre_train_model/chatglm3-6b\"\n+    qwen7 = \"pre_train_model/Qwen-7B-Chat\"\n     start = time.time()\n     llm = ChatLLM(qwen7)\n     test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n     generated_text = llm.infer(test)\n"
                },
                {
                    "date": 1732347941298,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,9 +103,9 @@\n        torch_gc()\n        return batch_response\n \n if __name__ == \"__main__\":\n-    qwen7 = \"pre_train_model/Qwen-7B-Chat\"\n+    qwen7 = \"pre_train_model/Qwen1.5-7B-Chat\"\n     start = time.time()\n     llm = ChatLLM(qwen7)\n     test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n     generated_text = llm.infer(test)\n"
                },
                {
                    "date": 1732348135649,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,9 +10,10 @@\n from qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n \n \n os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n-\n+os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n+x\n DEVICE = LLM_DEVICE\n DEVICE_ID = \"0\"\n CUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\n \n"
                },
                {
                    "date": 1732348271771,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n \n \n os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n-x\n+\n DEVICE = LLM_DEVICE\n DEVICE_ID = \"0\"\n CUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\n \n@@ -36,9 +36,9 @@\n         with torch.cuda.device(CUDA_DEVICE):\n             torch.cuda.empty_cache()\n             torch.cuda.ipc_collect()\n \n-class ChatLLM(object):\n+class Qwen(object):\n \n     def __init__(self, model_path):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n                         model_path,\n@@ -103,12 +103,81 @@\n            batch_response.append(output_str)\n        torch_gc()\n        return batch_response\n \n+class ChatGLM(object):\n+\n+    def __init__(self, model_path):\n+       self.tokenizer = AutoTokenizer.from_pretrained(\n+                        model_path,\n+                        pad_token='<|extra_0|>',\n+                        eos_token='<|endoftext|>',\n+                        padding_side='left',\n+                        trust_remote_code=True\n+                    )\n+       self.generation_config = GenerationConfig.from_pretrained(model_path, pad_token_id=self.tokenizer.pad_token_id)\n+       self.tokenizer.eos_token_id = self.generation_config.eos_token_id\n+       self.stop_words_ids = []\n+\n+       # 加载vLLM大模型\n+       self.model = LLM(model=model_path,\n+                            tokenizer=model_path,\n+                            tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n+                            trust_remote_code=True,\n+                            gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n+                            dtype='half')\n+                            # dtype=\"bfloat16\")\n+       for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n+            self.stop_words_ids.extend(stop_id)\n+       self.stop_words_ids.extend([self.generation_config.eos_token_id])\n+\n+       # LLM的采样参数\n+       sampling_kwargs = {\n+            \"stop_token_ids\": self.stop_words_ids,\n+            \"early_stopping\": False,\n+            \"top_p\": 1.0,\n+            \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n+            \"temperature\": 0.0,\n+            \"max_tokens\": 2000,\n+            \"repetition_penalty\": self.generation_config.repetition_penalty,\n+            \"n\":1,\n+            \"best_of\":2,\n+            \"use_beam_search\":True\n+       }\n+       self.sampling_params = SamplingParams(**sampling_kwargs)\n+\n+    # 批量推理，输入一个batch，返回一个batch的答案\n+    def infer(self, prompts):\n+       batch_text = []\n+       for q in prompts:\n+            raw_text, _ = make_context(\n+              self.tokenizer,\n+              q,\n+              system=\"You are a helpful assistant.\",\n+              max_window_size=self.generation_config.max_window_size,\n+              chat_format=self.generation_config.chat_format,\n+            )\n+            batch_text.append(raw_text)\n+       outputs = self.model.generate(batch_text,\n+                                sampling_params = self.sampling_params\n+                               )\n+       batch_response = []\n+       for output in outputs:\n+           output_str = output.outputs[0].text\n+           if IMEND in output_str:\n+               output_str = output_str[:-len(IMEND)]\n+           if ENDOFTEXT in output_str:\n+               output_str = output_str[:-len(ENDOFTEXT)]\n+           batch_response.append(output_str)\n+       torch_gc()\n+       return batch_response\n+\n+\n if __name__ == \"__main__\":\n     qwen7 = \"pre_train_model/Qwen1.5-7B-Chat\"\n+    chatglm = \"pre_train_model/chatglm3-6b\"\n     start = time.time()\n-    llm = ChatLLM(qwen7)\n+    llm = ChatGLM(qwen7)\n     test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n     generated_text = llm.infer(test)\n     print(generated_text)\n     end = time.time()\n"
                },
                {
                    "date": 1732348278214,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -175,9 +175,9 @@\n if __name__ == \"__main__\":\n     qwen7 = \"pre_train_model/Qwen1.5-7B-Chat\"\n     chatglm = \"pre_train_model/chatglm3-6b\"\n     start = time.time()\n-    llm = ChatGLM(qwen7)\n+    llm = ChatGLM(chatglm)\n     test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n     generated_text = llm.infer(test)\n     print(generated_text)\n     end = time.time()\n"
                },
                {
                    "date": 1732348293994,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -108,11 +108,8 @@\n \n     def __init__(self, model_path):\n        self.tokenizer = AutoTokenizer.from_pretrained(\n                         model_path,\n-                        pad_token='<|extra_0|>',\n-                        eos_token='<|endoftext|>',\n-                        padding_side='left',\n                         trust_remote_code=True\n                     )\n        self.generation_config = GenerationConfig.from_pretrained(model_path, pad_token_id=self.tokenizer.pad_token_id)\n        self.tokenizer.eos_token_id = self.generation_config.eos_token_id\n"
                },
                {
                    "date": 1732348933131,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -110,10 +110,10 @@\n        self.tokenizer = AutoTokenizer.from_pretrained(\n                         model_path,\n                         trust_remote_code=True\n                     )\n-       self.generation_config = GenerationConfig.from_pretrained(model_path, pad_token_id=self.tokenizer.pad_token_id)\n-       self.tokenizer.eos_token_id = self.generation_config.eos_token_id\n+    #    self.generation_config = GenerationConfig.from_pretrained(model_path, pad_token_id=self.tokenizer.pad_token_id)\n+    #    self.tokenizer.eos_token_id = self.generation_config.eos_token_id\n        self.stop_words_ids = []\n \n        # 加载vLLM大模型\n        self.model = LLM(model=model_path,\n@@ -145,8 +145,15 @@\n     # 批量推理，输入一个batch，返回一个batch的答案\n     def infer(self, prompts):\n        batch_text = []\n        for q in prompts:\n+            messages = [\n+                {\n+                    \"role\": \"system\",\n+                    \"content\": \"You are a helpful assistant.\",\n+                },\n+                {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n+            ]\n             raw_text, _ = make_context(\n               self.tokenizer,\n               q,\n               system=\"You are a helpful assistant.\",\n"
                },
                {
                    "date": 1732349007314,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -152,16 +152,10 @@\n                     \"content\": \"You are a helpful assistant.\",\n                 },\n                 {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n             ]\n-            raw_text, _ = make_context(\n-              self.tokenizer,\n-              q,\n-              system=\"You are a helpful assistant.\",\n-              max_window_size=self.generation_config.max_window_size,\n-              chat_format=self.generation_config.chat_format,\n-            )\n-            batch_text.append(raw_text)\n+            tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n+            batch_text.append(tokenized_chat)\n        outputs = self.model.generate(batch_text,\n                                 sampling_params = self.sampling_params\n                                )\n        batch_response = []\n"
                },
                {
                    "date": 1732349036374,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -122,11 +122,11 @@\n                             trust_remote_code=True,\n                             gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n                             # dtype=\"bfloat16\")\n-       for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n-            self.stop_words_ids.extend(stop_id)\n-       self.stop_words_ids.extend([self.generation_config.eos_token_id])\n+    #    for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n+    #         self.stop_words_ids.extend(stop_id)\n+    #    self.stop_words_ids.extend([self.generation_config.eos_token_id])\n \n        # LLM的采样参数\n        sampling_kwargs = {\n             \"stop_token_ids\": self.stop_words_ids,\n"
                },
                {
                    "date": 1732349051571,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -131,12 +131,12 @@\n        sampling_kwargs = {\n             \"stop_token_ids\": self.stop_words_ids,\n             \"early_stopping\": False,\n             \"top_p\": 1.0,\n-            \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n+            # \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n             \"temperature\": 0.0,\n             \"max_tokens\": 2000,\n-            \"repetition_penalty\": self.generation_config.repetition_penalty,\n+            # \"repetition_penalty\": self.generation_config.repetition_penalty,\n             \"n\":1,\n             \"best_of\":2,\n             \"use_beam_search\":True\n        }\n"
                },
                {
                    "date": 1732349089973,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,9 +150,9 @@\n                 {\n                     \"role\": \"system\",\n                     \"content\": \"You are a helpful assistant.\",\n                 },\n-                {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n+                {\"role\": \"user\", \"content\": q},\n             ]\n             tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n             batch_text.append(tokenized_chat)\n        outputs = self.model.generate(batch_text,\n"
                },
                {
                    "date": 1732411910578,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n from transformers import AutoModelForCausalLM, AutoTokenizer\n from transformers import GenerationConfig\n from qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n \n-\n+os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n \n DEVICE = LLM_DEVICE\n"
                },
                {
                    "date": 1732412022166,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n from transformers import AutoModelForCausalLM, AutoTokenizer\n from transformers import GenerationConfig\n from qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n \n-os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n+# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n \n DEVICE = LLM_DEVICE\n"
                },
                {
                    "date": 1732412240922,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -143,10 +143,10 @@\n        self.sampling_params = SamplingParams(**sampling_kwargs)\n \n     # 批量推理，输入一个batch，返回一个batch的答案\n     def infer(self, prompts):\n-       batch_text = []\n-       for q in prompts:\n+        batch_text = []\n+        for q in prompts:\n             messages = [\n                 {\n                     \"role\": \"system\",\n                     \"content\": \"You are a helpful assistant.\",\n@@ -154,21 +154,22 @@\n                 {\"role\": \"user\", \"content\": q},\n             ]\n             tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n             batch_text.append(tokenized_chat)\n-       outputs = self.model.generate(batch_text,\n+        batch_input = torch.cat(batch_text, dim=0)\n+        outputs = self.model.generate(batch_input,\n                                 sampling_params = self.sampling_params\n-                               )\n-       batch_response = []\n-       for output in outputs:\n-           output_str = output.outputs[0].text\n-           if IMEND in output_str:\n-               output_str = output_str[:-len(IMEND)]\n-           if ENDOFTEXT in output_str:\n-               output_str = output_str[:-len(ENDOFTEXT)]\n-           batch_response.append(output_str)\n-       torch_gc()\n-       return batch_response\n+                                )\n+        batch_response = []\n+        for output in outputs:\n+            output_str = output.outputs[0].text\n+            if IMEND in output_str:\n+                output_str = output_str[:-len(IMEND)]\n+            if ENDOFTEXT in output_str:\n+                output_str = output_str[:-len(ENDOFTEXT)]\n+            batch_response.append(output_str)\n+        torch_gc()\n+        return batch_response\n \n \n if __name__ == \"__main__\":\n     qwen7 = \"pre_train_model/Qwen1.5-7B-Chat\"\n"
                },
                {
                    "date": 1732412841250,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -152,9 +152,9 @@\n                     \"content\": \"You are a helpful assistant.\",\n                 },\n                 {\"role\": \"user\", \"content\": q},\n             ]\n-            tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n+            tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n             batch_text.append(tokenized_chat)\n         batch_input = torch.cat(batch_text, dim=0)\n         outputs = self.model.generate(batch_input,\n                                 sampling_params = self.sampling_params\n"
                },
                {
                    "date": 1732412914456,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -152,10 +152,10 @@\n                     \"content\": \"You are a helpful assistant.\",\n                 },\n                 {\"role\": \"user\", \"content\": q},\n             ]\n-            tokenized_chat = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n-            batch_text.append(tokenized_chat)\n+            chat = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n+            batch_text.append(chat)\n         batch_input = torch.cat(batch_text, dim=0)\n         outputs = self.model.generate(batch_input,\n                                 sampling_params = self.sampling_params\n                                 )\n"
                },
                {
                    "date": 1732412921455,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -154,10 +154,9 @@\n                 {\"role\": \"user\", \"content\": q},\n             ]\n             chat = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n             batch_text.append(chat)\n-        batch_input = torch.cat(batch_text, dim=0)\n-        outputs = self.model.generate(batch_input,\n+        outputs = self.model.generate(batch_text,\n                                 sampling_params = self.sampling_params\n                                 )\n         batch_response = []\n         for output in outputs:\n"
                },
                {
                    "date": 1732413890703,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -172,10 +172,12 @@\n \n if __name__ == \"__main__\":\n     qwen7 = \"pre_train_model/Qwen1.5-7B-Chat\"\n     chatglm = \"pre_train_model/chatglm3-6b\"\n+    baichuan = \"pre_train_model/Baichuan2-7B-Chat\"\n+    \n     start = time.time()\n-    llm = ChatGLM(chatglm)\n+    llm = ChatGLM(baichuan)\n     test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n     generated_text = llm.infer(test)\n     print(generated_text)\n     end = time.time()\n"
                },
                {
                    "date": 1732413906020,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -173,9 +173,9 @@\n if __name__ == \"__main__\":\n     qwen7 = \"pre_train_model/Qwen1.5-7B-Chat\"\n     chatglm = \"pre_train_model/chatglm3-6b\"\n     baichuan = \"pre_train_model/Baichuan2-7B-Chat\"\n-    \n+    qwen = \"pre_train_model/Qwen1.5-7B-Chat\"\n     start = time.time()\n     llm = ChatGLM(baichuan)\n     test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n     generated_text = llm.infer(test)\n"
                },
                {
                    "date": 1732416537963,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -168,9 +168,74 @@\n             batch_response.append(output_str)\n         torch_gc()\n         return batch_response\n \n+class Baichuan(object):\n \n+    def __init__(self, model_path):\n+       self.tokenizer = AutoTokenizer.from_pretrained(\n+                        model_path,\n+                        trust_remote_code=True\n+                    )\n+    #    self.generation_config = GenerationConfig.from_pretrained(model_path, pad_token_id=self.tokenizer.pad_token_id)\n+    #    self.tokenizer.eos_token_id = self.generation_config.eos_token_id\n+       self.stop_words_ids = []\n+\n+       # 加载vLLM大模型\n+       self.model = LLM(model=model_path,\n+                            tokenizer=model_path,\n+                            tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n+                            trust_remote_code=True,\n+                            gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n+                            dtype='half')\n+                            # dtype=\"bfloat16\")\n+    #    for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n+    #         self.stop_words_ids.extend(stop_id)\n+    #    self.stop_words_ids.extend([self.generation_config.eos_token_id])\n+\n+       # LLM的采样参数\n+       sampling_kwargs = {\n+            \"stop_token_ids\": self.stop_words_ids,\n+            \"early_stopping\": False,\n+            \"top_p\": 1.0,\n+            # \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n+            \"temperature\": 0.0,\n+            \"max_tokens\": 2000,\n+            # \"repetition_penalty\": self.generation_config.repetition_penalty,\n+            \"n\":1,\n+            \"best_of\":2,\n+            \"use_beam_search\":True\n+       }\n+       self.sampling_params = SamplingParams(**sampling_kwargs)\n+\n+    # 批量推理，输入一个batch，返回一个batch的答案\n+    def infer(self, prompts):\n+        batch_text = []\n+        for q in prompts:\n+            messages = [\n+                {\n+                    \"role\": \"system\",\n+                    \"content\": \"You are a helpful assistant.\",\n+                },\n+                {\"role\": \"user\", \"content\": q},\n+            ]\n+            chat = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n+            batch_text.append(chat)\n+        outputs = self.model.generate(batch_text,\n+                                sampling_params = self.sampling_params\n+                                )\n+        batch_response = []\n+        for output in outputs:\n+            output_str = output.outputs[0].text\n+            if IMEND in output_str:\n+                output_str = output_str[:-len(IMEND)]\n+            if ENDOFTEXT in output_str:\n+                output_str = output_str[:-len(ENDOFTEXT)]\n+            batch_response.append(output_str)\n+        torch_gc()\n+        return batch_response\n+\n+\n if __name__ == \"__main__\":\n     qwen7 = \"pre_train_model/Qwen1.5-7B-Chat\"\n     chatglm = \"pre_train_model/chatglm3-6b\"\n     baichuan = \"pre_train_model/Baichuan2-7B-Chat\"\n"
                },
                {
                    "date": 1732416573898,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -186,8 +186,10 @@\n                             tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n                             gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n+       self.model.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\")\n+\n                             # dtype=\"bfloat16\")\n     #    for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n     #         self.stop_words_ids.extend(stop_id)\n     #    self.stop_words_ids.extend([self.generation_config.eos_token_id])\n"
                },
                {
                    "date": 1732416604867,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -221,9 +221,9 @@\n                 {\"role\": \"user\", \"content\": q},\n             ]\n             chat = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n             batch_text.append(chat)\n-        outputs = self.model.generate(batch_text,\n+        outputs = self.model.chat(tokenizer, batch_text,\n                                 sampling_params = self.sampling_params\n                                 )\n         batch_response = []\n         for output in outputs:\n"
                },
                {
                    "date": 1732418420454,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,9 +7,9 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer\n from transformers import GenerationConfig\n from qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n-\n+from baichuan_generation_utils import build_chat_input\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n \n@@ -219,11 +219,11 @@\n                     \"content\": \"You are a helpful assistant.\",\n                 },\n                 {\"role\": \"user\", \"content\": q},\n             ]\n-            chat = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, return_tensors=\"pt\")\n+            chat, _ = build_chat_input(self.model, self.tokenizer, messages)\n             batch_text.append(chat)\n-        outputs = self.model.chat(tokenizer, batch_text,\n+        outputs = self.model.chat(self.tokenizer, batch_text,\n                                 sampling_params = self.sampling_params\n                                 )\n         batch_response = []\n         for output in outputs:\n"
                },
                {
                    "date": 1732418434653,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -242,9 +242,9 @@\n     chatglm = \"pre_train_model/chatglm3-6b\"\n     baichuan = \"pre_train_model/Baichuan2-7B-Chat\"\n     qwen = \"pre_train_model/Qwen1.5-7B-Chat\"\n     start = time.time()\n-    llm = ChatGLM(baichuan)\n+    llm = Baichuan(baichuan)\n     test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n     generated_text = llm.infer(test)\n     print(generated_text)\n     end = time.time()\n"
                },
                {
                    "date": 1732418473123,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -221,9 +221,9 @@\n                 {\"role\": \"user\", \"content\": q},\n             ]\n             chat, _ = build_chat_input(self.model, self.tokenizer, messages)\n             batch_text.append(chat)\n-        outputs = self.model.chat(self.tokenizer, batch_text,\n+        outputs = self.model.generate(batch_text,\n                                 sampling_params = self.sampling_params\n                                 )\n         batch_response = []\n         for output in outputs:\n"
                },
                {
                    "date": 1732418891466,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -186,9 +186,9 @@\n                             tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n                             gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n-       self.model.generation_config = GenerationConfig.from_pretrained(\"baichuan-inc/Baichuan2-7B-Chat\")\n+       self.model.generation_config = GenerationConfig.from_pretrained(model_path)\n \n                             # dtype=\"bfloat16\")\n     #    for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n     #         self.stop_words_ids.extend(stop_id)\n"
                },
                {
                    "date": 1732419241085,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -186,9 +186,9 @@\n                             tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n                             gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n-       self.model.generation_config = GenerationConfig.from_pretrained(model_path)\n+       self.model.config = GenerationConfig.from_pretrained(model_path)\n \n                             # dtype=\"bfloat16\")\n     #    for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n     #         self.stop_words_ids.extend(stop_id)\n"
                },
                {
                    "date": 1732419294757,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -186,9 +186,9 @@\n                             tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n                             gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n-       self.model.config = GenerationConfig.from_pretrained(model_path)\n+       self.model.generation_config = GenerationConfig.from_pretrained(model_path)\n \n                             # dtype=\"bfloat16\")\n     #    for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n     #         self.stop_words_ids.extend(stop_id)\n"
                },
                {
                    "date": 1732419468493,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -219,10 +219,10 @@\n                     \"content\": \"You are a helpful assistant.\",\n                 },\n                 {\"role\": \"user\", \"content\": q},\n             ]\n-            chat, _ = build_chat_input(self.model, self.tokenizer, messages)\n-            batch_text.append(chat)\n+            # chat, _ = build_chat_input(self.model, self.tokenizer, messages)\n+            batch_text.append(messages)\n         outputs = self.model.generate(batch_text,\n                                 sampling_params = self.sampling_params\n                                 )\n         batch_response = []\n"
                },
                {
                    "date": 1732419515542,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -221,9 +221,9 @@\n                 {\"role\": \"user\", \"content\": q},\n             ]\n             # chat, _ = build_chat_input(self.model, self.tokenizer, messages)\n             batch_text.append(messages)\n-        outputs = self.model.generate(batch_text,\n+        outputs = self.model.chat(batch_text,add_generation_prompt =True,\n                                 sampling_params = self.sampling_params\n                                 )\n         batch_response = []\n         for output in outputs:\n"
                },
                {
                    "date": 1732420261787,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -219,11 +219,11 @@\n                     \"content\": \"You are a helpful assistant.\",\n                 },\n                 {\"role\": \"user\", \"content\": q},\n             ]\n-            # chat, _ = build_chat_input(self.model, self.tokenizer, messages)\n-            batch_text.append(messages)\n-        outputs = self.model.chat(batch_text,add_generation_prompt =True,\n+            chat, _ = build_chat_input(self.model, self.tokenizer, messages)\n+            batch_text.append(chat)\n+        outputs = self.model.generate(batch_text,\n                                 sampling_params = self.sampling_params\n                                 )\n         batch_response = []\n         for output in outputs:\n"
                },
                {
                    "date": 1732422712039,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -242,9 +242,10 @@\n     chatglm = \"pre_train_model/chatglm3-6b\"\n     baichuan = \"pre_train_model/Baichuan2-7B-Chat\"\n     qwen = \"pre_train_model/Qwen1.5-7B-Chat\"\n     start = time.time()\n-    llm = Baichuan(baichuan)\n+    # llm = Baichuan(baichuan)\n+    llm = ChatGLM(qwen)\n     test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n     generated_text = llm.infer(test)\n     print(generated_text)\n     end = time.time()\n"
                }
            ],
            "date": 1732076018952,
            "name": "Commit-0",
            "content": "import os\nimport torch\nimport time\n\nfrom config import *\nfrom vllm import LLM, SamplingParams\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nDEVICE = LLM_DEVICE\nDEVICE_ID = \"0\"\nCUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\n\nIMEND = \"<|im_end|>\"\nENDOFTEXT = \"<|endoftext|>\"\n\n# 获取stop token的id\ndef get_stop_words_ids(chat_format, tokenizer):\n    if chat_format == \"raw\":\n        stop_words_ids = [tokenizer.encode(\"Human:\"), [tokenizer.eod_id]]\n    elif chat_format == \"chatml\":\n        stop_words_ids = [[tokenizer.im_end_id], [tokenizer.im_start_id]]\n    else:\n        raise NotImplementedError(f\"Unknown chat format {chat_format!r}\")\n    return stop_words_ids\n\n# 释放gpu显存\ndef torch_gc():\n    if torch.cuda.is_available():\n        with torch.cuda.device(CUDA_DEVICE):\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n\nclass ChatLLM(object):\n\n    def __init__(self, model_path):\n       self.tokenizer = AutoTokenizer.from_pretrained(\n                        model_path,\n                        pad_token='<|extra_0|>',\n                        eos_token='<|endoftext|>',\n                        padding_side='left',\n                        trust_remote_code=True\n                    )\n       self.generation_config = GenerationConfig.from_pretrained(model_path, pad_token_id=self.tokenizer.pad_token_id)\n       self.tokenizer.eos_token_id = self.generation_config.eos_token_id\n       self.stop_words_ids = []\n\n       # 加载vLLM大模型\n       self.model = LLM(model=model_path,\n                            tokenizer=model_path,\n                            tensor_parallel_size=1,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                            trust_remote_code=True,\n                            gpu_memory_utilization=0.6, # 可以根据gpu的利用率自己调整这个比例\n                            dtype='half')\n                            # dtype=\"bfloat16\")\n       for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n            self.stop_words_ids.extend(stop_id)\n       self.stop_words_ids.extend([self.generation_config.eos_token_id])\n\n       # LLM的采样参数\n       sampling_kwargs = {\n            \"stop_token_ids\": self.stop_words_ids,\n            \"early_stopping\": False,\n            \"top_p\": 1.0,\n            \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n            \"temperature\": 0.0,\n            \"max_tokens\": 2000,\n            \"repetition_penalty\": self.generation_config.repetition_penalty,\n            \"n\":1,\n            \"best_of\":2,\n            \"use_beam_search\":True\n       }\n       self.sampling_params = SamplingParams(**sampling_kwargs)\n\n    # 批量推理，输入一个batch，返回一个batch的答案\n    def infer(self, prompts):\n       batch_text = []\n       for q in prompts:\n            raw_text, _ = make_context(\n              self.tokenizer,\n              q,\n              system=\"You are a helpful assistant.\",\n              max_window_size=self.generation_config.max_window_size,\n              chat_format=self.generation_config.chat_format,\n            )\n            batch_text.append(raw_text)\n       outputs = self.model.generate(batch_text,\n                                sampling_params = self.sampling_params\n                               )\n       batch_response = []\n       for output in outputs:\n           output_str = output.outputs[0].text\n           if IMEND in output_str:\n               output_str = output_str[:-len(IMEND)]\n           if ENDOFTEXT in output_str:\n               output_str = output_str[:-len(ENDOFTEXT)]\n           batch_response.append(output_str)\n       torch_gc()\n       return batch_response\n\nif __name__ == \"__main__\":\n    qwen7 = \"/root/autodl-tmp/codes/pre_train_model/Qwen-7B-Chat\"\n    start = time.time()\n    llm = ChatLLM(qwen7)\n    test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n    generated_text = llm.infer(test)\n    print(generated_text)\n    end = time.time()\n    print(\"cost time: \" + str((end-start)/60))\n"
        }
    ]
}
{
    "sourceFile": "vllm_model.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 11,
            "patches": [
                {
                    "date": 1732076018952,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1732077224335,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,9 +52,9 @@\n \n        # 加载vLLM大模型\n        self.model = LLM(model=model_path,\n                             tokenizer=model_path,\n-                            tensor_parallel_size=1,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n+                            tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n                             gpu_memory_utilization=0.6, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n                             # dtype=\"bfloat16\")\n"
                },
                {
                    "date": 1732084862406,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,8 +38,10 @@\n \n class ChatLLM(object):\n \n     def __init__(self, model_path):\n+            torch.multiprocessing.set_start_method('spawn')\n+\n        self.tokenizer = AutoTokenizer.from_pretrained(\n                         model_path,\n                         pad_token='<|extra_0|>',\n                         eos_token='<|endoftext|>',\n@@ -54,9 +56,9 @@\n        self.model = LLM(model=model_path,\n                             tokenizer=model_path,\n                             tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n-                            gpu_memory_utilization=0.6, # 可以根据gpu的利用率自己调整这个比例\n+                            gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n                             # dtype=\"bfloat16\")\n        for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n             self.stop_words_ids.extend(stop_id)\n"
                },
                {
                    "date": 1732084870267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,10 +38,9 @@\n \n class ChatLLM(object):\n \n     def __init__(self, model_path):\n-            torch.multiprocessing.set_start_method('spawn')\n-\n+       torch.multiprocessing.set_start_method('spawn')\n        self.tokenizer = AutoTokenizer.from_pretrained(\n                         model_path,\n                         pad_token='<|extra_0|>',\n                         eos_token='<|endoftext|>',\n"
                },
                {
                    "date": 1732084976019,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,9 +38,8 @@\n \n class ChatLLM(object):\n \n     def __init__(self, model_path):\n-       torch.multiprocessing.set_start_method('spawn')\n        self.tokenizer = AutoTokenizer.from_pretrained(\n                         model_path,\n                         pad_token='<|extra_0|>',\n                         eos_token='<|endoftext|>',\n"
                },
                {
                    "date": 1732085295377,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,9 +52,9 @@\n \n        # 加载vLLM大模型\n        self.model = LLM(model=model_path,\n                             tokenizer=model_path,\n-                            tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n+                            tensor_parallel_size=1,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n                             gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n                             # dtype=\"bfloat16\")\n"
                },
                {
                    "date": 1732086203961,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,9 +52,9 @@\n \n        # 加载vLLM大模型\n        self.model = LLM(model=model_path,\n                             tokenizer=model_path,\n-                            tensor_parallel_size=1,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n+                            tensor_parallel_size=2,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                             trust_remote_code=True,\n                             gpu_memory_utilization=0.8, # 可以根据gpu的利用率自己调整这个比例\n                             dtype='half')\n                             # dtype=\"bfloat16\")\n"
                },
                {
                    "date": 1732086384032,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -64,9 +64,9 @@\n \n        # LLM的采样参数\n        sampling_kwargs = {\n             \"stop_token_ids\": self.stop_words_ids,\n-            \"early_stopping\": False,\n+            # \"early_stopping\": False,\n             \"top_p\": 1.0,\n             \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n             \"temperature\": 0.0,\n             \"max_tokens\": 2000,\n"
                },
                {
                    "date": 1732086577567,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -72,9 +72,9 @@\n             \"max_tokens\": 2000,\n             \"repetition_penalty\": self.generation_config.repetition_penalty,\n             \"n\":1,\n             \"best_of\":2,\n-            \"use_beam_search\":True\n+            # \"use_beam_search\":True\n        }\n        self.sampling_params = SamplingParams(**sampling_kwargs)\n \n     # 批量推理，输入一个batch，返回一个batch的答案\n"
                },
                {
                    "date": 1732087061690,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,9 +70,9 @@\n             \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n             \"temperature\": 0.0,\n             \"max_tokens\": 2000,\n             \"repetition_penalty\": self.generation_config.repetition_penalty,\n-            \"n\":1,\n+            \"n\":2,\n             \"best_of\":2,\n             # \"use_beam_search\":True\n        }\n        self.sampling_params = SamplingParams(**sampling_kwargs)\n"
                },
                {
                    "date": 1732087131527,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -70,10 +70,10 @@\n             \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n             \"temperature\": 0.0,\n             \"max_tokens\": 2000,\n             \"repetition_penalty\": self.generation_config.repetition_penalty,\n-            \"n\":2,\n-            \"best_of\":2,\n+            \"n\":1,\n+            \"best_of\":1,\n             # \"use_beam_search\":True\n        }\n        self.sampling_params = SamplingParams(**sampling_kwargs)\n \n"
                },
                {
                    "date": 1732087425707,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -71,10 +71,10 @@\n             \"temperature\": 0.0,\n             \"max_tokens\": 2000,\n             \"repetition_penalty\": self.generation_config.repetition_penalty,\n             \"n\":1,\n-            \"best_of\":1,\n-            # \"use_beam_search\":True\n+            \"best_of\":2,\n+            \"use_beam_search\":True\n        }\n        self.sampling_params = SamplingParams(**sampling_kwargs)\n \n     # 批量推理，输入一个batch，返回一个batch的答案\n"
                }
            ],
            "date": 1732076018952,
            "name": "Commit-0",
            "content": "import os\nimport torch\nimport time\n\nfrom config import *\nfrom vllm import LLM, SamplingParams\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers import GenerationConfig\nfrom qwen_generation_utils import make_context, decode_tokens, get_stop_words_ids\n\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nDEVICE = LLM_DEVICE\nDEVICE_ID = \"0\"\nCUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE\n\nIMEND = \"<|im_end|>\"\nENDOFTEXT = \"<|endoftext|>\"\n\n# 获取stop token的id\ndef get_stop_words_ids(chat_format, tokenizer):\n    if chat_format == \"raw\":\n        stop_words_ids = [tokenizer.encode(\"Human:\"), [tokenizer.eod_id]]\n    elif chat_format == \"chatml\":\n        stop_words_ids = [[tokenizer.im_end_id], [tokenizer.im_start_id]]\n    else:\n        raise NotImplementedError(f\"Unknown chat format {chat_format!r}\")\n    return stop_words_ids\n\n# 释放gpu显存\ndef torch_gc():\n    if torch.cuda.is_available():\n        with torch.cuda.device(CUDA_DEVICE):\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n\nclass ChatLLM(object):\n\n    def __init__(self, model_path):\n       self.tokenizer = AutoTokenizer.from_pretrained(\n                        model_path,\n                        pad_token='<|extra_0|>',\n                        eos_token='<|endoftext|>',\n                        padding_side='left',\n                        trust_remote_code=True\n                    )\n       self.generation_config = GenerationConfig.from_pretrained(model_path, pad_token_id=self.tokenizer.pad_token_id)\n       self.tokenizer.eos_token_id = self.generation_config.eos_token_id\n       self.stop_words_ids = []\n\n       # 加载vLLM大模型\n       self.model = LLM(model=model_path,\n                            tokenizer=model_path,\n                            tensor_parallel_size=1,     # 如果是多卡，可以自己把这个并行度设置为卡数N\n                            trust_remote_code=True,\n                            gpu_memory_utilization=0.6, # 可以根据gpu的利用率自己调整这个比例\n                            dtype='half')\n                            # dtype=\"bfloat16\")\n       for stop_id in get_stop_words_ids(self.generation_config.chat_format, self.tokenizer):\n            self.stop_words_ids.extend(stop_id)\n       self.stop_words_ids.extend([self.generation_config.eos_token_id])\n\n       # LLM的采样参数\n       sampling_kwargs = {\n            \"stop_token_ids\": self.stop_words_ids,\n            \"early_stopping\": False,\n            \"top_p\": 1.0,\n            \"top_k\": -1 if self.generation_config.top_k == 0 else self.generation_config.top_k,\n            \"temperature\": 0.0,\n            \"max_tokens\": 2000,\n            \"repetition_penalty\": self.generation_config.repetition_penalty,\n            \"n\":1,\n            \"best_of\":2,\n            \"use_beam_search\":True\n       }\n       self.sampling_params = SamplingParams(**sampling_kwargs)\n\n    # 批量推理，输入一个batch，返回一个batch的答案\n    def infer(self, prompts):\n       batch_text = []\n       for q in prompts:\n            raw_text, _ = make_context(\n              self.tokenizer,\n              q,\n              system=\"You are a helpful assistant.\",\n              max_window_size=self.generation_config.max_window_size,\n              chat_format=self.generation_config.chat_format,\n            )\n            batch_text.append(raw_text)\n       outputs = self.model.generate(batch_text,\n                                sampling_params = self.sampling_params\n                               )\n       batch_response = []\n       for output in outputs:\n           output_str = output.outputs[0].text\n           if IMEND in output_str:\n               output_str = output_str[:-len(IMEND)]\n           if ENDOFTEXT in output_str:\n               output_str = output_str[:-len(ENDOFTEXT)]\n           batch_response.append(output_str)\n       torch_gc()\n       return batch_response\n\nif __name__ == \"__main__\":\n    qwen7 = \"/root/autodl-tmp/codes/pre_train_model/Qwen-7B-Chat\"\n    start = time.time()\n    llm = ChatLLM(qwen7)\n    test = [\"吉利汽车座椅按摩\",\"吉利汽车语音组手唤醒\",\"自动驾驶功能介绍\"]\n    generated_text = llm.infer(test)\n    print(generated_text)\n    end = time.time()\n    print(\"cost time: \" + str((end-start)/60))\n"
        }
    ]
}
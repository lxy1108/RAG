{
    "sourceFile": "run.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 50,
            "patches": [
                {
                    "date": 1732074092473,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1732075567406,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -97,9 +97,9 @@\n \n     start = time.time()\n \n     base = \".\"\n-    qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chatbase\"# + \"/pre_train_model/Qwen-7B-Chat\"\n+    qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n \n     # 解析pdf文档，构造数据\n"
                },
                {
                    "date": 1732083713290,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,8 +18,9 @@\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n \n+os['CUDA_VISIBLE_DEVICES']=2,3\n # 获取Langchain的工具链 \n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n"
                },
                {
                    "date": 1732083809216,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,11 +17,10 @@\n from rerank_model import reRankLLM\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n-\n-os['CUDA_VISIBLE_DEVICES']=2,3\n-# 获取Langchain的工具链 \n+import os\n+os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"# 获取Langchain的工具链 \n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732083817768,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,9 +18,9 @@\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n-os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"# 获取Langchain的工具链 \n+os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732084635377,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,8 +19,9 @@\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n+os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732084652738,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,8 +101,12 @@\n     base = \".\"\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n+    \n+    # LLM大模型\n+    llm = ChatLLM(qwen7)\n+    print(\"llm qwen load ok\")\n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n     dp.ParseBlock(max_seq = 1024)\n@@ -125,11 +129,9 @@\n     # BM25召回\n     bm25 = BM25(data)\n     print(\"bm25 load ok\")\n \n-    # LLM大模型\n-    llm = ChatLLM(qwen7)\n-    print(\"llm qwen load ok\")\n+    \n \n     # reRank模型\n     rerank = reRankLLM(bge_reranker_large)\n     print(\"rerank model load ok\")\n"
                },
                {
                    "date": 1732084999903,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,8 +20,9 @@\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n+os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = =spawn\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732085006725,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = =spawn\n+os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732085016414,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n+os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732085438283,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,11 +18,11 @@\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n-os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n-os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n+# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n+# os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n+# os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732086188229,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n from pdf_parse import DataProcess\n import os\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n # os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-# os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n+os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732087573877,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,9 +19,9 @@\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n-# os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n+os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n@@ -137,9 +137,9 @@\n     rerank = reRankLLM(bge_reranker_large)\n     print(\"rerank model load ok\")\n \n     # 对每一条测试问题，做答案生成处理\n-    with open(base + \"/data/test_question.json\", \"r\") as f:\n+    with open(base + \"/data/test_question.json\", \"r\" , encoding=\"utf-8\") as f:\n         jdata = json.loads(f.read())\n         print(len(jdata))\n         max_length = 4000\n         for idx, line in enumerate(jdata):\n"
                },
                {
                    "date": 1732089879571,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,11 +103,8 @@\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n     \n-    # LLM大模型\n-    llm = ChatLLM(qwen7)\n-    print(\"llm qwen load ok\")\n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n     dp.ParseBlock(max_seq = 1024)\n@@ -130,9 +127,11 @@\n     # BM25召回\n     bm25 = BM25(data)\n     print(\"bm25 load ok\")\n \n-    \n+    # LLM大模型\n+    llm = ChatLLM(qwen7)\n+    print(\"llm qwen load ok\")\n \n     # reRank模型\n     rerank = reRankLLM(bge_reranker_large)\n     print(\"rerank model load ok\")\n"
                },
                {
                    "date": 1732157192892,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,9 +51,9 @@\n         if(cnt > 6):\n             break\n \n     prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n-                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\"，不允许在答案中添加编造成分，答案请使用中文。\n+                                如果无法从中得到答案，请说 \"无答案\"，不允许在答案中添加编造成分，答案请使用中文。\n                                 已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                 1: {emb_ans}\n                                 2: {bm25_ans}\n                                 问题:\n"
                },
                {
                    "date": 1732157561934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n \n def get_rerank(emb_ans, query):\n \n     prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n-                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n+                                如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                 已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                 1: {emb_ans}\n                                 问题:\n                                 {question}\"\"\".format(emb_ans=emb_ans, question = query)\n"
                },
                {
                    "date": 1732161157109,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,10 +14,10 @@\n import re\n \n from vllm_model import ChatLLM\n from rerank_model import reRankLLM\n-from faiss_retriever import FaissRetriever\n-from bm25_retriever import BM25\n+from retrievers.faiss_retriever import FaissRetriever\n+from retrievers.bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n"
                },
                {
                    "date": 1732261808904,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,10 +119,14 @@\n     data = dp.data\n     print(\"data load ok\")\n \n     # Faiss召回\n-    faissretriever = FaissRetriever(m3e, data)\n-    vector_store = faissretriever.vector_store\n+    embed_models = [\"m3e\",\"bge\",\"gte\"]\n+    vector_store_list = []\n+    for embed_model in embed_models:\n+        faissretriever = FaissRetriever(m3e, data)\n+        vector_store = faissretriever.vector_store\n+        vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     # BM25召回\n     bm25 = BM25(data)\n"
                },
                {
                    "date": 1732261886740,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,8 +101,12 @@\n \n     base = \".\"\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n+    reranker_dict = {\n+        \"bge\": base + \"/pre_train_model/bge-reranker-large\"\n+        \"gte\"\n+    }\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n     \n \n     # 解析pdf文档，构造数据\n@@ -119,13 +123,12 @@\n     data = dp.data\n     print(\"data load ok\")\n \n     # Faiss召回\n-    embed_models = [\"m3e\",\"bge\",\"gte\"]\n+    embed_models = [\"m3e\",\"bge\",\"gte\",\"bce\"]\n     vector_store_list = []\n     for embed_model in embed_models:\n         faissretriever = FaissRetriever(m3e, data)\n-        vector_store = faissretriever.vector_store\n         vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     # BM25召回\n"
                },
                {
                    "date": 1732261911984,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,9 +103,9 @@\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     reranker_dict = {\n         \"bge\": base + \"/pre_train_model/bge-reranker-large\"\n-        \"gte\"\n+        \"bce\": base + \"/pre_train_model/bce-reranker-base\"\n     }\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n     \n \n"
                },
                {
                    "date": 1732261917278,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,12 +102,11 @@\n     base = \".\"\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     reranker_dict = {\n-        \"bge\": base + \"/pre_train_model/bge-reranker-large\"\n+        \"bge\": base + \"/pre_train_model/bge-reranker-large\",\n         \"bce\": base + \"/pre_train_model/bce-reranker-base\"\n     }\n-    bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n     \n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n"
                },
                {
                    "date": 1732261935772,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -105,8 +105,9 @@\n     reranker_dict = {\n         \"bge\": base + \"/pre_train_model/bge-reranker-large\",\n         \"bce\": base + \"/pre_train_model/bce-reranker-base\"\n     }\n+    reranker_name = \"bge\"\n     \n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n@@ -138,9 +139,9 @@\n     llm = ChatLLM(qwen7)\n     print(\"llm qwen load ok\")\n \n     # reRank模型\n-    rerank = reRankLLM(bge_reranker_large)\n+    rerank = reRankLLM(reranker_dict[])\n     print(\"rerank model load ok\")\n \n     # 对每一条测试问题，做答案生成处理\n     with open(base + \"/data/test_question.json\", \"r\" , encoding=\"utf-8\") as f:\n"
                },
                {
                    "date": 1732261947348,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -139,9 +139,9 @@\n     llm = ChatLLM(qwen7)\n     print(\"llm qwen load ok\")\n \n     # reRank模型\n-    rerank = reRankLLM(reranker_dict[])\n+    rerank = reRankLLM(reranker_dict[\"bge\"])\n     print(\"rerank model load ok\")\n \n     # 对每一条测试问题，做答案生成处理\n     with open(base + \"/data/test_question.json\", \"r\" , encoding=\"utf-8\") as f:\n"
                },
                {
                    "date": 1732262002155,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,8 +16,9 @@\n from vllm_model import ChatLLM\n from rerank_model import reRankLLM\n from retrievers.faiss_retriever import FaissRetriever\n from retrievers.bm25_retriever import BM25\n+from retrievers.tfidf_retriever import TFIDF\n from pdf_parse import DataProcess\n import os\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n@@ -133,8 +134,11 @@\n \n     # BM25召回\n     bm25 = BM25(data)\n     print(\"bm25 load ok\")\n+    \n+    tfidf = TFIDF(data)\n+    print(\"tfidf load ok\")\n \n     # LLM大模型\n     llm = ChatLLM(qwen7)\n     print(\"llm qwen load ok\")\n"
                },
                {
                    "date": 1732262012899,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -135,8 +135,9 @@\n     # BM25召回\n     bm25 = BM25(data)\n     print(\"bm25 load ok\")\n     \n+    # TFIDF召回\n     tfidf = TFIDF(data)\n     print(\"tfidf load ok\")\n \n     # LLM大模型\n"
                },
                {
                    "date": 1732262735248,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -123,11 +123,12 @@\n     print(len(dp.data))\n     data = dp.data\n     print(\"data load ok\")\n \n+    retrivers = []\n     # Faiss召回\n     embed_models = [\"m3e\",\"bge\",\"gte\",\"bce\"]\n-    vector_store_list = []\n+    # vector_store_list = []\n     for embed_model in embed_models:\n         faissretriever = FaissRetriever(m3e, data)\n         vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n"
                },
                {
                    "date": 1732262749859,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,10 +128,10 @@\n     # Faiss召回\n     embed_models = [\"m3e\",\"bge\",\"gte\",\"bce\"]\n     # vector_store_list = []\n     for embed_model in embed_models:\n-        faissretriever = FaissRetriever(m3e, data)\n-        vector_store_list.append(faissretriever.vector_store)\n+        retrivers.append(FaissRetriever(m3e, data))\n+        # vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     # BM25召回\n     bm25 = BM25(data)\n"
                },
                {
                    "date": 1732262780069,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,13 +133,13 @@\n         # vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     # BM25召回\n-    bm25 = BM25(data)\n+    retrivers.append(BM25(data))\n     print(\"bm25 load ok\")\n     \n     # TFIDF召回\n-    tfidf = TFIDF(data)\n+    retrivers.append(TFIDF(data))\n     print(\"tfidf load ok\")\n \n     # LLM大模型\n     llm = ChatLLM(qwen7)\n"
                },
                {
                    "date": 1732263472754,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,8 +29,13 @@\n                             input_variables=[\"context\", \"question\"])\n \n     return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n \n+def retrievers_recall(retrievers, query, topk=15, maxlen=2500):\n+    context = \"\"\n+    for retriever in retrivers:\n+    \n+\n # 构造提示，根据merged faiss和bm25的召回结果返回答案\n def get_emb_bm25_merge(faiss_context, bm25_context, query):\n     max_length = 2500\n     emb_ans = \"\"\n"
                },
                {
                    "date": 1732263635242,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,8 +32,22 @@\n \n def retrievers_recall(retrievers, query, topk=15, maxlen=2500):\n     context = \"\"\n     for retriever in retrivers:\n+        cur_context = \"\"\n+        cnt = 0\n+        docs = retrivers.GetTopK(query, topk)\n+        for doc in docs:\n+            if isinstance(doc, tuple):\n+                doc = doc[0]\n+            cnt += 1\n+            if(cnt>6):\n+                break\n+            if(len(emb_ans + doc.page_content) > max_length):\n+                break\n+            emb_ans = emb_ans + doc.page_content\n+            \n+            \n     \n \n # 构造提示，根据merged faiss和bm25的召回结果返回答案\n def get_emb_bm25_merge(faiss_context, bm25_context, query):\n"
                },
                {
                    "date": 1732263643267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n                             input_variables=[\"context\", \"question\"])\n \n     return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n \n-def retrievers_recall(retrievers, query, topk=15, maxlen=2500):\n+def retrievers_recall(retrievers, query, topk=15, maxlen=2500, maxnum=6):\n     context = \"\"\n     for retriever in retrivers:\n         cur_context = \"\"\n         cnt = 0\n"
                },
                {
                    "date": 1732263651579,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,11 +39,11 @@\n         for doc in docs:\n             if isinstance(doc, tuple):\n                 doc = doc[0]\n             cnt += 1\n-            if(cnt>6):\n+            if(cnt>maxnum):\n                 break\n-            if(len(emb_ans + doc.page_content) > max_length):\n+            if(len(cur_context + doc.page_content) > max_length):\n                 break\n             emb_ans = emb_ans + doc.page_content\n             \n             \n"
                },
                {
                    "date": 1732263658408,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,9 +41,9 @@\n                 doc = doc[0]\n             cnt += 1\n             if(cnt>maxnum):\n                 break\n-            if(len(cur_context + doc.page_content) > max_length):\n+            if(len(cur_context + doc.page_content) > maxlen):\n                 break\n             emb_ans = emb_ans + doc.page_content\n             \n             \n"
                },
                {
                    "date": 1732263774647,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -31,21 +31,24 @@\n     return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n \n def retrievers_recall(retrievers, query, topk=15, maxlen=2500, maxnum=6):\n     context = \"\"\n+    doc_set = set()\n     for retriever in retrivers:\n-        cur_context = \"\"\n         cnt = 0\n         docs = retrivers.GetTopK(query, topk)\n         for doc in docs:\n             if isinstance(doc, tuple):\n                 doc = doc[0]\n             cnt += 1\n             if(cnt>maxnum):\n                 break\n-            if(len(cur_context + doc.page_content) > maxlen):\n+            if doc.metadata['id'] in doc_set:\n                 break\n+            if(len(context + doc.page_content) > maxlen):\n+                break\n             emb_ans = emb_ans + doc.page_content\n+            doc_set.add(doc.metadata['id'])\n             \n             \n     \n \n"
                },
                {
                    "date": 1732263784715,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,9 +45,9 @@\n             if doc.metadata['id'] in doc_set:\n                 break\n             if(len(context + doc.page_content) > maxlen):\n                 break\n-            emb_ans = emb_ans + doc.page_content\n+            context += doc.page_content\n             doc_set.add(doc.metadata['id'])\n             \n             \n     \n"
                },
                {
                    "date": 1732263802049,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,9 +34,9 @@\n     context = \"\"\n     doc_set = set()\n     for retriever in retrivers:\n         cnt = 0\n-        docs = retrivers.GetTopK(query, topk)\n+        docs = retriever.GetTopK(query, topk)\n         for doc in docs:\n             if isinstance(doc, tuple):\n                 doc = doc[0]\n             cnt += 1\n@@ -47,8 +47,9 @@\n             if(len(context + doc.page_content) > maxlen):\n                 break\n             context += doc.page_content\n             doc_set.add(doc.metadata['id'])\n+    return context\n             \n             \n     \n \n"
                },
                {
                    "date": 1732263813726,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,9 +32,9 @@\n \n def retrievers_recall(retrievers, query, topk=15, maxlen=2500, maxnum=6):\n     context = \"\"\n     doc_set = set()\n-    for retriever in retrivers:\n+    for retriever in retrievers:\n         cnt = 0\n         docs = retriever.GetTopK(query, topk)\n         for doc in docs:\n             if isinstance(doc, tuple):\n"
                },
                {
                    "date": 1732263861012,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -180,38 +180,38 @@\n         for idx, line in enumerate(jdata):\n             query = line[\"question\"]\n \n             # faiss召回topk\n-            faiss_context = faissretriever.GetTopK(query, 15)\n-            faiss_min_score = 0.0\n-            if(len(faiss_context) > 0):\n-                faiss_min_score = faiss_context[0][1]\n-            cnt = 0\n-            emb_ans = \"\"\n-            for doc, score in faiss_context:\n-                cnt =cnt + 1\n-                # 最长选择max length\n-                if(len(emb_ans + doc.page_content) > max_length):\n-                    break\n-                emb_ans = emb_ans + doc.page_content\n-                # 最多选择6个\n-                if(cnt>6):\n-                    break\n+            # faiss_context = faissretriever.GetTopK(query, 15)\n+            # faiss_min_score = 0.0\n+            # if(len(faiss_context) > 0):\n+            #     faiss_min_score = faiss_context[0][1]\n+            # cnt = 0\n+            # emb_ans = \"\"\n+            # for doc, score in faiss_context:\n+            #     cnt =cnt + 1\n+            #     # 最长选择max length\n+            #     if(len(emb_ans + doc.page_content) > max_length):\n+            #         break\n+            #     emb_ans = emb_ans + doc.page_content\n+            #     # 最多选择6个\n+            #     if(cnt>6):\n+            #         break\n \n-            # bm2.5召回topk\n-            bm25_context = bm25.GetBM25TopK(query, 15)\n-            bm25_ans = \"\"\n-            cnt = 0\n-            for doc in bm25_context:\n-                cnt = cnt + 1\n-                if(len(bm25_ans + doc.page_content) > max_length):\n-                    break\n-                bm25_ans = bm25_ans + doc.page_content\n-                if(cnt > 6):\n-                    break\n+            # # bm2.5召回topk\n+            # bm25_context = bm25.GetBM25TopK(query, 15)\n+            # bm25_ans = \"\"\n+            # cnt = 0\n+            # for doc in bm25_context:\n+            #     cnt = cnt + 1\n+            #     if(len(bm25_ans + doc.page_content) > max_length):\n+            #         break\n+            #     bm25_ans = bm25_ans + doc.page_content\n+            #     if(cnt > 6):\n+            #         break\n \n-            # 构造合并bm25召回和向量召回的prompt\n-            emb_bm25_merge_inputs = get_emb_bm25_merge(faiss_context, bm25_context, query)\n+            # # 构造合并bm25召回和向量召回的prompt\n+            # emb_bm25_merge_inputs = get_emb_bm25_merge(faiss_context, bm25_context, query)\n \n             # 构造bm25召回的prompt\n             bm25_inputs = get_rerank(bm25_ans, query)\n \n"
                },
                {
                    "date": 1732263942512,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,9 +30,9 @@\n \n     return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n \n def retrievers_recall(retrievers, query, topk=15, maxlen=2500, maxnum=6):\n-    context = \"\"\n+    merged_docs = []\n     doc_set = set()\n     for retriever in retrievers:\n         cnt = 0\n         docs = retriever.GetTopK(query, topk)\n"
                },
                {
                    "date": 1732263969284,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n                             input_variables=[\"context\", \"question\"])\n \n     return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n \n-def retrievers_recall(retrievers, query, topk=15, maxlen=2500, maxnum=6):\n+def retrievers_recall(retrievers, query, topk=15, maxnum=6):\n     merged_docs = []\n     doc_set = set()\n     for retriever in retrievers:\n         cnt = 0\n"
                },
                {
                    "date": 1732263995586,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,11 +43,9 @@\n             if(cnt>maxnum):\n                 break\n             if doc.metadata['id'] in doc_set:\n                 break\n-            if(len(context + doc.page_content) > maxlen):\n-                break\n-            context += doc.page_content\n+            merged_docs.append(doc)\n             doc_set.add(doc.metadata['id'])\n     return context\n             \n             \n"
                },
                {
                    "date": 1732264007550,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,9 +42,9 @@\n             cnt += 1\n             if(cnt>maxnum):\n                 break\n             if doc.metadata['id'] in doc_set:\n-                break\n+                continue\n             merged_docs.append(doc)\n             doc_set.add(doc.metadata['id'])\n     return context\n             \n"
                },
                {
                    "date": 1732264014365,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,9 +45,9 @@\n             if doc.metadata['id'] in doc_set:\n                 continue\n             merged_docs.append(doc)\n             doc_set.add(doc.metadata['id'])\n-    return context\n+    return merged_docs\n             \n             \n     \n \n"
                },
                {
                    "date": 1732264046059,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,10 +46,25 @@\n                 continue\n             merged_docs.append(doc)\n             doc_set.add(doc.metadata['id'])\n     return merged_docs\n+\n+def reRank(rerank, top_k, query, docs):\n+    items = []\n+    max_length = 4000\n+    for doc, score in faiss_ans:\n+        items.append(doc)\n+    items.extend(bm25_ans)\n+    rerank_ans = rerank.predict(query, items)\n+    rerank_ans = rerank_ans[:top_k]\n+    # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n+    emb_ans = \"\"\n+    for doc in rerank_ans:\n+        if(len(emb_ans + doc.page_content) > max_length):\n+            break\n+        emb_ans = emb_ans + doc.page_content\n+    return emb_ans         \n             \n-            \n     \n \n # 构造提示，根据merged faiss和bm25的召回结果返回答案\n def get_emb_bm25_merge(faiss_context, bm25_context, query):\n@@ -100,23 +115,23 @@\n \n     response = chain({\"query\": text})\n     return response\n \n-def reRank(rerank, top_k, query, bm25_ans, faiss_ans):\n-    items = []\n-    max_length = 4000\n-    for doc, score in faiss_ans:\n-        items.append(doc)\n-    items.extend(bm25_ans)\n-    rerank_ans = rerank.predict(query, items)\n-    rerank_ans = rerank_ans[:top_k]\n-    # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n-    emb_ans = \"\"\n-    for doc in rerank_ans:\n-        if(len(emb_ans + doc.page_content) > max_length):\n-            break\n-        emb_ans = emb_ans + doc.page_content\n-    return emb_ans\n+# def reRank(rerank, top_k, query, bm25_ans, faiss_ans):\n+#     items = []\n+#     max_length = 4000\n+#     for doc, score in faiss_ans:\n+#         items.append(doc)\n+#     items.extend(bm25_ans)\n+#     rerank_ans = rerank.predict(query, items)\n+#     rerank_ans = rerank_ans[:top_k]\n+#     # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n+#     emb_ans = \"\"\n+#     for doc in rerank_ans:\n+#         if(len(emb_ans + doc.page_content) > max_length):\n+#             break\n+#         emb_ans = emb_ans + doc.page_content\n+#     return emb_ans\n \n if __name__ == \"__main__\":\n \n     start = time.time()\n"
                },
                {
                    "date": 1732264058369,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -48,14 +48,9 @@\n             doc_set.add(doc.metadata['id'])\n     return merged_docs\n \n def reRank(rerank, top_k, query, docs):\n-    items = []\n-    max_length = 4000\n-    for doc, score in faiss_ans:\n-        items.append(doc)\n-    items.extend(bm25_ans)\n-    rerank_ans = rerank.predict(query, items)\n+    rerank_ans = rerank.predict(query, docs)\n     rerank_ans = rerank_ans[:top_k]\n     # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n     emb_ans = \"\"\n     for doc in rerank_ans:\n"
                },
                {
                    "date": 1732264212206,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -219,18 +219,22 @@\n \n             # # 构造合并bm25召回和向量召回的prompt\n             # emb_bm25_merge_inputs = get_emb_bm25_merge(faiss_context, bm25_context, query)\n \n-            # 构造bm25召回的prompt\n-            bm25_inputs = get_rerank(bm25_ans, query)\n+            # # 构造bm25召回的prompt\n+            # bm25_inputs = get_rerank(bm25_ans, query)\n \n-            # 构造向量召回的prompt\n-            emb_inputs = get_rerank(emb_ans, query)\n+            # # 构造向量召回的prompt\n+            # emb_inputs = get_rerank(emb_ans, query)\n \n-            # rerank召回的候选，并按照相关性得分排序\n-            rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n-            # 构造得到rerank后生成答案的prompt\n-            rerank_inputs = get_rerank(rerank_ans, query)\n+            # # rerank召回的候选，并按照相关性得分排序\n+            # rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n+            # # 构造得到rerank后生成答案的prompt\n+            # rerank_inputs = get_rerank(rerank_ans, query)\n+            \n+            docs = retrievers_recall(retrivers,query)\n+            context = reRank(rerank, 6, query, docs)\n+            inputs = get_rerank(context, query)\n \n             batch_input = []\n             batch_input.append(emb_bm25_merge_inputs)\n             batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732264220918,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -234,13 +234,13 @@\n             docs = retrievers_recall(retrivers,query)\n             context = reRank(rerank, 6, query, docs)\n             inputs = get_rerank(context, query)\n \n-            batch_input = []\n-            batch_input.append(emb_bm25_merge_inputs)\n-            batch_input.append(bm25_inputs)\n-            batch_input.append(emb_inputs)\n-            batch_input.append(rerank_inputs)\n+            batch_input = [inputs]\n+            # batch_input.append(emb_bm25_merge_inputs)\n+            # batch_input.append(bm25_inputs)\n+            # batch_input.append(emb_inputs)\n+            # batch_input.append(rerank_inputs)\n             # 执行batch推理\n             batch_output = llm.infer(batch_input)\n             line[\"answer_1\"] = batch_output[0] # 合并两路召回的结果\n             line[\"answer_2\"] = batch_output[1] # bm召回的结果\n"
                },
                {
                    "date": 1732264240319,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -242,19 +242,19 @@\n             # batch_input.append(rerank_inputs)\n             # 执行batch推理\n             batch_output = llm.infer(batch_input)\n             line[\"answer_1\"] = batch_output[0] # 合并两路召回的结果\n-            line[\"answer_2\"] = batch_output[1] # bm召回的结果\n-            line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n-            line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n-            line[\"answer_5\"] = emb_ans\n-            line[\"answer_6\"] = bm25_ans\n-            line[\"answer_7\"] = rerank_ans\n+            # line[\"answer_2\"] = batch_output[1] # bm召回的结果\n+            # line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n+            # line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n+            # line[\"answer_5\"] = emb_ans\n+            # line[\"answer_6\"] = bm25_ans\n+            # line[\"answer_7\"] = rerank_ans\n             # 如果faiss检索跟query的距离高于500，输出无答案\n-            if(faiss_min_score >500):\n-                line[\"answer_5\"] = \"无答案\"\n-            else:\n-                line[\"answer_5\"] = str(faiss_min_score)\n+            # if(faiss_min_score >500):\n+            #     line[\"answer_5\"] = \"无答案\"\n+            # else:\n+            #     line[\"answer_5\"] = str(faiss_min_score)\n \n         # 保存结果，生成submission文件\n         json.dump(jdata, open(base + \"/data/result.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n         end = time.time()\n"
                },
                {
                    "date": 1732264246133,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -241,9 +241,9 @@\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n             # 执行batch推理\n             batch_output = llm.infer(batch_input)\n-            line[\"answer_1\"] = batch_output[0] # 合并两路召回的结果\n+            line[\"answer\"] = batch_output[0] # 合并两路召回的结果\n             # line[\"answer_2\"] = batch_output[1] # bm召回的结果\n             # line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n             # line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n             # line[\"answer_5\"] = emb_ans\n"
                },
                {
                    "date": 1732265513256,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,8 +52,9 @@\n     rerank_ans = rerank.predict(query, docs)\n     rerank_ans = rerank_ans[:top_k]\n     # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n     emb_ans = \"\"\n+    \n     for doc in rerank_ans:\n         if(len(emb_ans + doc.page_content) > max_length):\n             break\n         emb_ans = emb_ans + doc.page_content\n@@ -255,7 +256,7 @@\n             # else:\n             #     line[\"answer_5\"] = str(faiss_min_score)\n \n         # 保存结果，生成submission文件\n-        json.dump(jdata, open(base + \"/data/result.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n+        json.dump(jdata, open(base + \"/data/result1.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n         end = time.time()\n         print(\"cost time: \" + str(int(end-start)/60))\n"
                },
                {
                    "date": 1732267028578,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,9 +160,9 @@\n     # Faiss召回\n     embed_models = [\"m3e\",\"bge\",\"gte\",\"bce\"]\n     # vector_store_list = []\n     for embed_model in embed_models:\n-        retrivers.append(FaissRetriever(m3e, data))\n+        retrivers.append(FaissRetriever(embed_model, data))\n         # vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     # BM25召回\n"
                }
            ],
            "date": 1732074092473,
            "name": "Commit-0",
            "content": "#!/usr/bin/env python\n# coding: utf-8\n\nimport json\nimport jieba\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom langchain.schema import Document\nfrom langchain.vectorstores import Chroma,FAISS\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain.chains import RetrievalQA\nimport time\nimport re\n\nfrom vllm_model import ChatLLM\nfrom rerank_model import reRankLLM\nfrom faiss_retriever import FaissRetriever\nfrom bm25_retriever import BM25\nfrom pdf_parse import DataProcess\n\n# 获取Langchain的工具链 \ndef get_qa_chain(llm, vector_store, prompt_template):\n\n    prompt = PromptTemplate(template=prompt_template,\n                            input_variables=[\"context\", \"question\"])\n\n    return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n\n# 构造提示，根据merged faiss和bm25的召回结果返回答案\ndef get_emb_bm25_merge(faiss_context, bm25_context, query):\n    max_length = 2500\n    emb_ans = \"\"\n    cnt = 0\n    for doc, score in faiss_context:\n        cnt =cnt + 1\n        if(cnt>6):\n            break\n        if(len(emb_ans + doc.page_content) > max_length):\n            break\n        emb_ans = emb_ans + doc.page_content\n    bm25_ans = \"\"\n    cnt = 0\n    for doc in bm25_context:\n        cnt = cnt + 1\n        if(len(bm25_ans + doc.page_content) > max_length):\n            break\n        bm25_ans = bm25_ans + doc.page_content\n        if(cnt > 6):\n            break\n\n    prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\"，不允许在答案中添加编造成分，答案请使用中文。\n                                已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                1: {emb_ans}\n                                2: {bm25_ans}\n                                问题:\n                                {question}\"\"\".format(emb_ans=emb_ans, bm25_ans = bm25_ans, question = query)\n    return prompt_template\n\n\ndef get_rerank(emb_ans, query):\n\n    prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                1: {emb_ans}\n                                问题:\n                                {question}\"\"\".format(emb_ans=emb_ans, question = query)\n    return prompt_template\n\n\ndef question(text, llm, vector_store, prompt_template):\n\n    chain = get_qa_chain(llm, vector_store, prompt_template)\n\n    response = chain({\"query\": text})\n    return response\n\ndef reRank(rerank, top_k, query, bm25_ans, faiss_ans):\n    items = []\n    max_length = 4000\n    for doc, score in faiss_ans:\n        items.append(doc)\n    items.extend(bm25_ans)\n    rerank_ans = rerank.predict(query, items)\n    rerank_ans = rerank_ans[:top_k]\n    # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n    emb_ans = \"\"\n    for doc in rerank_ans:\n        if(len(emb_ans + doc.page_content) > max_length):\n            break\n        emb_ans = emb_ans + doc.page_content\n    return emb_ans\n\nif __name__ == \"__main__\":\n\n    start = time.time()\n\n    base = \".\"\n    qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chatbase\"# + \"/pre_train_model/Qwen-7B-Chat\"\n    m3e =  base + \"/pre_train_model/m3e-large\"\n    bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n\n    # 解析pdf文档，构造数据\n    dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n    dp.ParseBlock(max_seq = 1024)\n    dp.ParseBlock(max_seq = 512)\n    print(len(dp.data))\n    dp.ParseAllPage(max_seq = 256)\n    dp.ParseAllPage(max_seq = 512)\n    print(len(dp.data))\n    dp.ParseOnePageWithRule(max_seq = 256)\n    dp.ParseOnePageWithRule(max_seq = 512)\n    print(len(dp.data))\n    data = dp.data\n    print(\"data load ok\")\n\n    # Faiss召回\n    faissretriever = FaissRetriever(m3e, data)\n    vector_store = faissretriever.vector_store\n    print(\"faissretriever load ok\")\n\n    # BM25召回\n    bm25 = BM25(data)\n    print(\"bm25 load ok\")\n\n    # LLM大模型\n    llm = ChatLLM(qwen7)\n    print(\"llm qwen load ok\")\n\n    # reRank模型\n    rerank = reRankLLM(bge_reranker_large)\n    print(\"rerank model load ok\")\n\n    # 对每一条测试问题，做答案生成处理\n    with open(base + \"/data/test_question.json\", \"r\") as f:\n        jdata = json.loads(f.read())\n        print(len(jdata))\n        max_length = 4000\n        for idx, line in enumerate(jdata):\n            query = line[\"question\"]\n\n            # faiss召回topk\n            faiss_context = faissretriever.GetTopK(query, 15)\n            faiss_min_score = 0.0\n            if(len(faiss_context) > 0):\n                faiss_min_score = faiss_context[0][1]\n            cnt = 0\n            emb_ans = \"\"\n            for doc, score in faiss_context:\n                cnt =cnt + 1\n                # 最长选择max length\n                if(len(emb_ans + doc.page_content) > max_length):\n                    break\n                emb_ans = emb_ans + doc.page_content\n                # 最多选择6个\n                if(cnt>6):\n                    break\n\n            # bm2.5召回topk\n            bm25_context = bm25.GetBM25TopK(query, 15)\n            bm25_ans = \"\"\n            cnt = 0\n            for doc in bm25_context:\n                cnt = cnt + 1\n                if(len(bm25_ans + doc.page_content) > max_length):\n                    break\n                bm25_ans = bm25_ans + doc.page_content\n                if(cnt > 6):\n                    break\n\n            # 构造合并bm25召回和向量召回的prompt\n            emb_bm25_merge_inputs = get_emb_bm25_merge(faiss_context, bm25_context, query)\n\n            # 构造bm25召回的prompt\n            bm25_inputs = get_rerank(bm25_ans, query)\n\n            # 构造向量召回的prompt\n            emb_inputs = get_rerank(emb_ans, query)\n\n            # rerank召回的候选，并按照相关性得分排序\n            rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n            # 构造得到rerank后生成答案的prompt\n            rerank_inputs = get_rerank(rerank_ans, query)\n\n            batch_input = []\n            batch_input.append(emb_bm25_merge_inputs)\n            batch_input.append(bm25_inputs)\n            batch_input.append(emb_inputs)\n            batch_input.append(rerank_inputs)\n            # 执行batch推理\n            batch_output = llm.infer(batch_input)\n            line[\"answer_1\"] = batch_output[0] # 合并两路召回的结果\n            line[\"answer_2\"] = batch_output[1] # bm召回的结果\n            line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n            line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n            line[\"answer_5\"] = emb_ans\n            line[\"answer_6\"] = bm25_ans\n            line[\"answer_7\"] = rerank_ans\n            # 如果faiss检索跟query的距离高于500，输出无答案\n            if(faiss_min_score >500):\n                line[\"answer_5\"] = \"无答案\"\n            else:\n                line[\"answer_5\"] = str(faiss_min_score)\n\n        # 保存结果，生成submission文件\n        json.dump(jdata, open(base + \"/data/result.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n        end = time.time()\n        print(\"cost time: \" + str(int(end-start)/60))\n"
        }
    ]
}
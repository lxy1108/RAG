{
    "sourceFile": "run.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 158,
            "patches": [
                {
                    "date": 1732074092473,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1732075567406,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -97,9 +97,9 @@\n \n     start = time.time()\n \n     base = \".\"\n-    qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chatbase\"# + \"/pre_train_model/Qwen-7B-Chat\"\n+    qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n \n     # 解析pdf文档，构造数据\n"
                },
                {
                    "date": 1732083713290,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,8 +18,9 @@\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n \n+os['CUDA_VISIBLE_DEVICES']=2,3\n # 获取Langchain的工具链 \n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n"
                },
                {
                    "date": 1732083809216,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,11 +17,10 @@\n from rerank_model import reRankLLM\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n-\n-os['CUDA_VISIBLE_DEVICES']=2,3\n-# 获取Langchain的工具链 \n+import os\n+os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"# 获取Langchain的工具链 \n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732083817768,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,9 +18,9 @@\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n-os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"# 获取Langchain的工具链 \n+os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732084635377,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,8 +19,9 @@\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n+os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732084652738,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,8 +101,12 @@\n     base = \".\"\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n+    \n+    # LLM大模型\n+    llm = ChatLLM(qwen7)\n+    print(\"llm qwen load ok\")\n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n     dp.ParseBlock(max_seq = 1024)\n@@ -125,11 +129,9 @@\n     # BM25召回\n     bm25 = BM25(data)\n     print(\"bm25 load ok\")\n \n-    # LLM大模型\n-    llm = ChatLLM(qwen7)\n-    print(\"llm qwen load ok\")\n+    \n \n     # reRank模型\n     rerank = reRankLLM(bge_reranker_large)\n     print(\"rerank model load ok\")\n"
                },
                {
                    "date": 1732084999903,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,8 +20,9 @@\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n+os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = =spawn\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732085006725,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = =spawn\n+os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732085016414,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n+os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732085438283,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,11 +18,11 @@\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n-os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n-os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n+# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n+# os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n+# os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732086188229,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n from pdf_parse import DataProcess\n import os\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n # os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-# os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n+os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732087573877,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,9 +19,9 @@\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n-# os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n+os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n@@ -137,9 +137,9 @@\n     rerank = reRankLLM(bge_reranker_large)\n     print(\"rerank model load ok\")\n \n     # 对每一条测试问题，做答案生成处理\n-    with open(base + \"/data/test_question.json\", \"r\") as f:\n+    with open(base + \"/data/test_question.json\", \"r\" , encoding=\"utf-8\") as f:\n         jdata = json.loads(f.read())\n         print(len(jdata))\n         max_length = 4000\n         for idx, line in enumerate(jdata):\n"
                },
                {
                    "date": 1732089879571,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,11 +103,8 @@\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n     \n-    # LLM大模型\n-    llm = ChatLLM(qwen7)\n-    print(\"llm qwen load ok\")\n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n     dp.ParseBlock(max_seq = 1024)\n@@ -130,9 +127,11 @@\n     # BM25召回\n     bm25 = BM25(data)\n     print(\"bm25 load ok\")\n \n-    \n+    # LLM大模型\n+    llm = ChatLLM(qwen7)\n+    print(\"llm qwen load ok\")\n \n     # reRank模型\n     rerank = reRankLLM(bge_reranker_large)\n     print(\"rerank model load ok\")\n"
                },
                {
                    "date": 1732157192892,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,9 +51,9 @@\n         if(cnt > 6):\n             break\n \n     prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n-                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\"，不允许在答案中添加编造成分，答案请使用中文。\n+                                如果无法从中得到答案，请说 \"无答案\"，不允许在答案中添加编造成分，答案请使用中文。\n                                 已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                 1: {emb_ans}\n                                 2: {bm25_ans}\n                                 问题:\n"
                },
                {
                    "date": 1732157561934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n \n def get_rerank(emb_ans, query):\n \n     prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n-                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n+                                如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                 已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                 1: {emb_ans}\n                                 问题:\n                                 {question}\"\"\".format(emb_ans=emb_ans, question = query)\n"
                },
                {
                    "date": 1732161157109,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -14,10 +14,10 @@\n import re\n \n from vllm_model import ChatLLM\n from rerank_model import reRankLLM\n-from faiss_retriever import FaissRetriever\n-from bm25_retriever import BM25\n+from retrievers.faiss_retriever import FaissRetriever\n+from retrievers.bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n"
                },
                {
                    "date": 1732261808904,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -119,10 +119,14 @@\n     data = dp.data\n     print(\"data load ok\")\n \n     # Faiss召回\n-    faissretriever = FaissRetriever(m3e, data)\n-    vector_store = faissretriever.vector_store\n+    embed_models = [\"m3e\",\"bge\",\"gte\"]\n+    vector_store_list = []\n+    for embed_model in embed_models:\n+        faissretriever = FaissRetriever(m3e, data)\n+        vector_store = faissretriever.vector_store\n+        vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     # BM25召回\n     bm25 = BM25(data)\n"
                },
                {
                    "date": 1732261886740,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,8 +101,12 @@\n \n     base = \".\"\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n+    reranker_dict = {\n+        \"bge\": base + \"/pre_train_model/bge-reranker-large\"\n+        \"gte\"\n+    }\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n     \n \n     # 解析pdf文档，构造数据\n@@ -119,13 +123,12 @@\n     data = dp.data\n     print(\"data load ok\")\n \n     # Faiss召回\n-    embed_models = [\"m3e\",\"bge\",\"gte\"]\n+    embed_models = [\"m3e\",\"bge\",\"gte\",\"bce\"]\n     vector_store_list = []\n     for embed_model in embed_models:\n         faissretriever = FaissRetriever(m3e, data)\n-        vector_store = faissretriever.vector_store\n         vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     # BM25召回\n"
                },
                {
                    "date": 1732261911984,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,9 +103,9 @@\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     reranker_dict = {\n         \"bge\": base + \"/pre_train_model/bge-reranker-large\"\n-        \"gte\"\n+        \"bce\": base + \"/pre_train_model/bce-reranker-base\"\n     }\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n     \n \n"
                },
                {
                    "date": 1732261917278,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,12 +102,11 @@\n     base = \".\"\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     reranker_dict = {\n-        \"bge\": base + \"/pre_train_model/bge-reranker-large\"\n+        \"bge\": base + \"/pre_train_model/bge-reranker-large\",\n         \"bce\": base + \"/pre_train_model/bce-reranker-base\"\n     }\n-    bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n     \n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n"
                },
                {
                    "date": 1732261935772,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -105,8 +105,9 @@\n     reranker_dict = {\n         \"bge\": base + \"/pre_train_model/bge-reranker-large\",\n         \"bce\": base + \"/pre_train_model/bce-reranker-base\"\n     }\n+    reranker_name = \"bge\"\n     \n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n@@ -138,9 +139,9 @@\n     llm = ChatLLM(qwen7)\n     print(\"llm qwen load ok\")\n \n     # reRank模型\n-    rerank = reRankLLM(bge_reranker_large)\n+    rerank = reRankLLM(reranker_dict[])\n     print(\"rerank model load ok\")\n \n     # 对每一条测试问题，做答案生成处理\n     with open(base + \"/data/test_question.json\", \"r\" , encoding=\"utf-8\") as f:\n"
                },
                {
                    "date": 1732261947348,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -139,9 +139,9 @@\n     llm = ChatLLM(qwen7)\n     print(\"llm qwen load ok\")\n \n     # reRank模型\n-    rerank = reRankLLM(reranker_dict[])\n+    rerank = reRankLLM(reranker_dict[\"bge\"])\n     print(\"rerank model load ok\")\n \n     # 对每一条测试问题，做答案生成处理\n     with open(base + \"/data/test_question.json\", \"r\" , encoding=\"utf-8\") as f:\n"
                },
                {
                    "date": 1732262002155,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,8 +16,9 @@\n from vllm_model import ChatLLM\n from rerank_model import reRankLLM\n from retrievers.faiss_retriever import FaissRetriever\n from retrievers.bm25_retriever import BM25\n+from retrievers.tfidf_retriever import TFIDF\n from pdf_parse import DataProcess\n import os\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n@@ -133,8 +134,11 @@\n \n     # BM25召回\n     bm25 = BM25(data)\n     print(\"bm25 load ok\")\n+    \n+    tfidf = TFIDF(data)\n+    print(\"tfidf load ok\")\n \n     # LLM大模型\n     llm = ChatLLM(qwen7)\n     print(\"llm qwen load ok\")\n"
                },
                {
                    "date": 1732262012899,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -135,8 +135,9 @@\n     # BM25召回\n     bm25 = BM25(data)\n     print(\"bm25 load ok\")\n     \n+    # TFIDF召回\n     tfidf = TFIDF(data)\n     print(\"tfidf load ok\")\n \n     # LLM大模型\n"
                },
                {
                    "date": 1732262735248,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -123,11 +123,12 @@\n     print(len(dp.data))\n     data = dp.data\n     print(\"data load ok\")\n \n+    retrivers = []\n     # Faiss召回\n     embed_models = [\"m3e\",\"bge\",\"gte\",\"bce\"]\n-    vector_store_list = []\n+    # vector_store_list = []\n     for embed_model in embed_models:\n         faissretriever = FaissRetriever(m3e, data)\n         vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n"
                },
                {
                    "date": 1732262749859,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -128,10 +128,10 @@\n     # Faiss召回\n     embed_models = [\"m3e\",\"bge\",\"gte\",\"bce\"]\n     # vector_store_list = []\n     for embed_model in embed_models:\n-        faissretriever = FaissRetriever(m3e, data)\n-        vector_store_list.append(faissretriever.vector_store)\n+        retrivers.append(FaissRetriever(m3e, data))\n+        # vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     # BM25召回\n     bm25 = BM25(data)\n"
                },
                {
                    "date": 1732262780069,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,13 +133,13 @@\n         # vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     # BM25召回\n-    bm25 = BM25(data)\n+    retrivers.append(BM25(data))\n     print(\"bm25 load ok\")\n     \n     # TFIDF召回\n-    tfidf = TFIDF(data)\n+    retrivers.append(TFIDF(data))\n     print(\"tfidf load ok\")\n \n     # LLM大模型\n     llm = ChatLLM(qwen7)\n"
                },
                {
                    "date": 1732263472754,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,8 +29,13 @@\n                             input_variables=[\"context\", \"question\"])\n \n     return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n \n+def retrievers_recall(retrievers, query, topk=15, maxlen=2500):\n+    context = \"\"\n+    for retriever in retrivers:\n+    \n+\n # 构造提示，根据merged faiss和bm25的召回结果返回答案\n def get_emb_bm25_merge(faiss_context, bm25_context, query):\n     max_length = 2500\n     emb_ans = \"\"\n"
                },
                {
                    "date": 1732263635242,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,8 +32,22 @@\n \n def retrievers_recall(retrievers, query, topk=15, maxlen=2500):\n     context = \"\"\n     for retriever in retrivers:\n+        cur_context = \"\"\n+        cnt = 0\n+        docs = retrivers.GetTopK(query, topk)\n+        for doc in docs:\n+            if isinstance(doc, tuple):\n+                doc = doc[0]\n+            cnt += 1\n+            if(cnt>6):\n+                break\n+            if(len(emb_ans + doc.page_content) > max_length):\n+                break\n+            emb_ans = emb_ans + doc.page_content\n+            \n+            \n     \n \n # 构造提示，根据merged faiss和bm25的召回结果返回答案\n def get_emb_bm25_merge(faiss_context, bm25_context, query):\n"
                },
                {
                    "date": 1732263643267,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n                             input_variables=[\"context\", \"question\"])\n \n     return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n \n-def retrievers_recall(retrievers, query, topk=15, maxlen=2500):\n+def retrievers_recall(retrievers, query, topk=15, maxlen=2500, maxnum=6):\n     context = \"\"\n     for retriever in retrivers:\n         cur_context = \"\"\n         cnt = 0\n"
                },
                {
                    "date": 1732263651579,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,11 +39,11 @@\n         for doc in docs:\n             if isinstance(doc, tuple):\n                 doc = doc[0]\n             cnt += 1\n-            if(cnt>6):\n+            if(cnt>maxnum):\n                 break\n-            if(len(emb_ans + doc.page_content) > max_length):\n+            if(len(cur_context + doc.page_content) > max_length):\n                 break\n             emb_ans = emb_ans + doc.page_content\n             \n             \n"
                },
                {
                    "date": 1732263658408,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -41,9 +41,9 @@\n                 doc = doc[0]\n             cnt += 1\n             if(cnt>maxnum):\n                 break\n-            if(len(cur_context + doc.page_content) > max_length):\n+            if(len(cur_context + doc.page_content) > maxlen):\n                 break\n             emb_ans = emb_ans + doc.page_content\n             \n             \n"
                },
                {
                    "date": 1732263774647,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -31,21 +31,24 @@\n     return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n \n def retrievers_recall(retrievers, query, topk=15, maxlen=2500, maxnum=6):\n     context = \"\"\n+    doc_set = set()\n     for retriever in retrivers:\n-        cur_context = \"\"\n         cnt = 0\n         docs = retrivers.GetTopK(query, topk)\n         for doc in docs:\n             if isinstance(doc, tuple):\n                 doc = doc[0]\n             cnt += 1\n             if(cnt>maxnum):\n                 break\n-            if(len(cur_context + doc.page_content) > maxlen):\n+            if doc.metadata['id'] in doc_set:\n                 break\n+            if(len(context + doc.page_content) > maxlen):\n+                break\n             emb_ans = emb_ans + doc.page_content\n+            doc_set.add(doc.metadata['id'])\n             \n             \n     \n \n"
                },
                {
                    "date": 1732263784715,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,9 +45,9 @@\n             if doc.metadata['id'] in doc_set:\n                 break\n             if(len(context + doc.page_content) > maxlen):\n                 break\n-            emb_ans = emb_ans + doc.page_content\n+            context += doc.page_content\n             doc_set.add(doc.metadata['id'])\n             \n             \n     \n"
                },
                {
                    "date": 1732263802049,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -34,9 +34,9 @@\n     context = \"\"\n     doc_set = set()\n     for retriever in retrivers:\n         cnt = 0\n-        docs = retrivers.GetTopK(query, topk)\n+        docs = retriever.GetTopK(query, topk)\n         for doc in docs:\n             if isinstance(doc, tuple):\n                 doc = doc[0]\n             cnt += 1\n@@ -47,8 +47,9 @@\n             if(len(context + doc.page_content) > maxlen):\n                 break\n             context += doc.page_content\n             doc_set.add(doc.metadata['id'])\n+    return context\n             \n             \n     \n \n"
                },
                {
                    "date": 1732263813726,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -32,9 +32,9 @@\n \n def retrievers_recall(retrievers, query, topk=15, maxlen=2500, maxnum=6):\n     context = \"\"\n     doc_set = set()\n-    for retriever in retrivers:\n+    for retriever in retrievers:\n         cnt = 0\n         docs = retriever.GetTopK(query, topk)\n         for doc in docs:\n             if isinstance(doc, tuple):\n"
                },
                {
                    "date": 1732263861012,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -180,38 +180,38 @@\n         for idx, line in enumerate(jdata):\n             query = line[\"question\"]\n \n             # faiss召回topk\n-            faiss_context = faissretriever.GetTopK(query, 15)\n-            faiss_min_score = 0.0\n-            if(len(faiss_context) > 0):\n-                faiss_min_score = faiss_context[0][1]\n-            cnt = 0\n-            emb_ans = \"\"\n-            for doc, score in faiss_context:\n-                cnt =cnt + 1\n-                # 最长选择max length\n-                if(len(emb_ans + doc.page_content) > max_length):\n-                    break\n-                emb_ans = emb_ans + doc.page_content\n-                # 最多选择6个\n-                if(cnt>6):\n-                    break\n+            # faiss_context = faissretriever.GetTopK(query, 15)\n+            # faiss_min_score = 0.0\n+            # if(len(faiss_context) > 0):\n+            #     faiss_min_score = faiss_context[0][1]\n+            # cnt = 0\n+            # emb_ans = \"\"\n+            # for doc, score in faiss_context:\n+            #     cnt =cnt + 1\n+            #     # 最长选择max length\n+            #     if(len(emb_ans + doc.page_content) > max_length):\n+            #         break\n+            #     emb_ans = emb_ans + doc.page_content\n+            #     # 最多选择6个\n+            #     if(cnt>6):\n+            #         break\n \n-            # bm2.5召回topk\n-            bm25_context = bm25.GetBM25TopK(query, 15)\n-            bm25_ans = \"\"\n-            cnt = 0\n-            for doc in bm25_context:\n-                cnt = cnt + 1\n-                if(len(bm25_ans + doc.page_content) > max_length):\n-                    break\n-                bm25_ans = bm25_ans + doc.page_content\n-                if(cnt > 6):\n-                    break\n+            # # bm2.5召回topk\n+            # bm25_context = bm25.GetBM25TopK(query, 15)\n+            # bm25_ans = \"\"\n+            # cnt = 0\n+            # for doc in bm25_context:\n+            #     cnt = cnt + 1\n+            #     if(len(bm25_ans + doc.page_content) > max_length):\n+            #         break\n+            #     bm25_ans = bm25_ans + doc.page_content\n+            #     if(cnt > 6):\n+            #         break\n \n-            # 构造合并bm25召回和向量召回的prompt\n-            emb_bm25_merge_inputs = get_emb_bm25_merge(faiss_context, bm25_context, query)\n+            # # 构造合并bm25召回和向量召回的prompt\n+            # emb_bm25_merge_inputs = get_emb_bm25_merge(faiss_context, bm25_context, query)\n \n             # 构造bm25召回的prompt\n             bm25_inputs = get_rerank(bm25_ans, query)\n \n"
                },
                {
                    "date": 1732263942512,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,9 +30,9 @@\n \n     return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n \n def retrievers_recall(retrievers, query, topk=15, maxlen=2500, maxnum=6):\n-    context = \"\"\n+    merged_docs = []\n     doc_set = set()\n     for retriever in retrievers:\n         cnt = 0\n         docs = retriever.GetTopK(query, topk)\n"
                },
                {
                    "date": 1732263969284,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n                             input_variables=[\"context\", \"question\"])\n \n     return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n \n-def retrievers_recall(retrievers, query, topk=15, maxlen=2500, maxnum=6):\n+def retrievers_recall(retrievers, query, topk=15, maxnum=6):\n     merged_docs = []\n     doc_set = set()\n     for retriever in retrievers:\n         cnt = 0\n"
                },
                {
                    "date": 1732263995586,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -43,11 +43,9 @@\n             if(cnt>maxnum):\n                 break\n             if doc.metadata['id'] in doc_set:\n                 break\n-            if(len(context + doc.page_content) > maxlen):\n-                break\n-            context += doc.page_content\n+            merged_docs.append(doc)\n             doc_set.add(doc.metadata['id'])\n     return context\n             \n             \n"
                },
                {
                    "date": 1732264007550,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -42,9 +42,9 @@\n             cnt += 1\n             if(cnt>maxnum):\n                 break\n             if doc.metadata['id'] in doc_set:\n-                break\n+                continue\n             merged_docs.append(doc)\n             doc_set.add(doc.metadata['id'])\n     return context\n             \n"
                },
                {
                    "date": 1732264014365,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -45,9 +45,9 @@\n             if doc.metadata['id'] in doc_set:\n                 continue\n             merged_docs.append(doc)\n             doc_set.add(doc.metadata['id'])\n-    return context\n+    return merged_docs\n             \n             \n     \n \n"
                },
                {
                    "date": 1732264046059,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -46,10 +46,25 @@\n                 continue\n             merged_docs.append(doc)\n             doc_set.add(doc.metadata['id'])\n     return merged_docs\n+\n+def reRank(rerank, top_k, query, docs):\n+    items = []\n+    max_length = 4000\n+    for doc, score in faiss_ans:\n+        items.append(doc)\n+    items.extend(bm25_ans)\n+    rerank_ans = rerank.predict(query, items)\n+    rerank_ans = rerank_ans[:top_k]\n+    # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n+    emb_ans = \"\"\n+    for doc in rerank_ans:\n+        if(len(emb_ans + doc.page_content) > max_length):\n+            break\n+        emb_ans = emb_ans + doc.page_content\n+    return emb_ans         \n             \n-            \n     \n \n # 构造提示，根据merged faiss和bm25的召回结果返回答案\n def get_emb_bm25_merge(faiss_context, bm25_context, query):\n@@ -100,23 +115,23 @@\n \n     response = chain({\"query\": text})\n     return response\n \n-def reRank(rerank, top_k, query, bm25_ans, faiss_ans):\n-    items = []\n-    max_length = 4000\n-    for doc, score in faiss_ans:\n-        items.append(doc)\n-    items.extend(bm25_ans)\n-    rerank_ans = rerank.predict(query, items)\n-    rerank_ans = rerank_ans[:top_k]\n-    # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n-    emb_ans = \"\"\n-    for doc in rerank_ans:\n-        if(len(emb_ans + doc.page_content) > max_length):\n-            break\n-        emb_ans = emb_ans + doc.page_content\n-    return emb_ans\n+# def reRank(rerank, top_k, query, bm25_ans, faiss_ans):\n+#     items = []\n+#     max_length = 4000\n+#     for doc, score in faiss_ans:\n+#         items.append(doc)\n+#     items.extend(bm25_ans)\n+#     rerank_ans = rerank.predict(query, items)\n+#     rerank_ans = rerank_ans[:top_k]\n+#     # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n+#     emb_ans = \"\"\n+#     for doc in rerank_ans:\n+#         if(len(emb_ans + doc.page_content) > max_length):\n+#             break\n+#         emb_ans = emb_ans + doc.page_content\n+#     return emb_ans\n \n if __name__ == \"__main__\":\n \n     start = time.time()\n"
                },
                {
                    "date": 1732264058369,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -48,14 +48,9 @@\n             doc_set.add(doc.metadata['id'])\n     return merged_docs\n \n def reRank(rerank, top_k, query, docs):\n-    items = []\n-    max_length = 4000\n-    for doc, score in faiss_ans:\n-        items.append(doc)\n-    items.extend(bm25_ans)\n-    rerank_ans = rerank.predict(query, items)\n+    rerank_ans = rerank.predict(query, docs)\n     rerank_ans = rerank_ans[:top_k]\n     # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n     emb_ans = \"\"\n     for doc in rerank_ans:\n"
                },
                {
                    "date": 1732264212206,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -219,18 +219,22 @@\n \n             # # 构造合并bm25召回和向量召回的prompt\n             # emb_bm25_merge_inputs = get_emb_bm25_merge(faiss_context, bm25_context, query)\n \n-            # 构造bm25召回的prompt\n-            bm25_inputs = get_rerank(bm25_ans, query)\n+            # # 构造bm25召回的prompt\n+            # bm25_inputs = get_rerank(bm25_ans, query)\n \n-            # 构造向量召回的prompt\n-            emb_inputs = get_rerank(emb_ans, query)\n+            # # 构造向量召回的prompt\n+            # emb_inputs = get_rerank(emb_ans, query)\n \n-            # rerank召回的候选，并按照相关性得分排序\n-            rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n-            # 构造得到rerank后生成答案的prompt\n-            rerank_inputs = get_rerank(rerank_ans, query)\n+            # # rerank召回的候选，并按照相关性得分排序\n+            # rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n+            # # 构造得到rerank后生成答案的prompt\n+            # rerank_inputs = get_rerank(rerank_ans, query)\n+            \n+            docs = retrievers_recall(retrivers,query)\n+            context = reRank(rerank, 6, query, docs)\n+            inputs = get_rerank(context, query)\n \n             batch_input = []\n             batch_input.append(emb_bm25_merge_inputs)\n             batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732264220918,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -234,13 +234,13 @@\n             docs = retrievers_recall(retrivers,query)\n             context = reRank(rerank, 6, query, docs)\n             inputs = get_rerank(context, query)\n \n-            batch_input = []\n-            batch_input.append(emb_bm25_merge_inputs)\n-            batch_input.append(bm25_inputs)\n-            batch_input.append(emb_inputs)\n-            batch_input.append(rerank_inputs)\n+            batch_input = [inputs]\n+            # batch_input.append(emb_bm25_merge_inputs)\n+            # batch_input.append(bm25_inputs)\n+            # batch_input.append(emb_inputs)\n+            # batch_input.append(rerank_inputs)\n             # 执行batch推理\n             batch_output = llm.infer(batch_input)\n             line[\"answer_1\"] = batch_output[0] # 合并两路召回的结果\n             line[\"answer_2\"] = batch_output[1] # bm召回的结果\n"
                },
                {
                    "date": 1732264240319,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -242,19 +242,19 @@\n             # batch_input.append(rerank_inputs)\n             # 执行batch推理\n             batch_output = llm.infer(batch_input)\n             line[\"answer_1\"] = batch_output[0] # 合并两路召回的结果\n-            line[\"answer_2\"] = batch_output[1] # bm召回的结果\n-            line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n-            line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n-            line[\"answer_5\"] = emb_ans\n-            line[\"answer_6\"] = bm25_ans\n-            line[\"answer_7\"] = rerank_ans\n+            # line[\"answer_2\"] = batch_output[1] # bm召回的结果\n+            # line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n+            # line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n+            # line[\"answer_5\"] = emb_ans\n+            # line[\"answer_6\"] = bm25_ans\n+            # line[\"answer_7\"] = rerank_ans\n             # 如果faiss检索跟query的距离高于500，输出无答案\n-            if(faiss_min_score >500):\n-                line[\"answer_5\"] = \"无答案\"\n-            else:\n-                line[\"answer_5\"] = str(faiss_min_score)\n+            # if(faiss_min_score >500):\n+            #     line[\"answer_5\"] = \"无答案\"\n+            # else:\n+            #     line[\"answer_5\"] = str(faiss_min_score)\n \n         # 保存结果，生成submission文件\n         json.dump(jdata, open(base + \"/data/result.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n         end = time.time()\n"
                },
                {
                    "date": 1732264246133,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -241,9 +241,9 @@\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n             # 执行batch推理\n             batch_output = llm.infer(batch_input)\n-            line[\"answer_1\"] = batch_output[0] # 合并两路召回的结果\n+            line[\"answer\"] = batch_output[0] # 合并两路召回的结果\n             # line[\"answer_2\"] = batch_output[1] # bm召回的结果\n             # line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n             # line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n             # line[\"answer_5\"] = emb_ans\n"
                },
                {
                    "date": 1732265513256,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -52,8 +52,9 @@\n     rerank_ans = rerank.predict(query, docs)\n     rerank_ans = rerank_ans[:top_k]\n     # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n     emb_ans = \"\"\n+    \n     for doc in rerank_ans:\n         if(len(emb_ans + doc.page_content) > max_length):\n             break\n         emb_ans = emb_ans + doc.page_content\n@@ -255,7 +256,7 @@\n             # else:\n             #     line[\"answer_5\"] = str(faiss_min_score)\n \n         # 保存结果，生成submission文件\n-        json.dump(jdata, open(base + \"/data/result.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n+        json.dump(jdata, open(base + \"/data/result1.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n         end = time.time()\n         print(\"cost time: \" + str(int(end-start)/60))\n"
                },
                {
                    "date": 1732267028578,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -160,9 +160,9 @@\n     # Faiss召回\n     embed_models = [\"m3e\",\"bge\",\"gte\",\"bce\"]\n     # vector_store_list = []\n     for embed_model in embed_models:\n-        retrivers.append(FaissRetriever(m3e, data))\n+        retrivers.append(FaissRetriever(embed_model, data))\n         # vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     # BM25召回\n"
                },
                {
                    "date": 1732430960759,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -139,8 +139,11 @@\n         \"bge\": base + \"/pre_train_model/bge-reranker-large\",\n         \"bce\": base + \"/pre_train_model/bce-reranker-base\"\n     }\n     reranker_name = \"bge\"\n+    llm_dict = {\n+        'qwen': \"\"\n+    }\n     \n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n"
                },
                {
                    "date": 1732431020032,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -140,9 +140,11 @@\n         \"bce\": base + \"/pre_train_model/bce-reranker-base\"\n     }\n     reranker_name = \"bge\"\n     llm_dict = {\n-        'qwen': \"\"\n+        \"qwen\": \"pre_train_model/Qwen1.5-7B-Chat\"\n+        \"baichuan\": \"pre_train_model/Baichuan2-7B-Chat\",\n+        \"chatglm\": \"pre_train_model/chatglm3-6b\"\n     }\n     \n \n     # 解析pdf文档，构造数据\n"
                },
                {
                    "date": 1732431041927,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -140,11 +140,11 @@\n         \"bce\": base + \"/pre_train_model/bce-reranker-base\"\n     }\n     reranker_name = \"bge\"\n     llm_dict = {\n-        \"qwen\": \"pre_train_model/Qwen1.5-7B-Chat\"\n-        \"baichuan\": \"pre_train_model/Baichuan2-7B-Chat\",\n-        \"chatglm\": \"pre_train_model/chatglm3-6b\"\n+        \"qwen\": (\"pre_train_model/Qwen1.5-7B-Chat\"),\n+        \"baichuan\": (\"pre_train_model/Baichuan2-7B-Chat\"),\n+        \"chatglm\": (\"pre_train_model/chatglm3-6b\")\n     }\n     \n \n     # 解析pdf文档，构造数据\n"
                },
                {
                    "date": 1732431106555,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n from langchain.chains import RetrievalQA\n import time\n import re\n \n-from vllm_model import ChatLLM\n+from vllm_model import ChatLLM, Baichuan\n from rerank_model import reRankLLM\n from retrievers.faiss_retriever import FaissRetriever\n from retrievers.bm25_retriever import BM25\n from retrievers.tfidf_retriever import TFIDF\n@@ -140,11 +140,11 @@\n         \"bce\": base + \"/pre_train_model/bce-reranker-base\"\n     }\n     reranker_name = \"bge\"\n     llm_dict = {\n-        \"qwen\": (\"pre_train_model/Qwen1.5-7B-Chat\"),\n-        \"baichuan\": (\"pre_train_model/Baichuan2-7B-Chat\"),\n-        \"chatglm\": (\"pre_train_model/chatglm3-6b\")\n+        \"qwen\": (ChatLLM, \"pre_train_model/Qwen1.5-7B-Chat\"),\n+        \"baichuan\": (ChatLLM, \"pre_train_model/Baichuan2-7B-Chat\"),\n+        \"chatglm\": (Baichuan, \"pre_train_model/chatglm3-6b\")\n     }\n     \n \n     # 解析pdf文档，构造数据\n"
                },
                {
                    "date": 1732431166946,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -145,8 +145,9 @@\n         \"baichuan\": (ChatLLM, \"pre_train_model/Baichuan2-7B-Chat\"),\n         \"chatglm\": (Baichuan, \"pre_train_model/chatglm3-6b\")\n     }\n     \n+    \n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n     dp.ParseBlock(max_seq = 1024)\n@@ -178,9 +179,11 @@\n     retrivers.append(TFIDF(data))\n     print(\"tfidf load ok\")\n \n     # LLM大模型\n-    llm = ChatLLM(qwen7)\n+    llm_name = \"baichuan\"\n+    llm_model, llm_path = llm_dict[llm_name]\n+    llm = llm_model(llm_path)\n     print(\"llm qwen load ok\")\n \n     # reRank模型\n     rerank = reRankLLM(reranker_dict[\"bge\"])\n"
                },
                {
                    "date": 1732431174172,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -182,9 +182,9 @@\n     # LLM大模型\n     llm_name = \"baichuan\"\n     llm_model, llm_path = llm_dict[llm_name]\n     llm = llm_model(llm_path)\n-    print(\"llm qwen load ok\")\n+    print(\"llm {llm_name} load ok\")\n \n     # reRank模型\n     rerank = reRankLLM(reranker_dict[\"bge\"])\n     print(\"rerank model load ok\")\n"
                },
                {
                    "date": 1732670758499,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -5,8 +5,9 @@\n import jieba\n import pandas as pd\n import numpy as np\n from tqdm import tqdm\n+import argparse\n from langchain.schema import Document\n from langchain.vectorstores import Chroma,FAISS\n from langchain import PromptTemplate, LLMChain\n from langchain.chains import RetrievalQA\n"
                },
                {
                    "date": 1732671448403,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,8 +23,9 @@\n import os\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n+\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n@@ -129,9 +130,15 @@\n #         emb_ans = emb_ans + doc.page_content\n #     return emb_ans\n \n if __name__ == \"__main__\":\n+    \n+    parser = argparse.ArgumentParser(description='My RAG System')\n+    parser.add_argument('--llm_path', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model path')\n+    parser.add_argument('--reranker_path', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Reranker model path')\n+    \n \n+\n     start = time.time()\n \n     base = \".\"\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n"
                },
                {
                    "date": 1732671790362,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -129,24 +129,42 @@\n #             break\n #         emb_ans = emb_ans + doc.page_content\n #     return emb_ans\n \n+# 解析pdf文档，构造数据\n+def parse_pdf(file):\n+    dp =  DataProcess(pdf_path = file)\n+    dp.ParseBlock(max_seq = 1024)\n+    dp.ParseBlock(max_seq = 512)\n+    print(len(dp.data))\n+    dp.ParseAllPage(max_seq = 256)\n+    dp.ParseAllPage(max_seq = 512)\n+    print(len(dp.data))\n+    dp.ParseOnePageWithRule(max_seq = 256)\n+    dp.ParseOnePageWithRule(max_seq = 512)\n+    print(len(dp.data))\n+    data = dp.data\n+    print(\"data load ok\")\n+    return data\n+    \n+\n if __name__ == \"__main__\":\n     \n     parser = argparse.ArgumentParser(description='My RAG System')\n-    parser.add_argument('--llm_path', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model path')\n-    parser.add_argument('--reranker_path', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Reranker model path')\n+    parser.add_argument('--document_path', type=str, required=True, default='data/train_a.pdf', help='Document path')\n     \n+    parser.add_argument('--llm_name', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n+    parser.add_argument('--reranker_name', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Reranker model name')\n+    parser.add_argument('--dense_retriever_lists', type=str, required=True, default='m3e,bge,gte,bce\"', help='Dense retriever model lists')\n \n+    args = parser.parse_args()\n \n     start = time.time()\n \n-    base = \".\"\n-    qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     reranker_dict = {\n-        \"bge\": base + \"/pre_train_model/bge-reranker-large\",\n-        \"bce\": base + \"/pre_train_model/bce-reranker-base\"\n+        \"bge\": \"pre_train_model/bge-reranker-large\",\n+        \"bce\": \"pre_train_model/bce-reranker-base\"\n     }\n     reranker_name = \"bge\"\n     llm_dict = {\n         \"qwen\": (ChatLLM, \"pre_train_model/Qwen1.5-7B-Chat\"),\n@@ -154,23 +172,10 @@\n         \"chatglm\": (Baichuan, \"pre_train_model/chatglm3-6b\")\n     }\n     \n     \n+    data =  parse_pdf(args.document_path)\n \n-    # 解析pdf文档，构造数据\n-    dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n-    dp.ParseBlock(max_seq = 1024)\n-    dp.ParseBlock(max_seq = 512)\n-    print(len(dp.data))\n-    dp.ParseAllPage(max_seq = 256)\n-    dp.ParseAllPage(max_seq = 512)\n-    print(len(dp.data))\n-    dp.ParseOnePageWithRule(max_seq = 256)\n-    dp.ParseOnePageWithRule(max_seq = 512)\n-    print(len(dp.data))\n-    data = dp.data\n-    print(\"data load ok\")\n-\n     retrivers = []\n     # Faiss召回\n     embed_models = [\"m3e\",\"bge\",\"gte\",\"bce\"]\n     # vector_store_list = []\n"
                },
                {
                    "date": 1732671808211,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -159,9 +159,8 @@\n     args = parser.parse_args()\n \n     start = time.time()\n \n-    m3e =  base + \"/pre_train_model/m3e-large\"\n     reranker_dict = {\n         \"bge\": \"pre_train_model/bge-reranker-large\",\n         \"bce\": \"pre_train_model/bce-reranker-base\"\n     }\n"
                },
                {
                    "date": 1732671910879,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,11 +151,11 @@\n     \n     parser = argparse.ArgumentParser(description='My RAG System')\n     parser.add_argument('--document_path', type=str, required=True, default='data/train_a.pdf', help='Document path')\n     \n+    parser.add_argument('--dense_retriever_lists', type=str, required=True, default='m3e,bge,gte,bce\"', help='Dense retriever model lists')\n+    parser.add_argument('--reranker_name', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Reranker model name')\n     parser.add_argument('--llm_name', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n-    parser.add_argument('--reranker_name', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Reranker model name')\n-    parser.add_argument('--dense_retriever_lists', type=str, required=True, default='m3e,bge,gte,bce\"', help='Dense retriever model lists')\n \n     args = parser.parse_args()\n \n     start = time.time()\n@@ -163,9 +163,10 @@\n     reranker_dict = {\n         \"bge\": \"pre_train_model/bge-reranker-large\",\n         \"bce\": \"pre_train_model/bce-reranker-base\"\n     }\n-    reranker_name = \"bge\"\n+    assert args.reranker_name in reranker_dict.keys(), \"Unknown reranker model name.\"\n+    \n     llm_dict = {\n         \"qwen\": (ChatLLM, \"pre_train_model/Qwen1.5-7B-Chat\"),\n         \"baichuan\": (ChatLLM, \"pre_train_model/Baichuan2-7B-Chat\"),\n         \"chatglm\": (Baichuan, \"pre_train_model/chatglm3-6b\")\n"
                },
                {
                    "date": 1732671947169,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -170,8 +170,9 @@\n         \"qwen\": (ChatLLM, \"pre_train_model/Qwen1.5-7B-Chat\"),\n         \"baichuan\": (ChatLLM, \"pre_train_model/Baichuan2-7B-Chat\"),\n         \"chatglm\": (Baichuan, \"pre_train_model/chatglm3-6b\")\n     }\n+    assert args.llm_name in llm_dict.keys(), \"Unknown LLM model name\"\n     \n     \n     data =  parse_pdf(args.document_path)\n \n"
                },
                {
                    "date": 1732672146836,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -158,8 +158,15 @@\n \n     args = parser.parse_args()\n \n     start = time.time()\n+    \n+    dense_retriever_names = args.dense_retriever_lists.split(',')\n+    dense_retrievers = []\n+    for embed_model in args.dense_retriever_lists.split(','):\n+        retrivers.append(FaissRetriever(embed_model, data))\n+        # vector_store_list.append(faissretriever.vector_store)\n+    print(\"faissretriever load ok\")\n \n     reranker_dict = {\n         \"bge\": \"pre_train_model/bge-reranker-large\",\n         \"bce\": \"pre_train_model/bce-reranker-base\"\n"
                },
                {
                    "date": 1732672172463,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -159,12 +159,14 @@\n     args = parser.parse_args()\n \n     start = time.time()\n     \n+    data =  parse_pdf(args.document_path)\n+    \n     dense_retriever_names = args.dense_retriever_lists.split(',')\n     dense_retrievers = []\n     for embed_model in args.dense_retriever_lists.split(','):\n-        retrivers.append(FaissRetriever(embed_model, data))\n+        dense_retrievers.append(FaissRetriever(embed_model, data))\n         # vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     reranker_dict = {\n@@ -180,9 +182,9 @@\n     }\n     assert args.llm_name in llm_dict.keys(), \"Unknown LLM model name\"\n     \n     \n-    data =  parse_pdf(args.document_path)\n+    \n \n     retrivers = []\n     # Faiss召回\n     embed_models = [\"m3e\",\"bge\",\"gte\",\"bce\"]\n"
                },
                {
                    "date": 1732672201442,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -165,23 +165,22 @@\n     dense_retriever_names = args.dense_retriever_lists.split(',')\n     dense_retrievers = []\n     for embed_model in args.dense_retriever_lists.split(','):\n         dense_retrievers.append(FaissRetriever(embed_model, data))\n-        # vector_store_list.append(faissretriever.vector_store)\n     print(\"faissretriever load ok\")\n \n     reranker_dict = {\n         \"bge\": \"pre_train_model/bge-reranker-large\",\n         \"bce\": \"pre_train_model/bce-reranker-base\"\n     }\n-    assert args.reranker_name in reranker_dict.keys(), \"Unknown reranker model name.\"\n+    assert args.reranker_name in reranker_dict.keys(), \"Unknown reranker model name: {args.reranker_name}.\"\n     \n     llm_dict = {\n         \"qwen\": (ChatLLM, \"pre_train_model/Qwen1.5-7B-Chat\"),\n         \"baichuan\": (ChatLLM, \"pre_train_model/Baichuan2-7B-Chat\"),\n         \"chatglm\": (Baichuan, \"pre_train_model/chatglm3-6b\")\n     }\n-    assert args.llm_name in llm_dict.keys(), \"Unknown LLM model name\"\n+    assert args.llm_name in llm_dict.keys(), \"Unknown LLM model name: {args.llm_name}.\"\n     \n     \n     \n \n"
                },
                {
                    "date": 1732672234854,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -162,11 +162,11 @@\n     \n     data =  parse_pdf(args.document_path)\n     \n     dense_retriever_names = args.dense_retriever_lists.split(',')\n-    dense_retrievers = []\n+    retrievers = []\n     for embed_model in args.dense_retriever_lists.split(','):\n-        dense_retrievers.append(FaissRetriever(embed_model, data))\n+        retrievers.append(FaissRetriever(embed_model, data))\n     print(\"faissretriever load ok\")\n \n     reranker_dict = {\n         \"bge\": \"pre_train_model/bge-reranker-large\",\n@@ -181,19 +181,9 @@\n     }\n     assert args.llm_name in llm_dict.keys(), \"Unknown LLM model name: {args.llm_name}.\"\n     \n     \n-    \n \n-    retrivers = []\n-    # Faiss召回\n-    embed_models = [\"m3e\",\"bge\",\"gte\",\"bce\"]\n-    # vector_store_list = []\n-    for embed_model in embed_models:\n-        retrivers.append(FaissRetriever(embed_model, data))\n-        # vector_store_list.append(faissretriever.vector_store)\n-    print(\"faissretriever load ok\")\n-\n     # BM25召回\n     retrivers.append(BM25(data))\n     print(\"bm25 load ok\")\n     \n"
                },
                {
                    "date": 1732672254968,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -166,8 +166,16 @@\n     retrievers = []\n     for embed_model in args.dense_retriever_lists.split(','):\n         retrievers.append(FaissRetriever(embed_model, data))\n     print(\"faissretriever load ok\")\n+    \n+    # BM25召回\n+    retrievers.append(BM25(data))\n+    print(\"bm25 load ok\")\n+    \n+    # TFIDF召回\n+    retrievers.append(TFIDF(data))\n+    print(\"tfidf load ok\")\n \n     reranker_dict = {\n         \"bge\": \"pre_train_model/bge-reranker-large\",\n         \"bce\": \"pre_train_model/bce-reranker-base\"\n@@ -182,16 +190,10 @@\n     assert args.llm_name in llm_dict.keys(), \"Unknown LLM model name: {args.llm_name}.\"\n     \n     \n \n-    # BM25召回\n-    retrivers.append(BM25(data))\n-    print(\"bm25 load ok\")\n-    \n-    # TFIDF召回\n-    retrivers.append(TFIDF(data))\n-    print(\"tfidf load ok\")\n \n+\n     # LLM大模型\n     llm_name = \"baichuan\"\n     llm_model, llm_path = llm_dict[llm_name]\n     llm = llm_model(llm_path)\n"
                },
                {
                    "date": 1732672285439,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -181,8 +181,12 @@\n         \"bce\": \"pre_train_model/bce-reranker-base\"\n     }\n     assert args.reranker_name in reranker_dict.keys(), \"Unknown reranker model name: {args.reranker_name}.\"\n     \n+    # reRank模型\n+    rerank = reRankLLM(reranker_dict[args.reranker_name])\n+    print(\"rerank model load ok\")\n+    \n     llm_dict = {\n         \"qwen\": (ChatLLM, \"pre_train_model/Qwen1.5-7B-Chat\"),\n         \"baichuan\": (ChatLLM, \"pre_train_model/Baichuan2-7B-Chat\"),\n         \"chatglm\": (Baichuan, \"pre_train_model/chatglm3-6b\")\n@@ -198,11 +202,9 @@\n     llm_model, llm_path = llm_dict[llm_name]\n     llm = llm_model(llm_path)\n     print(\"llm {llm_name} load ok\")\n \n-    # reRank模型\n-    rerank = reRankLLM(reranker_dict[\"bge\"])\n-    print(\"rerank model load ok\")\n+    \n \n     # 对每一条测试问题，做答案生成处理\n     with open(base + \"/data/test_question.json\", \"r\" , encoding=\"utf-8\") as f:\n         jdata = json.loads(f.read())\n"
                },
                {
                    "date": 1732672368800,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,10 +151,10 @@\n     \n     parser = argparse.ArgumentParser(description='My RAG System')\n     parser.add_argument('--document_path', type=str, required=True, default='data/train_a.pdf', help='Document path')\n     \n-    parser.add_argument('--dense_retriever_lists', type=str, required=True, default='m3e,bge,gte,bce\"', help='Dense retriever model lists')\n-    parser.add_argument('--reranker_name', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Reranker model name')\n+    parser.add_argument('--dense_retriever_lists', type=str, required=True, default='m3e,bge,gte,bce\"', help='Dense retrieve model lists')\n+    parser.add_argument('--reranker_name', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Rerank model name')\n     parser.add_argument('--llm_name', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n \n     args = parser.parse_args()\n \n@@ -179,9 +179,9 @@\n     reranker_dict = {\n         \"bge\": \"pre_train_model/bge-reranker-large\",\n         \"bce\": \"pre_train_model/bce-reranker-base\"\n     }\n-    assert args.reranker_name in reranker_dict.keys(), \"Unknown reranker model name: {args.reranker_name}.\"\n+    assert args.reranker_name in reranker_dict.keys(), \"Unknown rerank model name: {args.reranker_name}.\"\n     \n     # reRank模型\n     rerank = reRankLLM(reranker_dict[args.reranker_name])\n     print(\"rerank model load ok\")\n@@ -192,17 +192,12 @@\n         \"chatglm\": (Baichuan, \"pre_train_model/chatglm3-6b\")\n     }\n     assert args.llm_name in llm_dict.keys(), \"Unknown LLM model name: {args.llm_name}.\"\n     \n-    \n-\n-\n-\n     # LLM大模型\n-    llm_name = \"baichuan\"\n-    llm_model, llm_path = llm_dict[llm_name]\n+    llm_model, llm_path = llm_dict[args.llm_name]\n     llm = llm_model(llm_path)\n-    print(\"llm {llm_name} load ok\")\n+    print(\"llm {args.llm_name} load ok\")\n \n     \n \n     # 对每一条测试问题，做答案生成处理\n"
                },
                {
                    "date": 1732672457252,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -150,8 +150,9 @@\n if __name__ == \"__main__\":\n     \n     parser = argparse.ArgumentParser(description='My RAG System')\n     parser.add_argument('--document_path', type=str, required=True, default='data/train_a.pdf', help='Document path')\n+    parser.add_argument('--question_path', type=str, required=True, default='data/test_question.json', help='Question path')\n     \n     parser.add_argument('--dense_retriever_lists', type=str, required=True, default='m3e,bge,gte,bce\"', help='Dense retrieve model lists')\n     parser.add_argument('--reranker_name', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Rerank model name')\n     parser.add_argument('--llm_name', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n@@ -200,9 +201,9 @@\n \n     \n \n     # 对每一条测试问题，做答案生成处理\n-    with open(base + \"/data/test_question.json\", \"r\" , encoding=\"utf-8\") as f:\n+    with open(args.question, \"r\" , encoding=\"utf-8\") as f:\n         jdata = json.loads(f.read())\n         print(len(jdata))\n         max_length = 4000\n         for idx, line in enumerate(jdata):\n"
                },
                {
                    "date": 1732672509286,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -251,9 +251,9 @@\n             # rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n             # # 构造得到rerank后生成答案的prompt\n             # rerank_inputs = get_rerank(rerank_ans, query)\n             \n-            docs = retrievers_recall(retrivers,query)\n+            docs = retrievers_recall(retrievers, query)\n             context = reRank(rerank, 6, query, docs)\n             inputs = get_rerank(context, query)\n \n             batch_input = [inputs]\n@@ -276,7 +276,7 @@\n             # else:\n             #     line[\"answer_5\"] = str(faiss_min_score)\n \n         # 保存结果，生成submission文件\n-        json.dump(jdata, open(base + \"/data/result1.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n+        json.dump(jdata, open(\"/data/result1.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n         end = time.time()\n         print(\"cost time: \" + str(int(end-start)/60))\n"
                },
                {
                    "date": 1732672536645,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,9 +151,10 @@\n     \n     parser = argparse.ArgumentParser(description='My RAG System')\n     parser.add_argument('--document_path', type=str, required=True, default='data/train_a.pdf', help='Document path')\n     parser.add_argument('--question_path', type=str, required=True, default='data/test_question.json', help='Question path')\n-    \n+    parser.add_argument('--output_path', type=str, required=True, default='data/result1.json', help='Question path')\n+\n     parser.add_argument('--dense_retriever_lists', type=str, required=True, default='m3e,bge,gte,bce\"', help='Dense retrieve model lists')\n     parser.add_argument('--reranker_name', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Rerank model name')\n     parser.add_argument('--llm_name', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n \n"
                },
                {
                    "date": 1732672548520,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,9 +151,9 @@\n     \n     parser = argparse.ArgumentParser(description='My RAG System')\n     parser.add_argument('--document_path', type=str, required=True, default='data/train_a.pdf', help='Document path')\n     parser.add_argument('--question_path', type=str, required=True, default='data/test_question.json', help='Question path')\n-    parser.add_argument('--output_path', type=str, required=True, default='data/result1.json', help='Question path')\n+    parser.add_argument('--output_path', type=str, required=True, default='data/result1.json', help='Output answer path')\n \n     parser.add_argument('--dense_retriever_lists', type=str, required=True, default='m3e,bge,gte,bce\"', help='Dense retrieve model lists')\n     parser.add_argument('--reranker_name', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Rerank model name')\n     parser.add_argument('--llm_name', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n"
                },
                {
                    "date": 1732672561160,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -277,7 +277,7 @@\n             # else:\n             #     line[\"answer_5\"] = str(faiss_min_score)\n \n         # 保存结果，生成submission文件\n-        json.dump(jdata, open(\"/data/result1.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n+        json.dump(jdata, open(args.output_path, \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n         end = time.time()\n         print(\"cost time: \" + str(int(end-start)/60))\n"
                },
                {
                    "date": 1732672614780,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -149,16 +149,22 @@\n \n if __name__ == \"__main__\":\n     \n     parser = argparse.ArgumentParser(description='My RAG System')\n+    \n+    # file paths\n     parser.add_argument('--document_path', type=str, required=True, default='data/train_a.pdf', help='Document path')\n     parser.add_argument('--question_path', type=str, required=True, default='data/test_question.json', help='Question path')\n     parser.add_argument('--output_path', type=str, required=True, default='data/result1.json', help='Output answer path')\n \n+    # model names\n     parser.add_argument('--dense_retriever_lists', type=str, required=True, default='m3e,bge,gte,bce\"', help='Dense retrieve model lists')\n     parser.add_argument('--reranker_name', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Rerank model name')\n     parser.add_argument('--llm_name', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n \n+    # hyper-parameters\n+    parser.add_argument('--llm_name', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n+\n     args = parser.parse_args()\n \n     start = time.time()\n     \n"
                },
                {
                    "date": 1732672655388,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -151,19 +151,19 @@\n     \n     parser = argparse.ArgumentParser(description='My RAG System')\n     \n     # file paths\n-    parser.add_argument('--document_path', type=str, required=True, default='data/train_a.pdf', help='Document path')\n-    parser.add_argument('--question_path', type=str, required=True, default='data/test_question.json', help='Question path')\n-    parser.add_argument('--output_path', type=str, required=True, default='data/result1.json', help='Output answer path')\n+    parser.add_argument('--document_path', type=str, required=False, default='data/train_a.pdf', help='Document path')\n+    parser.add_argument('--question_path', type=str, required=False, default='data/test_question.json', help='Question path')\n+    parser.add_argument('--output_path', type=str, required=False, default='data/result1.json', help='Output answer path')\n \n     # model names\n-    parser.add_argument('--dense_retriever_lists', type=str, required=True, default='m3e,bge,gte,bce\"', help='Dense retrieve model lists')\n-    parser.add_argument('--reranker_name', type=str, required=True, default='pre_train_model/bce-reranker-base', help='Rerank model name')\n-    parser.add_argument('--llm_name', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n+    parser.add_argument('--dense_retriever_lists', type=str, required=False, default='m3e,bge,gte,bce\"', help='Dense retrieve model lists')\n+    parser.add_argument('--reranker_name', type=str, required=False, default='pre_train_model/bce-reranker-base', help='Rerank model name')\n+    parser.add_argument('--llm_name', type=str, required=False, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n \n     # hyper-parameters\n-    parser.add_argument('--llm_name', type=str, required=True, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n+    parser.add_argument('--llm_name', type=str, required=False, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n \n     args = parser.parse_args()\n \n     start = time.time()\n"
                },
                {
                    "date": 1732672668512,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -157,10 +157,10 @@\n     parser.add_argument('--output_path', type=str, required=False, default='data/result1.json', help='Output answer path')\n \n     # model names\n     parser.add_argument('--dense_retriever_lists', type=str, required=False, default='m3e,bge,gte,bce\"', help='Dense retrieve model lists')\n-    parser.add_argument('--reranker_name', type=str, required=False, default='pre_train_model/bce-reranker-base', help='Rerank model name')\n-    parser.add_argument('--llm_name', type=str, required=False, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n+    parser.add_argument('--reranker_name', type=str, required=False, default='bce', help='Rerank model name')\n+    parser.add_argument('--llm_name', type=str, required=False, default='qwen', help='LLM model name')\n \n     # hyper-parameters\n     parser.add_argument('--llm_name', type=str, required=False, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n \n"
                },
                {
                    "date": 1732672682640,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -161,9 +161,9 @@\n     parser.add_argument('--reranker_name', type=str, required=False, default='bce', help='Rerank model name')\n     parser.add_argument('--llm_name', type=str, required=False, default='qwen', help='LLM model name')\n \n     # hyper-parameters\n-    parser.add_argument('--llm_name', type=str, required=False, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n+    parser.add_argument('--rerank_topk', type=int, required=False, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n \n     args = parser.parse_args()\n \n     start = time.time()\n"
                },
                {
                    "date": 1732672719896,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -161,9 +161,9 @@\n     parser.add_argument('--reranker_name', type=str, required=False, default='bce', help='Rerank model name')\n     parser.add_argument('--llm_name', type=str, required=False, default='qwen', help='LLM model name')\n \n     # hyper-parameters\n-    parser.add_argument('--rerank_topk', type=int, required=False, default='pre_train_model/Qwen1.5-7B-Chat', help='LLM model name')\n+    parser.add_argument('--rerank_topk', type=int, required=False, default=6, help='Topk documents from reranked outputs.')\n \n     args = parser.parse_args()\n \n     start = time.time()\n"
                },
                {
                    "date": 1732672733454,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -259,9 +259,9 @@\n             # # 构造得到rerank后生成答案的prompt\n             # rerank_inputs = get_rerank(rerank_ans, query)\n             \n             docs = retrievers_recall(retrievers, query)\n-            context = reRank(rerank, 6, query, docs)\n+            context = reRank(rerank, args.rerank_topk, query, docs)\n             inputs = get_rerank(context, query)\n \n             batch_input = [inputs]\n             # batch_input.append(emb_bm25_merge_inputs)\n"
                },
                {
                    "date": 1732673499947,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -159,9 +159,12 @@\n     # model names\n     parser.add_argument('--dense_retriever_lists', type=str, required=False, default='m3e,bge,gte,bce\"', help='Dense retrieve model lists')\n     parser.add_argument('--reranker_name', type=str, required=False, default='bce', help='Rerank model name')\n     parser.add_argument('--llm_name', type=str, required=False, default='qwen', help='LLM model name')\n+    \n+    parser.add_argument('--rephrase_context', action='store_true', help='Rephrase selected document chunks.', default=True)\n \n+\n     # hyper-parameters\n     parser.add_argument('--rerank_topk', type=int, required=False, default=6, help='Topk documents from reranked outputs.')\n \n     args = parser.parse_args()\n"
                },
                {
                    "date": 1732673541479,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -162,9 +162,8 @@\n     parser.add_argument('--llm_name', type=str, required=False, default='qwen', help='LLM model name')\n     \n     parser.add_argument('--rephrase_context', action='store_true', help='Rephrase selected document chunks.', default=True)\n \n-\n     # hyper-parameters\n     parser.add_argument('--rerank_topk', type=int, required=False, default=6, help='Topk documents from reranked outputs.')\n \n     args = parser.parse_args()\n"
                },
                {
                    "date": 1732673971828,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -95,9 +95,9 @@\n                                 {question}\"\"\".format(emb_ans=emb_ans, bm25_ans = bm25_ans, question = query)\n     return prompt_template\n \n \n-def get_rerank(emb_ans, query):\n+def qa_template(emb_ans, query):\n \n     prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n                                 如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                 已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n@@ -262,8 +262,9 @@\n             # rerank_inputs = get_rerank(rerank_ans, query)\n             \n             docs = retrievers_recall(retrievers, query)\n             context = reRank(rerank, args.rerank_topk, query, docs)\n+            \n             inputs = get_rerank(context, query)\n \n             batch_input = [inputs]\n             # batch_input.append(emb_bm25_merge_inputs)\n"
                },
                {
                    "date": 1732674050167,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -94,17 +94,22 @@\n                                 问题:\n                                 {question}\"\"\".format(emb_ans=emb_ans, bm25_ans = bm25_ans, question = query)\n     return prompt_template\n \n+def rephrase_context_template(context):\n \n+    prompt_template = \"\"\"请根据以下提供的文档块，重新组织和总结内容，使其逻辑清晰、结构完整。目标是使杂乱信息变得规整，便于进一步回答用户问题。\n+    \\n {context}\"\"\"\n+    return prompt_template\n+\n def qa_template(emb_ans, query):\n \n     prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n                                 如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                 已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                 1: {emb_ans}\n                                 问题:\n-                                {question}\"\"\".format(emb_ans=emb_ans, question = query)\n+                                {question}\"\"\"\n     return prompt_template\n \n \n def question(text, llm, vector_store, prompt_template):\n"
                },
                {
                    "date": 1732674115556,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -96,15 +96,15 @@\n     return prompt_template\n \n def rephrase_context_template(context):\n \n-    prompt_template = \"\"\"请根据以下提供的文档块，重新组织和总结内容，使其逻辑清晰、结构完整。目标是使杂乱信息变得规整，便于进一步回答用户问题。\n-    \\n {context}\"\"\"\n+    prompt_template = f\"\"\"请根据以下提供的文档块，重新组织和总结内容，使其逻辑清晰、结构完整，且没有重复内容。\n+    目标是使杂乱信息变得规整，便于进一步回答用户问题。\\n {context}\"\"\"\n     return prompt_template\n \n def qa_template(emb_ans, query):\n \n-    prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n+    prompt_template = f\"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n                                 如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                 已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                 1: {emb_ans}\n                                 问题:\n"
                },
                {
                    "date": 1732674143874,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -107,9 +107,9 @@\n                                 如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                 已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                 1: {emb_ans}\n                                 问题:\n-                                {question}\"\"\"\n+                                {query}\"\"\"\n     return prompt_template\n \n \n def question(text, llm, vector_store, prompt_template):\n@@ -194,9 +194,9 @@\n     reranker_dict = {\n         \"bge\": \"pre_train_model/bge-reranker-large\",\n         \"bce\": \"pre_train_model/bce-reranker-base\"\n     }\n-    assert args.reranker_name in reranker_dict.keys(), \"Unknown rerank model name: {args.reranker_name}.\"\n+    assert args.reranker_name in reranker_dict.keys(), f\"Unknown rerank model name: {args.reranker_name}.\"\n     \n     # reRank模型\n     rerank = reRankLLM(reranker_dict[args.reranker_name])\n     print(\"rerank model load ok\")\n@@ -205,14 +205,14 @@\n         \"qwen\": (ChatLLM, \"pre_train_model/Qwen1.5-7B-Chat\"),\n         \"baichuan\": (ChatLLM, \"pre_train_model/Baichuan2-7B-Chat\"),\n         \"chatglm\": (Baichuan, \"pre_train_model/chatglm3-6b\")\n     }\n-    assert args.llm_name in llm_dict.keys(), \"Unknown LLM model name: {args.llm_name}.\"\n+    assert args.llm_name in llm_dict.keys(), f\"Unknown LLM model name: {args.llm_name}.\"\n     \n     # LLM大模型\n     llm_model, llm_path = llm_dict[args.llm_name]\n     llm = llm_model(llm_path)\n-    print(\"llm {args.llm_name} load ok\")\n+    print(f\"llm {args.llm_name} load ok\")\n \n     \n \n     # 对每一条测试问题，做答案生成处理\n"
                },
                {
                    "date": 1732674157630,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -268,9 +268,9 @@\n             \n             docs = retrievers_recall(retrievers, query)\n             context = reRank(rerank, args.rerank_topk, query, docs)\n             \n-            inputs = get_rerank(context, query)\n+            inputs = qa_template(context, query)\n \n             batch_input = [inputs]\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732674181060,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -268,11 +268,13 @@\n             \n             docs = retrievers_recall(retrievers, query)\n             context = reRank(rerank, args.rerank_topk, query, docs)\n             \n-            inputs = qa_template(context, query)\n+            \n+            \n+            qa_inputs = qa_template(context, query)\n \n-            batch_input = [inputs]\n+            batch_qa_input = [qa_inputs]\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n"
                },
                {
                    "date": 1732674189967,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -278,10 +278,10 @@\n             # batch_input.append(bm25_inputs)\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n             # 执行batch推理\n-            batch_output = llm.infer(batch_input)\n-            line[\"answer\"] = batch_output[0] # 合并两路召回的结果\n+            batch_qa_output = llm.infer(batch_qa_input)\n+            line[\"answer\"] = batch_qa_output[0] # 合并两路召回的结果\n             # line[\"answer_2\"] = batch_output[1] # bm召回的结果\n             # line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n             # line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n             # line[\"answer_5\"] = emb_ans\n"
                },
                {
                    "date": 1732674230333,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -268,10 +268,10 @@\n             \n             docs = retrievers_recall(retrievers, query)\n             context = reRank(rerank, args.rerank_topk, query, docs)\n             \n+            rephrased_context = llm.infer([rephrase_context_template(context)])[0]\n             \n-            \n             qa_inputs = qa_template(context, query)\n \n             batch_qa_input = [qa_inputs]\n             # batch_input.append(emb_bm25_merge_inputs)\n"
                },
                {
                    "date": 1732674266528,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -268,9 +268,10 @@\n             \n             docs = retrievers_recall(retrievers, query)\n             context = reRank(rerank, args.rerank_topk, query, docs)\n             \n-            rephrased_context = llm.infer([rephrase_context_template(context)])[0]\n+            if args.rephrase_context:\n+                context = llm.infer([rephrase_context_template(context)])[0]\n             \n             qa_inputs = qa_template(context, query)\n \n             batch_qa_input = [qa_inputs]\n"
                },
                {
                    "date": 1732677078541,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -166,8 +166,10 @@\n     parser.add_argument('--reranker_name', type=str, required=False, default='bce', help='Rerank model name')\n     parser.add_argument('--llm_name', type=str, required=False, default='qwen', help='LLM model name')\n     \n     parser.add_argument('--rephrase_context', action='store_true', help='Rephrase selected document chunks.', default=True)\n+    parser.add_argument('--rephrase_before_retrieve', action='store_true', help='Rephrase and extend the original question for better retrieve.', default=True)\n+    parser.add_argument('--answer_before_retrieve', action='store_true', help='Rephrase and extend the original question for better retrieve.', default=True)\n \n     # hyper-parameters\n     parser.add_argument('--rerank_topk', type=int, required=False, default=6, help='Topk documents from reranked outputs.')\n \n@@ -264,8 +266,12 @@\n             # # rerank召回的候选，并按照相关性得分排序\n             # rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n             # # 构造得到rerank后生成答案的prompt\n             # rerank_inputs = get_rerank(rerank_ans, query)\n+            if args.rephrase_before_retrieve:\n+                pass\n+            if args.answer_before_retrieve:\n+                pass\n             \n             docs = retrievers_recall(retrievers, query)\n             context = reRank(rerank, args.rerank_topk, query, docs)\n             \n"
                },
                {
                    "date": 1732677210348,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -266,12 +266,13 @@\n             # # rerank召回的候选，并按照相关性得分排序\n             # rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n             # # 构造得到rerank后生成答案的prompt\n             # rerank_inputs = get_rerank(rerank_ans, query)\n+            extend_query = query\n             if args.rephrase_before_retrieve:\n-                pass\n+                extend_query += \" \" + llm.infer([rephrase_context_template(query)])[0]\n             if args.answer_before_retrieve:\n-                pass\n+                extend_query += \" \" + llm.infer([rephrase_context_template(query)])[0]\n             \n             docs = retrievers_recall(retrievers, query)\n             context = reRank(rerank, args.rerank_topk, query, docs)\n             \n"
                },
                {
                    "date": 1732677233315,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -266,16 +266,16 @@\n             # # rerank召回的候选，并按照相关性得分排序\n             # rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n             # # 构造得到rerank后生成答案的prompt\n             # rerank_inputs = get_rerank(rerank_ans, query)\n-            extend_query = query\n+            extended_query = query\n             if args.rephrase_before_retrieve:\n-                extend_query += \" \" + llm.infer([rephrase_context_template(query)])[0]\n+                extended_query += \" \" + llm.infer([rephrase_context_template(query)])[0]\n             if args.answer_before_retrieve:\n-                extend_query += \" \" + llm.infer([rephrase_context_template(query)])[0]\n+                extended_query += \" \" + llm.infer([rephrase_context_template(query)])[0]\n             \n-            docs = retrievers_recall(retrievers, query)\n-            context = reRank(rerank, args.rerank_topk, query, docs)\n+            docs = retrievers_recall(retrievers, extended_query)\n+            context = reRank(rerank, args.rerank_topk, extended_query, docs)\n             \n             if args.rephrase_context:\n                 context = llm.infer([rephrase_context_template(context)])[0]\n             \n"
                },
                {
                    "date": 1732677256059,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -268,11 +268,11 @@\n             # # 构造得到rerank后生成答案的prompt\n             # rerank_inputs = get_rerank(rerank_ans, query)\n             extended_query = query\n             if args.rephrase_before_retrieve:\n-                extended_query += \" \" + llm.infer([rephrase_context_template(query)])[0]\n+                extended_query += \" \" + llm.infer([rephrase_question_template(query)])[0]\n             if args.answer_before_retrieve:\n-                extended_query += \" \" + llm.infer([rephrase_context_template(query)])[0]\n+                extended_query += \" \" + llm.infer([answer_question_template(query)])[0]\n             \n             docs = retrievers_recall(retrievers, extended_query)\n             context = reRank(rerank, args.rerank_topk, extended_query, docs)\n             \n"
                },
                {
                    "date": 1732677296357,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -94,8 +94,20 @@\n                                 问题:\n                                 {question}\"\"\".format(emb_ans=emb_ans, bm25_ans = bm25_ans, question = query)\n     return prompt_template\n \n+def rephrase_question_template(question):\n+\n+    prompt_template = f\"\"\"请根据以下提供的文档块，重新组织和总结内容，使其逻辑清晰、结构完整，且没有重复内容。\n+    目标是使杂乱信息变得规整，便于进一步回答用户问题。\\n {context}\"\"\"\n+    return prompt_template\n+\n+def answer_question_template(question):\n+\n+    prompt_template = f\"\"\"请根据以下提供的文档块，重新组织和总结内容，使其逻辑清晰、结构完整，且没有重复内容。\n+    目标是使杂乱信息变得规整，便于进一步回答用户问题。\\n {context}\"\"\"\n+    return prompt_template\n+\n def rephrase_context_template(context):\n \n     prompt_template = f\"\"\"请根据以下提供的文档块，重新组织和总结内容，使其逻辑清晰、结构完整，且没有重复内容。\n     目标是使杂乱信息变得规整，便于进一步回答用户问题。\\n {context}\"\"\"\n"
                },
                {
                    "date": 1732678040820,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -102,10 +102,11 @@\n     return prompt_template\n \n def answer_question_template(question):\n \n-    prompt_template = f\"\"\"请根据以下提供的文档块，重新组织和总结内容，使其逻辑清晰、结构完整，且没有重复内容。\n-    目标是使杂乱信息变得规整，便于进一步回答用户问题。\\n {context}\"\"\"\n+    prompt_template = f\"\"\"请根据以下问题生成一个合理且基于常识的答案，尽量避免不确定或错误的信息。如果不知道答案，请回答 \"无答案\"。\n+    问题：{question}\n+    答案：\"\"\"\n     return prompt_template\n \n def rephrase_context_template(context):\n \n"
                },
                {
                    "date": 1732678087336,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -283,9 +283,11 @@\n             extended_query = query\n             if args.rephrase_before_retrieve:\n                 extended_query += \" \" + llm.infer([rephrase_question_template(query)])[0]\n             if args.answer_before_retrieve:\n-                extended_query += \" \" + llm.infer([answer_question_template(query)])[0]\n+                answer = llm.infer([answer_question_template(query)])[0]\n+                if answer.strip() != \"无答案\"：\n+                    extended_query += \" \" + answer\n             \n             docs = retrievers_recall(retrievers, extended_query)\n             context = reRank(rerank, args.rerank_topk, extended_query, docs)\n             \n"
                },
                {
                    "date": 1732678093484,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -284,9 +284,9 @@\n             if args.rephrase_before_retrieve:\n                 extended_query += \" \" + llm.infer([rephrase_question_template(query)])[0]\n             if args.answer_before_retrieve:\n                 answer = llm.infer([answer_question_template(query)])[0]\n-                if answer.strip() != \"无答案\"：\n+                if answer.strip() != \"无答案\":\n                     extended_query += \" \" + answer\n             \n             docs = retrievers_recall(retrievers, extended_query)\n             context = reRank(rerank, args.rerank_topk, extended_query, docs)\n"
                },
                {
                    "date": 1732678314819,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -96,10 +96,11 @@\n     return prompt_template\n \n def rephrase_question_template(question):\n \n-    prompt_template = f\"\"\"请根据以下提供的文档块，重新组织和总结内容，使其逻辑清晰、结构完整，且没有重复内容。\n-    目标是使杂乱信息变得规整，便于进一步回答用户问题。\\n {context}\"\"\"\n+    prompt_template = f\"\"\"以下是用户的问题，请将其改写为一个更详细且具有更广泛检索可能性的问题，保持原始语义一致：\n+原问题：{用户问题}\n+改写后的问题：\"\"\"\n     return prompt_template\n \n def answer_question_template(question):\n \n"
                },
                {
                    "date": 1732678323258,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -97,10 +97,10 @@\n \n def rephrase_question_template(question):\n \n     prompt_template = f\"\"\"以下是用户的问题，请将其改写为一个更详细且具有更广泛检索可能性的问题，保持原始语义一致：\n-原问题：{用户问题}\n-改写后的问题：\"\"\"\n+                        原问题：{question}\n+                        改写后的问题：\"\"\"\n     return prompt_template\n \n def answer_question_template(question):\n \n"
                },
                {
                    "date": 1732678370081,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -111,9 +111,11 @@\n \n def rephrase_context_template(context):\n \n     prompt_template = f\"\"\"请根据以下提供的文档块，重新组织和总结内容，使其逻辑清晰、结构完整，且没有重复内容。\n-    目标是使杂乱信息变得规整，便于进一步回答用户问题。\\n {context}\"\"\"\n+    目标是使杂乱信息变得规整，便于进一步回答用户问题。\n+    文档：\n+    {context}\"\"\"\n     return prompt_template\n \n def qa_template(emb_ans, query):\n \n"
                },
                {
                    "date": 1732678402703,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -121,11 +121,11 @@\n \n     prompt_template = f\"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n                                 如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                 已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n-                                1: {emb_ans}\n-                                问题:\n-                                {query}\"\"\"\n+                                {emb_ans}\n+                                问题: {query}\n+                                答案: \"\"\"\n     return prompt_template\n \n \n def question(text, llm, vector_store, prompt_template):\n"
                },
                {
                    "date": 1732678466685,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -282,9 +282,9 @@\n             # # rerank召回的候选，并按照相关性得分排序\n             # rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n             # # 构造得到rerank后生成答案的prompt\n             # rerank_inputs = get_rerank(rerank_ans, query)\n-            extended_query = query\n+            queries = [query]\n             if args.rephrase_before_retrieve:\n                 extended_query += \" \" + llm.infer([rephrase_question_template(query)])[0]\n             if args.answer_before_retrieve:\n                 answer = llm.infer([answer_question_template(query)])[0]\n"
                },
                {
                    "date": 1732678500284,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -284,12 +284,13 @@\n             # # 构造得到rerank后生成答案的prompt\n             # rerank_inputs = get_rerank(rerank_ans, query)\n             queries = [query]\n             if args.rephrase_before_retrieve:\n-                extended_query += \" \" + llm.infer([rephrase_question_template(query)])[0]\n+                queries.append(llm.infer([rephrase_question_template(query)])[0])\n             if args.answer_before_retrieve:\n                 answer = llm.infer([answer_question_template(query)])[0]\n                 if answer.strip() != \"无答案\":\n+                    queries.append(query+\" \"+answer)\n                     extended_query += \" \" + answer\n             \n             docs = retrievers_recall(retrievers, extended_query)\n             context = reRank(rerank, args.rerank_topk, extended_query, docs)\n"
                },
                {
                    "date": 1732678537032,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -289,12 +289,13 @@\n             if args.answer_before_retrieve:\n                 answer = llm.infer([answer_question_template(query)])[0]\n                 if answer.strip() != \"无答案\":\n                     queries.append(query+\" \"+answer)\n-                    extended_query += \" \" + answer\n             \n-            docs = retrievers_recall(retrievers, extended_query)\n-            context = reRank(rerank, args.rerank_topk, extended_query, docs)\n+            contexts = []\n+            for q in queries:            \n+                docs = retrievers_recall(retrievers, q)\n+                context = reRank(rerank, args.rerank_topk, q, docs)\n             \n             if args.rephrase_context:\n                 context = llm.infer([rephrase_context_template(context)])[0]\n             \n"
                },
                {
                    "date": 1732678560896,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -295,12 +295,12 @@\n             for q in queries:            \n                 docs = retrievers_recall(retrievers, q)\n                 context = reRank(rerank, args.rerank_topk, q, docs)\n             \n-            if args.rephrase_context:\n-                context = llm.infer([rephrase_context_template(context)])[0]\n-            \n-            qa_inputs = qa_template(context, query)\n+                if args.rephrase_context:\n+                    context = llm.infer([rephrase_context_template(context)])[0]\n+                \n+                qa_inputs = qa_template(context, query)\n \n             batch_qa_input = [qa_inputs]\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732678569634,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -290,9 +290,9 @@\n                 answer = llm.infer([answer_question_template(query)])[0]\n                 if answer.strip() != \"无答案\":\n                     queries.append(query+\" \"+answer)\n             \n-            contexts = []\n+            batch_qa_input =\n             for q in queries:            \n                 docs = retrievers_recall(retrievers, q)\n                 context = reRank(rerank, args.rerank_topk, q, docs)\n             \n"
                },
                {
                    "date": 1732678581350,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -290,19 +290,18 @@\n                 answer = llm.infer([answer_question_template(query)])[0]\n                 if answer.strip() != \"无答案\":\n                     queries.append(query+\" \"+answer)\n             \n-            batch_qa_input =\n+            batch_qa_input = []\n             for q in queries:            \n                 docs = retrievers_recall(retrievers, q)\n                 context = reRank(rerank, args.rerank_topk, q, docs)\n             \n                 if args.rephrase_context:\n                     context = llm.infer([rephrase_context_template(context)])[0]\n                 \n                 qa_inputs = qa_template(context, query)\n-\n-            batch_qa_input = [qa_inputs]\n+                batch_qa_input.append(qa_inputs)\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n"
                },
                {
                    "date": 1732678616516,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -306,9 +306,10 @@\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n             # 执行batch推理\n             batch_qa_output = llm.infer(batch_qa_input)\n-            line[\"answer\"] = batch_qa_output[0] # 合并两路召回的结果\n+            for i, qa_output in enumerate(batch_qa_output):\n+                line[f\"answer_{i+1}\"] = qa_output # 合并两路召回的结果\n             # line[\"answer_2\"] = batch_output[1] # bm召回的结果\n             # line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n             # line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n             # line[\"answer_5\"] = emb_ans\n"
                },
                {
                    "date": 1732691217672,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,14 +53,14 @@\n def reRank(rerank, top_k, query, docs):\n     rerank_ans = rerank.predict(query, docs)\n     rerank_ans = rerank_ans[:top_k]\n     # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n-    emb_ans = \"\"\n+    emb_ans = []\n     \n     for doc in rerank_ans:\n         if(len(emb_ans + doc.page_content) > max_length):\n             break\n-        emb_ans = emb_ans + doc.page_content\n+        emb_ans.append(doc.page_content)\n     return emb_ans         \n             \n     \n \n"
                },
                {
                    "date": 1732691235681,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -296,9 +296,9 @@\n                 docs = retrievers_recall(retrievers, q)\n                 context = reRank(rerank, args.rerank_topk, q, docs)\n             \n                 if args.rephrase_context:\n-                    context = llm.infer([rephrase_context_template(context)])[0]\n+                    context = llm.infer([rephrase_context_template(''.join(context))])[0]\n                 \n                 qa_inputs = qa_template(context, query)\n                 batch_qa_input.append(qa_inputs)\n             # batch_input.append(emb_bm25_merge_inputs)\n"
                },
                {
                    "date": 1732691357044,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -295,13 +295,18 @@\n             for q in queries:            \n                 docs = retrievers_recall(retrievers, q)\n                 context = reRank(rerank, args.rerank_topk, q, docs)\n             \n-                if args.rephrase_context:\n-                    context = llm.infer([rephrase_context_template(''.join(context))])[0]\n-                \n-                qa_inputs = qa_template(context, query)\n-                batch_qa_input.append(qa_inputs)\n+                if args.recursive_answer:\n+                    pass\n+                else:\n+                    if args.rephrase_context:\n+                        context = llm.infer([rephrase_context_template(''.join(context))])[0]\n+                    else:\n+                        context = \"\".join(context)\n+                    qa_inputs = qa_template(context, query)                \n+                    batch_qa_input.append(qa_inputs)\n+                    batch_qa_output = llm.infer(batch_qa_input)\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n"
                },
                {
                    "date": 1732691411612,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -181,11 +181,12 @@\n     parser.add_argument('--dense_retriever_lists', type=str, required=False, default='m3e,bge,gte,bce\"', help='Dense retrieve model lists')\n     parser.add_argument('--reranker_name', type=str, required=False, default='bce', help='Rerank model name')\n     parser.add_argument('--llm_name', type=str, required=False, default='qwen', help='LLM model name')\n     \n-    parser.add_argument('--rephrase_context', action='store_true', help='Rephrase selected document chunks.', default=True)\n-    parser.add_argument('--rephrase_before_retrieve', action='store_true', help='Rephrase and extend the original question for better retrieve.', default=True)\n-    parser.add_argument('--answer_before_retrieve', action='store_true', help='Rephrase and extend the original question for better retrieve.', default=True)\n+    parser.add_argument('--recursive_answer', action='store_true', help='Rephrase selected document chunks.', default=False)\n+    parser.add_argument('--rephrase_context', action='store_true', help='Rephrase selected document chunks.', default=False)\n+    parser.add_argument('--rephrase_before_retrieve', action='store_true', help='Rephrase and extend the original question for better retrieve.', default=False)\n+    parser.add_argument('--answer_before_retrieve', action='store_true', help='Rephrase and extend the original question for better retrieve.', default=False)\n \n     # hyper-parameters\n     parser.add_argument('--rerank_topk', type=int, required=False, default=6, help='Topk documents from reranked outputs.')\n \n"
                },
                {
                    "date": 1732692269379,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -297,23 +297,25 @@\n                 docs = retrievers_recall(retrievers, q)\n                 context = reRank(rerank, args.rerank_topk, q, docs)\n             \n                 if args.recursive_answer:\n-                    pass\n+                    for i, c in enumerate(context):\n+                        if i==0:\n+                            llm.infer([qa_template(c, query))\n                 else:\n                     if args.rephrase_context:\n                         context = llm.infer([rephrase_context_template(''.join(context))])[0]\n                     else:\n                         context = \"\".join(context)\n-                    qa_inputs = qa_template(context, query)                \n-                    batch_qa_input.append(qa_inputs)\n-                    batch_qa_output = llm.infer(batch_qa_input)\n+            qa_inputs = qa_template(context, query)                \n+            batch_qa_input.append(qa_inputs)\n+            # 执行batch推理\n+            batch_qa_output = llm.infer(batch_qa_input)\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n-            # 执行batch推理\n-            batch_qa_output = llm.infer(batch_qa_input)\n+            \n             for i, qa_output in enumerate(batch_qa_output):\n                 line[f\"answer_{i+1}\"] = qa_output # 合并两路召回的结果\n             # line[\"answer_2\"] = batch_output[1] # bm召回的结果\n             # line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n"
                },
                {
                    "date": 1732692333107,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -291,22 +291,17 @@\n                 answer = llm.infer([answer_question_template(query)])[0]\n                 if answer.strip() != \"无答案\":\n                     queries.append(query+\" \"+answer)\n             \n-            batch_qa_input = []\n+            batch_context = []\n             for q in queries:            \n                 docs = retrievers_recall(retrievers, q)\n-                context = reRank(rerank, args.rerank_topk, q, docs)\n-            \n-                if args.recursive_answer:\n-                    for i, c in enumerate(context):\n-                        if i==0:\n-                            llm.infer([qa_template(c, query))\n-                else:\n-                    if args.rephrase_context:\n-                        context = llm.infer([rephrase_context_template(''.join(context))])[0]\n-                    else:\n-                        context = \"\".join(context)\n+                batch_context.append(reRank(rerank, args.rerank_topk, q, docs))\n+                \n+                # if args.rephrase_context:\n+                #     context = llm.infer([rephrase_context_template(''.join(context))])[0]\n+                # else:\n+                #     context = \"\".join(context)\n             qa_inputs = qa_template(context, query)                \n             batch_qa_input.append(qa_inputs)\n             # 执行batch推理\n             batch_qa_output = llm.infer(batch_qa_input)\n"
                },
                {
                    "date": 1732692349325,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -300,8 +300,9 @@\n                 # if args.rephrase_context:\n                 #     context = llm.infer([rephrase_context_template(''.join(context))])[0]\n                 # else:\n                 #     context = \"\".join(context)\n+            #baseline\n             qa_inputs = qa_template(context, query)                \n             batch_qa_input.append(qa_inputs)\n             # 执行batch推理\n             batch_qa_output = llm.infer(batch_qa_input)\n"
                },
                {
                    "date": 1732692413236,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -300,10 +300,10 @@\n                 # if args.rephrase_context:\n                 #     context = llm.infer([rephrase_context_template(''.join(context))])[0]\n                 # else:\n                 #     context = \"\".join(context)\n-            #baseline\n-            qa_inputs = qa_template(context, query)                \n+            \n+            batch_qa_inputs = [qa_template(context, query) for context in batch_context]         \n             batch_qa_input.append(qa_inputs)\n             # 执行batch推理\n             batch_qa_output = llm.infer(batch_qa_input)\n             # batch_input.append(emb_bm25_merge_inputs)\n"
                },
                {
                    "date": 1732692487734,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -301,12 +301,13 @@\n                 #     context = llm.infer([rephrase_context_template(''.join(context))])[0]\n                 # else:\n                 #     context = \"\".join(context)\n             \n-            batch_qa_inputs = [qa_template(context, query) for context in batch_context]         \n-            batch_qa_input.append(qa_inputs)\n+            batch_qa_inputs = [qa_template(''.join(context), query) for context in batch_context]\n+            if args.rephrase_context:\n+                context = [rephrase_context_template(''.join(context)) for context in batch_context]\n             # 执行batch推理\n-            batch_qa_output = llm.infer(batch_qa_input)\n+            batch_qa_output = llm.infer(batch_qa_inputs)\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n"
                },
                {
                    "date": 1732692497568,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -303,9 +303,9 @@\n                 #     context = \"\".join(context)\n             \n             batch_qa_inputs = [qa_template(''.join(context), query) for context in batch_context]\n             if args.rephrase_context:\n-                context = [rephrase_context_template(''.join(context)) for context in batch_context]\n+                batch_qa_inputs = [rephrase_context_template(''.join(context)) for context in batch_context]\n             # 执行batch推理\n             batch_qa_output = llm.infer(batch_qa_inputs)\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732692515903,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -304,8 +304,10 @@\n             \n             batch_qa_inputs = [qa_template(''.join(context), query) for context in batch_context]\n             if args.rephrase_context:\n                 batch_qa_inputs = [rephrase_context_template(''.join(context)) for context in batch_context]\n+            if args.recursive_answer:\n+                \n             # 执行batch推理\n             batch_qa_output = llm.infer(batch_qa_inputs)\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732693266510,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -292,12 +292,13 @@\n                 if answer.strip() != \"无答案\":\n                     queries.append(query+\" \"+answer)\n             \n             batch_context = []\n+            max_context_num = 0\n             for q in queries:            \n                 docs = retrievers_recall(retrievers, q)\n                 batch_context.append(reRank(rerank, args.rerank_topk, q, docs))\n-                \n+                max_context_num = max(max_context_num, len(batch_context))\n                 # if args.rephrase_context:\n                 #     context = llm.infer([rephrase_context_template(''.join(context))])[0]\n                 # else:\n                 #     context = \"\".join(context)\n"
                },
                {
                    "date": 1732693311204,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -306,9 +306,10 @@\n             batch_qa_inputs = [qa_template(''.join(context), query) for context in batch_context]\n             if args.rephrase_context:\n                 batch_qa_inputs = [rephrase_context_template(''.join(context)) for context in batch_context]\n             if args.recursive_answer:\n-                \n+                for i in range(max_context_num):\n+                    batch_qa_inputs = [rephrase_context_template(''.join(context)) for context in batch_context]\n             # 执行batch推理\n             batch_qa_output = llm.infer(batch_qa_inputs)\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732693428390,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -304,9 +304,11 @@\n                 #     context = \"\".join(context)\n             \n             batch_qa_inputs = [qa_template(''.join(context), query) for context in batch_context]\n             if args.rephrase_context:\n-                batch_qa_inputs = [rephrase_context_template(''.join(context)) for context in batch_context]\n+                rephrase_inputs = [rephrase_context_template(''.join(context)) for context in batch_context]\n+                rephrase_context = llm.infer(rephrase_inputs)\n+                batch_qa_inputs += [qa_template(''.join(context), query) for context in rephrase_context]\n             if args.recursive_answer:\n                 for i in range(max_context_num):\n                     batch_qa_inputs = [rephrase_context_template(''.join(context)) for context in batch_context]\n             # 执行batch推理\n"
                },
                {
                    "date": 1732693546371,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -309,9 +309,10 @@\n                 rephrase_context = llm.infer(rephrase_inputs)\n                 batch_qa_inputs += [qa_template(''.join(context), query) for context in rephrase_context]\n             if args.recursive_answer:\n                 for i in range(max_context_num):\n-                    batch_qa_inputs = [rephrase_context_template(''.join(context)) for context in batch_context]\n+                    if i==0:\n+                        batch_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n             # 执行batch推理\n             batch_qa_output = llm.infer(batch_qa_inputs)\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732693703132,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -310,9 +310,12 @@\n                 batch_qa_inputs += [qa_template(''.join(context), query) for context in rephrase_context]\n             if args.recursive_answer:\n                 for i in range(max_context_num):\n                     if i==0:\n-                        batch_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n+                        recursive_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n+                    else:\n+                        recursive_qa_inputs = [recursive_qa_template(context[i], query, pre_answer) for context,pre_answer in zip(batch_context, recursive_qa_outputs)]\n+                        recursive_outputs = llm.infer(recursive_qa_inputs)\n             # 执行batch推理\n             batch_qa_output = llm.infer(batch_qa_inputs)\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732693732669,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -313,9 +313,9 @@\n                     if i==0:\n                         recursive_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n                     else:\n                         recursive_qa_inputs = [recursive_qa_template(context[i], query, pre_answer) for context,pre_answer in zip(batch_context, recursive_qa_outputs)]\n-                        recursive_outputs = llm.infer(recursive_qa_inputs)\n+                        batch_qa_outputs = llm.infer(recursive_qa_inputs)\n             # 执行batch推理\n             batch_qa_output = llm.infer(batch_qa_inputs)\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732693781543,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -307,17 +307,19 @@\n             if args.rephrase_context:\n                 rephrase_inputs = [rephrase_context_template(''.join(context)) for context in batch_context]\n                 rephrase_context = llm.infer(rephrase_inputs)\n                 batch_qa_inputs += [qa_template(''.join(context), query) for context in rephrase_context]\n+            # 执行batch推理\n+            batch_qa_output = llm.infer(batch_qa_inputs)\n+            \n             if args.recursive_answer:\n                 for i in range(max_context_num):\n                     if i==0:\n                         recursive_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n                     else:\n                         recursive_qa_inputs = [recursive_qa_template(context[i], query, pre_answer) for context,pre_answer in zip(batch_context, recursive_qa_outputs)]\n-                        batch_qa_outputs = llm.infer(recursive_qa_inputs)\n-            # 执行batch推理\n-            batch_qa_output = llm.infer(batch_qa_inputs)\n+                        recursive_qa_outputs = llm.infer(recursive_qa_inputs)\n+            \n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n"
                },
                {
                    "date": 1732693818480,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -308,17 +308,18 @@\n                 rephrase_inputs = [rephrase_context_template(''.join(context)) for context in batch_context]\n                 rephrase_context = llm.infer(rephrase_inputs)\n                 batch_qa_inputs += [qa_template(''.join(context), query) for context in rephrase_context]\n             # 执行batch推理\n-            batch_qa_output = llm.infer(batch_qa_inputs)\n+            batch_qa_outputs = llm.infer(batch_qa_inputs)\n             \n             if args.recursive_answer:\n                 for i in range(max_context_num):\n                     if i==0:\n                         recursive_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n                     else:\n                         recursive_qa_inputs = [recursive_qa_template(context[i], query, pre_answer) for context,pre_answer in zip(batch_context, recursive_qa_outputs)]\n                         recursive_qa_outputs = llm.infer(recursive_qa_inputs)\n+                batch_qa_outputs += recursive_qa_outputs\n             \n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n             # batch_input.append(emb_inputs)\n"
                },
                {
                    "date": 1732693839874,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -126,9 +126,19 @@\n                                 问题: {query}\n                                 答案: \"\"\"\n     return prompt_template\n \n+def recursive_qa_template(emb_ans, query):\n \n+    prompt_template = f\"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n+                                如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n+                                已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n+                                {emb_ans}\n+                                问题: {query}\n+                                答案: \"\"\"\n+    return prompt_template\n+\n+\n def question(text, llm, vector_store, prompt_template):\n \n     chain = get_qa_chain(llm, vector_store, prompt_template)\n \n@@ -324,9 +334,9 @@\n             # batch_input.append(bm25_inputs)\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n             \n-            for i, qa_output in enumerate(batch_qa_output):\n+            for i, qa_output in enumerate(batch_qa_outputs):\n                 line[f\"answer_{i+1}\"] = qa_output # 合并两路召回的结果\n             # line[\"answer_2\"] = batch_output[1] # bm召回的结果\n             # line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n             # line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n"
                },
                {
                    "date": 1732694881394,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -120,19 +120,20 @@\n def qa_template(emb_ans, query):\n \n     prompt_template = f\"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n                                 如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n-                                已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n-                                {emb_ans}\n+                                已知文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\n                                 问题: {query}\n+                                文档内容: {emb_ans}\n                                 答案: \"\"\"\n     return prompt_template\n \n def recursive_qa_template(emb_ans, query):\n \n-    prompt_template = f\"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n+    prompt_template = f\"\"\"基于当前生成的答案以及一个相关文档，简洁和专业的来回答用户的问题。\n                                 如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n-                                已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n+                                当前答案基于前几轮内容生成，请结合新文档内容对答案进行优化，使其更准确、更全面。\n+                                文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\n                                 {emb_ans}\n                                 问题: {query}\n                                 答案: \"\"\"\n     return prompt_template\n"
                },
                {
                    "date": 1732694906704,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -126,16 +126,16 @@\n                                 文档内容: {emb_ans}\n                                 答案: \"\"\"\n     return prompt_template\n \n-def recursive_qa_template(emb_ans, query):\n+def recursive_qa_template(emb_ans, answer, query):\n \n     prompt_template = f\"\"\"基于当前生成的答案以及一个相关文档，简洁和专业的来回答用户的问题。\n                                 如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                 当前答案基于前几轮内容生成，请结合新文档内容对答案进行优化，使其更准确、更全面。\n                                 文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\n-                                {emb_ans}\n                                 问题: {query}\n+                                文档内容: {emb_ans}\n                                 答案: \"\"\"\n     return prompt_template\n \n \n"
                },
                {
                    "date": 1732694927186,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,9 +133,10 @@\n                                 如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                 当前答案基于前几轮内容生成，请结合新文档内容对答案进行优化，使其更准确、更全面。\n                                 文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\n                                 问题: {query}\n-                                文档内容: {emb_ans}\n+                                当前答案: {answer}\n+                                新文档内容: {emb_ans}\n                                 答案: \"\"\"\n     return prompt_template\n \n \n"
                },
                {
                    "date": 1732694950475,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -327,9 +327,9 @@\n                 for i in range(max_context_num):\n                     if i==0:\n                         recursive_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n                     else:\n-                        recursive_qa_inputs = [recursive_qa_template(context[i], query, pre_answer) for context,pre_answer in zip(batch_context, recursive_qa_outputs)]\n+                        recursive_qa_inputs = [recursive_qa_template(context[i], pre_answer, query) for context,pre_answer in zip(batch_context, recursive_qa_outputs)]\n                         recursive_qa_outputs = llm.infer(recursive_qa_inputs)\n                 batch_qa_outputs += recursive_qa_outputs\n             \n             # batch_input.append(emb_bm25_merge_inputs)\n"
                },
                {
                    "date": 1732696501306,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -207,9 +207,8 @@\n     start = time.time()\n     \n     data =  parse_pdf(args.document_path)\n     \n-    dense_retriever_names = args.dense_retriever_lists.split(',')\n     retrievers = []\n     for embed_model in args.dense_retriever_lists.split(','):\n         retrievers.append(FaissRetriever(embed_model, data))\n     print(\"faissretriever load ok\")\n"
                },
                {
                    "date": 1732696511964,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -209,8 +209,9 @@\n     data =  parse_pdf(args.document_path)\n     \n     retrievers = []\n     for embed_model in args.dense_retriever_lists.split(','):\n+    \n         retrievers.append(FaissRetriever(embed_model, data))\n     print(\"faissretriever load ok\")\n     \n     # BM25召回\n"
                },
                {
                    "date": 1732696522455,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -168,14 +168,14 @@\n     dp =  DataProcess(pdf_path = file)\n     dp.ParseBlock(max_seq = 1024)\n     dp.ParseBlock(max_seq = 512)\n     print(len(dp.data))\n-    dp.ParseAllPage(max_seq = 256)\n-    dp.ParseAllPage(max_seq = 512)\n-    print(len(dp.data))\n-    dp.ParseOnePageWithRule(max_seq = 256)\n-    dp.ParseOnePageWithRule(max_seq = 512)\n-    print(len(dp.data))\n+    # dp.ParseAllPage(max_seq = 256)\n+    # dp.ParseAllPage(max_seq = 512)\n+    # print(len(dp.data))\n+    # dp.ParseOnePageWithRule(max_seq = 256)\n+    # dp.ParseOnePageWithRule(max_seq = 512)\n+    # print(len(dp.data))\n     data = dp.data\n     print(\"data load ok\")\n     return data\n     \n@@ -209,9 +209,8 @@\n     data =  parse_pdf(args.document_path)\n     \n     retrievers = []\n     for embed_model in args.dense_retriever_lists.split(','):\n-    \n         retrievers.append(FaissRetriever(embed_model, data))\n     print(\"faissretriever load ok\")\n     \n     # BM25召回\n"
                },
                {
                    "date": 1732696608427,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -189,9 +189,9 @@\n     parser.add_argument('--question_path', type=str, required=False, default='data/test_question.json', help='Question path')\n     parser.add_argument('--output_path', type=str, required=False, default='data/result1.json', help='Output answer path')\n \n     # model names\n-    parser.add_argument('--dense_retriever_lists', type=str, required=False, default='m3e,bge,gte,bce\"', help='Dense retrieve model lists')\n+    parser.add_argument('--dense_retriever_lists', type=str, required=False, default='m3e,bge,gte,bce', help='Dense retrieve model lists')\n     parser.add_argument('--reranker_name', type=str, required=False, default='bce', help='Rerank model name')\n     parser.add_argument('--llm_name', type=str, required=False, default='qwen', help='LLM model name')\n     \n     parser.add_argument('--recursive_answer', action='store_true', help='Rephrase selected document chunks.', default=False)\n"
                },
                {
                    "date": 1732697356233,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -245,9 +245,9 @@\n \n     \n \n     # 对每一条测试问题，做答案生成处理\n-    with open(args.question, \"r\" , encoding=\"utf-8\") as f:\n+    with open(args.question.path, \"r\" , encoding=\"utf-8\") as f:\n         jdata = json.loads(f.read())\n         print(len(jdata))\n         max_length = 4000\n         for idx, line in enumerate(jdata):\n"
                },
                {
                    "date": 1732698008756,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -245,9 +245,9 @@\n \n     \n \n     # 对每一条测试问题，做答案生成处理\n-    with open(args.question.path, \"r\" , encoding=\"utf-8\") as f:\n+    with open(args.question_path, \"r\" , encoding=\"utf-8\") as f:\n         jdata = json.loads(f.read())\n         print(len(jdata))\n         max_length = 4000\n         for idx, line in enumerate(jdata):\n"
                },
                {
                    "date": 1732699287504,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -54,9 +54,9 @@\n     rerank_ans = rerank.predict(query, docs)\n     rerank_ans = rerank_ans[:top_k]\n     # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n     emb_ans = []\n-    \n+    length = 0\n     for doc in rerank_ans:\n         if(len(emb_ans + doc.page_content) > max_length):\n             break\n         emb_ans.append(doc.page_content)\n"
                },
                {
                    "date": 1732699302381,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -59,8 +59,9 @@\n     for doc in rerank_ans:\n         if(len(emb_ans + doc.page_content) > max_length):\n             break\n         emb_ans.append(doc.page_content)\n+        length += len(doc.page_content)\n     return emb_ans         \n             \n     \n \n"
                },
                {
                    "date": 1732699310650,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -56,9 +56,9 @@\n     # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n     emb_ans = []\n     length = 0\n     for doc in rerank_ans:\n-        if(len(emb_ans + doc.page_content) > max_length):\n+        if(length + len(doc.page_content) > max_length):\n             break\n         emb_ans.append(doc.page_content)\n         length += len(doc.page_content)\n     return emb_ans         \n"
                },
                {
                    "date": 1732700656276,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -53,8 +53,9 @@\n def reRank(rerank, top_k, query, docs):\n     rerank_ans = rerank.predict(query, docs)\n     rerank_ans = rerank_ans[:top_k]\n     # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n+    max_length = 4000\n     emb_ans = []\n     length = 0\n     for doc in rerank_ans:\n         if(length + len(doc.page_content) > max_length):\n"
                },
                {
                    "date": 1732700681028,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -309,9 +309,9 @@\n             max_context_num = 0\n             for q in queries:            \n                 docs = retrievers_recall(retrievers, q)\n                 batch_context.append(reRank(rerank, args.rerank_topk, q, docs))\n-                max_context_num = max(max_context_num, len(batch_context))\n+                max_context_num = max(max_context_num, len(batch_context[-1]))\n                 # if args.rephrase_context:\n                 #     context = llm.infer([rephrase_context_template(''.join(context))])[0]\n                 # else:\n                 #     context = \"\".join(context)\n"
                },
                {
                    "date": 1732701117047,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -324,8 +324,9 @@\n             # 执行batch推理\n             batch_qa_outputs = llm.infer(batch_qa_inputs)\n             \n             if args.recursive_answer:\n+                print(max_context_num)\n                 for i in range(max_context_num):\n                     if i==0:\n                         recursive_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n                     else:\n"
                },
                {
                    "date": 1732701474324,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -330,9 +330,9 @@\n                     if i==0:\n                         recursive_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n                     else:\n                         recursive_qa_inputs = [recursive_qa_template(context[i], pre_answer, query) for context,pre_answer in zip(batch_context, recursive_qa_outputs)]\n-                        recursive_qa_outputs = llm.infer(recursive_qa_inputs)\n+                    recursive_qa_outputs = llm.infer(recursive_qa_inputs)\n                 batch_qa_outputs += recursive_qa_outputs\n             \n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732701485379,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -324,9 +324,8 @@\n             # 执行batch推理\n             batch_qa_outputs = llm.infer(batch_qa_inputs)\n             \n             if args.recursive_answer:\n-                print(max_context_num)\n                 for i in range(max_context_num):\n                     if i==0:\n                         recursive_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n                     else:\n"
                },
                {
                    "date": 1732875265815,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -170,14 +170,14 @@\n     dp =  DataProcess(pdf_path = file)\n     dp.ParseBlock(max_seq = 1024)\n     dp.ParseBlock(max_seq = 512)\n     print(len(dp.data))\n-    # dp.ParseAllPage(max_seq = 256)\n-    # dp.ParseAllPage(max_seq = 512)\n-    # print(len(dp.data))\n-    # dp.ParseOnePageWithRule(max_seq = 256)\n-    # dp.ParseOnePageWithRule(max_seq = 512)\n-    # print(len(dp.data))\n+    dp.ParseAllPage(max_seq = 256)\n+    dp.ParseAllPage(max_seq = 512)\n+    print(len(dp.data))\n+    dp.ParseOnePageWithRule(max_seq = 256)\n+    dp.ParseOnePageWithRule(max_seq = 512)\n+    print(len(dp.data))\n     data = dp.data\n     print(\"data load ok\")\n     return data\n     \n@@ -253,8 +253,9 @@\n         print(len(jdata))\n         max_length = 4000\n         for idx, line in enumerate(jdata):\n             query = line[\"question\"]\n+            print(query)\n \n             # faiss召回topk\n             # faiss_context = faissretriever.GetTopK(query, 15)\n             # faiss_min_score = 0.0\n@@ -303,8 +304,10 @@\n             if args.answer_before_retrieve:\n                 answer = llm.infer([answer_question_template(query)])[0]\n                 if answer.strip() != \"无答案\":\n                     queries.append(query+\" \"+answer)\n+                    \n+            print(queries)\n             \n             batch_context = []\n             max_context_num = 0\n             for q in queries:            \n"
                },
                {
                    "date": 1732875283074,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -304,11 +304,9 @@\n             if args.answer_before_retrieve:\n                 answer = llm.infer([answer_question_template(query)])[0]\n                 if answer.strip() != \"无答案\":\n                     queries.append(query+\" \"+answer)\n-                    \n-            print(queries)\n-            \n+                                \n             batch_context = []\n             max_context_num = 0\n             for q in queries:            \n                 docs = retrievers_recall(retrievers, q)\n"
                },
                {
                    "date": 1732876946140,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -331,8 +331,9 @@\n                         recursive_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n                     else:\n                         recursive_qa_inputs = [recursive_qa_template(context[i], pre_answer, query) for context,pre_answer in zip(batch_context, recursive_qa_outputs)]\n                     recursive_qa_outputs = llm.infer(recursive_qa_inputs)\n+                    print(recursive_qa_outputs)\n                 batch_qa_outputs += recursive_qa_outputs\n             \n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732876977956,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -338,9 +338,10 @@\n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n-            \n+            print(batch_qa_outputs)\n+            print('-----------------')\n             for i, qa_output in enumerate(batch_qa_outputs):\n                 line[f\"answer_{i+1}\"] = qa_output # 合并两路召回的结果\n             # line[\"answer_2\"] = batch_output[1] # bm召回的结果\n             # line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n"
                },
                {
                    "date": 1732879636070,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -253,9 +253,9 @@\n         print(len(jdata))\n         max_length = 4000\n         for idx, line in enumerate(jdata):\n             query = line[\"question\"]\n-            print(query)\n+            # print(query)\n \n             # faiss召回topk\n             # faiss_context = faissretriever.GetTopK(query, 15)\n             # faiss_min_score = 0.0\n@@ -331,17 +331,17 @@\n                         recursive_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n                     else:\n                         recursive_qa_inputs = [recursive_qa_template(context[i], pre_answer, query) for context,pre_answer in zip(batch_context, recursive_qa_outputs)]\n                     recursive_qa_outputs = llm.infer(recursive_qa_inputs)\n-                    print(recursive_qa_outputs)\n+                    # print(recursive_qa_outputs)\n                 batch_qa_outputs += recursive_qa_outputs\n             \n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n             # batch_input.append(emb_inputs)\n             # batch_input.append(rerank_inputs)\n-            print(batch_qa_outputs)\n-            print('-----------------')\n+            # print(batch_qa_outputs)\n+            # print('-----------------')\n             for i, qa_output in enumerate(batch_qa_outputs):\n                 line[f\"answer_{i+1}\"] = qa_output # 合并两路召回的结果\n             # line[\"answer_2\"] = batch_output[1] # bm召回的结果\n             # line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n"
                },
                {
                    "date": 1732936005567,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -98,10 +98,10 @@\n     return prompt_template\n \n def rephrase_question_template(question):\n \n-    prompt_template = f\"\"\"以下是用户的问题，请将其改写为一个更详细且具有更广泛检索可能性的问题，保持原始语义一致：\n-                        原问题：{question}\n+    prompt_template = f\"\"\"以下是用户的问题，请将其改写为一个更详细且具有更广泛检索可能性的问题，保持原始语义一致：\\n\n+                        原问题：{question} \\n\n                         改写后的问题：\"\"\"\n     return prompt_template\n \n def answer_question_template(question):\n@@ -130,15 +130,15 @@\n     return prompt_template\n \n def recursive_qa_template(emb_ans, answer, query):\n \n-    prompt_template = f\"\"\"基于当前生成的答案以及一个相关文档，简洁和专业的来回答用户的问题。\n-                                如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n-                                当前答案基于前几轮内容生成，请结合新文档内容对答案进行优化，使其更准确、更全面。\n-                                文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\n-                                问题: {query}\n-                                当前答案: {answer}\n-                                新文档内容: {emb_ans}\n+    prompt_template = f\"\"\"基于当前生成的答案以及一个相关文档，简洁和专业的来回答用户的问题。\\n\n+                                如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\\n\n+                                当前答案基于前几轮内容生成，请结合新文档内容对答案进行优化，使其更准确、更全面。\\n\n+                                文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\\n\n+                                问题: {query}\\n\n+                                当前答案: {answer}\\n\n+                                新文档内容: {emb_ans}\\n\n                                 答案: \"\"\"\n     return prompt_template\n \n \n"
                },
                {
                    "date": 1732936081551,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -131,15 +131,15 @@\n \n def recursive_qa_template(emb_ans, answer, query):\n \n     prompt_template = f\"\"\"基于当前生成的答案以及一个相关文档，简洁和专业的来回答用户的问题。\\n\n-                                如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\\n\n-                                当前答案基于前几轮内容生成，请结合新文档内容对答案进行优化，使其更准确、更全面。\\n\n-                                文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\\n\n-                                问题: {query}\\n\n-                                当前答案: {answer}\\n\n-                                新文档内容: {emb_ans}\\n\n-                                答案: \"\"\"\n+    如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\\n\n+    当前答案基于前几轮内容生成，请结合新文档内容对答案进行优化，使其更准确、更全面。\\n\n+    文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\\n\n+    问题: {query}\\n\n+    当前答案: {answer}\\n\n+    新文档内容: {emb_ans}\\n\n+    答案: \"\"\"\n     return prompt_template\n \n \n def question(text, llm, vector_store, prompt_template):\n"
                },
                {
                    "date": 1732936294232,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -99,10 +99,10 @@\n \n def rephrase_question_template(question):\n \n     prompt_template = f\"\"\"以下是用户的问题，请将其改写为一个更详细且具有更广泛检索可能性的问题，保持原始语义一致：\\n\n-                        原问题：{question} \\n\n-                        改写后的问题：\"\"\"\n+    原问题：{question} \\n\n+    改写后的问题：\"\"\"\n     return prompt_template\n \n def answer_question_template(question):\n \n@@ -121,24 +121,24 @@\n \n def qa_template(emb_ans, query):\n \n     prompt_template = f\"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n-                                如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n-                                已知文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\n-                                问题: {query}\n-                                文档内容: {emb_ans}\n-                                答案: \"\"\"\n+    如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n+    已知文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\n+    问题: {query}\n+    文档内容: {emb_ans}\n+    答案: \"\"\"\n     return prompt_template\n \n def recursive_qa_template(emb_ans, answer, query):\n \n-    prompt_template = f\"\"\"基于当前生成的答案以及一个相关文档，简洁和专业的来回答用户的问题。\\n\n-    如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\\n\n-    当前答案基于前几轮内容生成，请结合新文档内容对答案进行优化，使其更准确、更全面。\\n\n-    文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\\n\n-    问题: {query}\\n\n-    当前答案: {answer}\\n\n-    新文档内容: {emb_ans}\\n\n+    prompt_template = f\"\"\"基于当前生成的答案以及一个相关文档，简洁和专业的来回答用户的问题。\n+    如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n+    当前答案基于前几轮内容生成，请结合新文档内容对答案进行优化，使其更准确、更全面。\n+    文档内容为吉利控股集团汽车销售有限公司的吉利用户手册。\n+    问题: {query}\n+    当前答案: {answer}\n+    新文档内容: {emb_ans}\n     答案: \"\"\"\n     return prompt_template\n \n \n"
                },
                {
                    "date": 1732943030519,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -306,13 +306,11 @@\n                 if answer.strip() != \"无答案\":\n                     queries.append(query+\" \"+answer)\n                                 \n             batch_context = []\n-            max_context_num = 0\n             for q in queries:            \n                 docs = retrievers_recall(retrievers, q)\n                 batch_context.append(reRank(rerank, args.rerank_topk, q, docs))\n-                max_context_num = max(max_context_num, len(batch_context[-1]))\n                 # if args.rephrase_context:\n                 #     context = llm.infer([rephrase_context_template(''.join(context))])[0]\n                 # else:\n                 #     context = \"\".join(context)\n@@ -325,15 +323,17 @@\n             # 执行batch推理\n             batch_qa_outputs = llm.infer(batch_qa_inputs)\n             \n             if args.recursive_answer:\n-                for i in range(max_context_num):\n+                for i in range(len(batch_context[0])):\n                     if i==0:\n-                        recursive_qa_inputs = [qa_template(context[i], query) for context in batch_context]\n+                        recursive_qa_inputs = [qa_template(batch_context[0][i], query)]\n                     else:\n-                        recursive_qa_inputs = [recursive_qa_template(context[i], pre_answer, query) for context,pre_answer in zip(batch_context, recursive_qa_outputs)]\n+                        recursive_qa_inputs = [recursive_qa_template(batch_context[0][i], recursive_qa_outputs[0], query)]\n                     recursive_qa_outputs = llm.infer(recursive_qa_inputs)\n-                    # print(recursive_qa_outputs)\n+                    print(f\"round {i}\")\n+                    print(recursive_qa_inputs[0])\n+                    # print(recursive_qa_outputs[0])\n                 batch_qa_outputs += recursive_qa_outputs\n             \n             # batch_input.append(emb_bm25_merge_inputs)\n             # batch_input.append(bm25_inputs)\n"
                },
                {
                    "date": 1732943070666,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,9 @@\n #!/usr/bin/env python\n # coding: utf-8\n-\n+import os\n+os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\"\n+print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n import json\n import jieba\n import pandas as pd\n import numpy as np\n@@ -329,10 +331,8 @@\n                         recursive_qa_inputs = [qa_template(batch_context[0][i], query)]\n                     else:\n                         recursive_qa_inputs = [recursive_qa_template(batch_context[0][i], recursive_qa_outputs[0], query)]\n                     recursive_qa_outputs = llm.infer(recursive_qa_inputs)\n-                    print(f\"round {i}\")\n-                    print(recursive_qa_inputs[0])\n                     # print(recursive_qa_outputs[0])\n                 batch_qa_outputs += recursive_qa_outputs\n             \n             # batch_input.append(emb_bm25_merge_inputs)\n"
                }
            ],
            "date": 1732074092473,
            "name": "Commit-0",
            "content": "#!/usr/bin/env python\n# coding: utf-8\n\nimport json\nimport jieba\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom langchain.schema import Document\nfrom langchain.vectorstores import Chroma,FAISS\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain.chains import RetrievalQA\nimport time\nimport re\n\nfrom vllm_model import ChatLLM\nfrom rerank_model import reRankLLM\nfrom faiss_retriever import FaissRetriever\nfrom bm25_retriever import BM25\nfrom pdf_parse import DataProcess\n\n# 获取Langchain的工具链 \ndef get_qa_chain(llm, vector_store, prompt_template):\n\n    prompt = PromptTemplate(template=prompt_template,\n                            input_variables=[\"context\", \"question\"])\n\n    return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n\n# 构造提示，根据merged faiss和bm25的召回结果返回答案\ndef get_emb_bm25_merge(faiss_context, bm25_context, query):\n    max_length = 2500\n    emb_ans = \"\"\n    cnt = 0\n    for doc, score in faiss_context:\n        cnt =cnt + 1\n        if(cnt>6):\n            break\n        if(len(emb_ans + doc.page_content) > max_length):\n            break\n        emb_ans = emb_ans + doc.page_content\n    bm25_ans = \"\"\n    cnt = 0\n    for doc in bm25_context:\n        cnt = cnt + 1\n        if(len(bm25_ans + doc.page_content) > max_length):\n            break\n        bm25_ans = bm25_ans + doc.page_content\n        if(cnt > 6):\n            break\n\n    prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\"，不允许在答案中添加编造成分，答案请使用中文。\n                                已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                1: {emb_ans}\n                                2: {bm25_ans}\n                                问题:\n                                {question}\"\"\".format(emb_ans=emb_ans, bm25_ans = bm25_ans, question = query)\n    return prompt_template\n\n\ndef get_rerank(emb_ans, query):\n\n    prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                1: {emb_ans}\n                                问题:\n                                {question}\"\"\".format(emb_ans=emb_ans, question = query)\n    return prompt_template\n\n\ndef question(text, llm, vector_store, prompt_template):\n\n    chain = get_qa_chain(llm, vector_store, prompt_template)\n\n    response = chain({\"query\": text})\n    return response\n\ndef reRank(rerank, top_k, query, bm25_ans, faiss_ans):\n    items = []\n    max_length = 4000\n    for doc, score in faiss_ans:\n        items.append(doc)\n    items.extend(bm25_ans)\n    rerank_ans = rerank.predict(query, items)\n    rerank_ans = rerank_ans[:top_k]\n    # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n    emb_ans = \"\"\n    for doc in rerank_ans:\n        if(len(emb_ans + doc.page_content) > max_length):\n            break\n        emb_ans = emb_ans + doc.page_content\n    return emb_ans\n\nif __name__ == \"__main__\":\n\n    start = time.time()\n\n    base = \".\"\n    qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chatbase\"# + \"/pre_train_model/Qwen-7B-Chat\"\n    m3e =  base + \"/pre_train_model/m3e-large\"\n    bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n\n    # 解析pdf文档，构造数据\n    dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n    dp.ParseBlock(max_seq = 1024)\n    dp.ParseBlock(max_seq = 512)\n    print(len(dp.data))\n    dp.ParseAllPage(max_seq = 256)\n    dp.ParseAllPage(max_seq = 512)\n    print(len(dp.data))\n    dp.ParseOnePageWithRule(max_seq = 256)\n    dp.ParseOnePageWithRule(max_seq = 512)\n    print(len(dp.data))\n    data = dp.data\n    print(\"data load ok\")\n\n    # Faiss召回\n    faissretriever = FaissRetriever(m3e, data)\n    vector_store = faissretriever.vector_store\n    print(\"faissretriever load ok\")\n\n    # BM25召回\n    bm25 = BM25(data)\n    print(\"bm25 load ok\")\n\n    # LLM大模型\n    llm = ChatLLM(qwen7)\n    print(\"llm qwen load ok\")\n\n    # reRank模型\n    rerank = reRankLLM(bge_reranker_large)\n    print(\"rerank model load ok\")\n\n    # 对每一条测试问题，做答案生成处理\n    with open(base + \"/data/test_question.json\", \"r\") as f:\n        jdata = json.loads(f.read())\n        print(len(jdata))\n        max_length = 4000\n        for idx, line in enumerate(jdata):\n            query = line[\"question\"]\n\n            # faiss召回topk\n            faiss_context = faissretriever.GetTopK(query, 15)\n            faiss_min_score = 0.0\n            if(len(faiss_context) > 0):\n                faiss_min_score = faiss_context[0][1]\n            cnt = 0\n            emb_ans = \"\"\n            for doc, score in faiss_context:\n                cnt =cnt + 1\n                # 最长选择max length\n                if(len(emb_ans + doc.page_content) > max_length):\n                    break\n                emb_ans = emb_ans + doc.page_content\n                # 最多选择6个\n                if(cnt>6):\n                    break\n\n            # bm2.5召回topk\n            bm25_context = bm25.GetBM25TopK(query, 15)\n            bm25_ans = \"\"\n            cnt = 0\n            for doc in bm25_context:\n                cnt = cnt + 1\n                if(len(bm25_ans + doc.page_content) > max_length):\n                    break\n                bm25_ans = bm25_ans + doc.page_content\n                if(cnt > 6):\n                    break\n\n            # 构造合并bm25召回和向量召回的prompt\n            emb_bm25_merge_inputs = get_emb_bm25_merge(faiss_context, bm25_context, query)\n\n            # 构造bm25召回的prompt\n            bm25_inputs = get_rerank(bm25_ans, query)\n\n            # 构造向量召回的prompt\n            emb_inputs = get_rerank(emb_ans, query)\n\n            # rerank召回的候选，并按照相关性得分排序\n            rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n            # 构造得到rerank后生成答案的prompt\n            rerank_inputs = get_rerank(rerank_ans, query)\n\n            batch_input = []\n            batch_input.append(emb_bm25_merge_inputs)\n            batch_input.append(bm25_inputs)\n            batch_input.append(emb_inputs)\n            batch_input.append(rerank_inputs)\n            # 执行batch推理\n            batch_output = llm.infer(batch_input)\n            line[\"answer_1\"] = batch_output[0] # 合并两路召回的结果\n            line[\"answer_2\"] = batch_output[1] # bm召回的结果\n            line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n            line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n            line[\"answer_5\"] = emb_ans\n            line[\"answer_6\"] = bm25_ans\n            line[\"answer_7\"] = rerank_ans\n            # 如果faiss检索跟query的距离高于500，输出无答案\n            if(faiss_min_score >500):\n                line[\"answer_5\"] = \"无答案\"\n            else:\n                line[\"answer_5\"] = str(faiss_min_score)\n\n        # 保存结果，生成submission文件\n        json.dump(jdata, open(base + \"/data/result.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n        end = time.time()\n        print(\"cost time: \" + str(int(end-start)/60))\n"
        }
    ]
}
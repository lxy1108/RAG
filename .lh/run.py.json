{
    "sourceFile": "run.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 15,
            "patches": [
                {
                    "date": 1732074092473,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1732075567406,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -97,9 +97,9 @@\n \n     start = time.time()\n \n     base = \".\"\n-    qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chatbase\"# + \"/pre_train_model/Qwen-7B-Chat\"\n+    qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n \n     # 解析pdf文档，构造数据\n"
                },
                {
                    "date": 1732083713290,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,8 +18,9 @@\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n \n+os['CUDA_VISIBLE_DEVICES']=2,3\n # 获取Langchain的工具链 \n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n"
                },
                {
                    "date": 1732083809216,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,11 +17,10 @@\n from rerank_model import reRankLLM\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n-\n-os['CUDA_VISIBLE_DEVICES']=2,3\n-# 获取Langchain的工具链 \n+import os\n+os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"# 获取Langchain的工具链 \n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732083817768,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,9 +18,9 @@\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n-os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"# 获取Langchain的工具链 \n+os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732084635377,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,8 +19,9 @@\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n+os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732084652738,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -101,8 +101,12 @@\n     base = \".\"\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n+    \n+    # LLM大模型\n+    llm = ChatLLM(qwen7)\n+    print(\"llm qwen load ok\")\n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n     dp.ParseBlock(max_seq = 1024)\n@@ -125,11 +129,9 @@\n     # BM25召回\n     bm25 = BM25(data)\n     print(\"bm25 load ok\")\n \n-    # LLM大模型\n-    llm = ChatLLM(qwen7)\n-    print(\"llm qwen load ok\")\n+    \n \n     # reRank模型\n     rerank = reRankLLM(bge_reranker_large)\n     print(\"rerank model load ok\")\n"
                },
                {
                    "date": 1732084999903,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,8 +20,9 @@\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n+os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = =spawn\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732085006725,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = =spawn\n+os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732085016414,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n from pdf_parse import DataProcess\n import os\n os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-os.environ[\"export VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n+os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732085438283,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,11 +18,11 @@\n from faiss_retriever import FaissRetriever\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n-os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n-os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n+# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n+# os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n+# os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732086188229,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -20,9 +20,9 @@\n from pdf_parse import DataProcess\n import os\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n # os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n-# os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n+os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n                             input_variables=[\"context\", \"question\"])\n"
                },
                {
                    "date": 1732087573877,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,9 +19,9 @@\n from bm25_retriever import BM25\n from pdf_parse import DataProcess\n import os\n # os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"# 获取Langchain的工具链 \n-# os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n+os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = '1'\n os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n def get_qa_chain(llm, vector_store, prompt_template):\n \n     prompt = PromptTemplate(template=prompt_template,\n@@ -137,9 +137,9 @@\n     rerank = reRankLLM(bge_reranker_large)\n     print(\"rerank model load ok\")\n \n     # 对每一条测试问题，做答案生成处理\n-    with open(base + \"/data/test_question.json\", \"r\") as f:\n+    with open(base + \"/data/test_question.json\", \"r\" , encoding=\"utf-8\") as f:\n         jdata = json.loads(f.read())\n         print(len(jdata))\n         max_length = 4000\n         for idx, line in enumerate(jdata):\n"
                },
                {
                    "date": 1732089879571,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -103,11 +103,8 @@\n     qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chat\"# + \"/pre_train_model/Qwen-7B-Chat\"\n     m3e =  base + \"/pre_train_model/m3e-large\"\n     bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n     \n-    # LLM大模型\n-    llm = ChatLLM(qwen7)\n-    print(\"llm qwen load ok\")\n \n     # 解析pdf文档，构造数据\n     dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n     dp.ParseBlock(max_seq = 1024)\n@@ -130,9 +127,11 @@\n     # BM25召回\n     bm25 = BM25(data)\n     print(\"bm25 load ok\")\n \n-    \n+    # LLM大模型\n+    llm = ChatLLM(qwen7)\n+    print(\"llm qwen load ok\")\n \n     # reRank模型\n     rerank = reRankLLM(bge_reranker_large)\n     print(\"rerank model load ok\")\n"
                },
                {
                    "date": 1732157192892,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -51,9 +51,9 @@\n         if(cnt > 6):\n             break\n \n     prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n-                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\"，不允许在答案中添加编造成分，答案请使用中文。\n+                                如果无法从中得到答案，请说 \"无答案\"，不允许在答案中添加编造成分，答案请使用中文。\n                                 已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                 1: {emb_ans}\n                                 2: {bm25_ans}\n                                 问题:\n"
                },
                {
                    "date": 1732157561934,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -63,9 +63,9 @@\n \n def get_rerank(emb_ans, query):\n \n     prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n-                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n+                                如果无法从中得到答案，请说 \"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                 已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                 1: {emb_ans}\n                                 问题:\n                                 {question}\"\"\".format(emb_ans=emb_ans, question = query)\n"
                }
            ],
            "date": 1732074092473,
            "name": "Commit-0",
            "content": "#!/usr/bin/env python\n# coding: utf-8\n\nimport json\nimport jieba\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom langchain.schema import Document\nfrom langchain.vectorstores import Chroma,FAISS\nfrom langchain import PromptTemplate, LLMChain\nfrom langchain.chains import RetrievalQA\nimport time\nimport re\n\nfrom vllm_model import ChatLLM\nfrom rerank_model import reRankLLM\nfrom faiss_retriever import FaissRetriever\nfrom bm25_retriever import BM25\nfrom pdf_parse import DataProcess\n\n# 获取Langchain的工具链 \ndef get_qa_chain(llm, vector_store, prompt_template):\n\n    prompt = PromptTemplate(template=prompt_template,\n                            input_variables=[\"context\", \"question\"])\n\n    return RetrievalQA.from_llm(llm=llm, retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), prompt=prompt)\n\n# 构造提示，根据merged faiss和bm25的召回结果返回答案\ndef get_emb_bm25_merge(faiss_context, bm25_context, query):\n    max_length = 2500\n    emb_ans = \"\"\n    cnt = 0\n    for doc, score in faiss_context:\n        cnt =cnt + 1\n        if(cnt>6):\n            break\n        if(len(emb_ans + doc.page_content) > max_length):\n            break\n        emb_ans = emb_ans + doc.page_content\n    bm25_ans = \"\"\n    cnt = 0\n    for doc in bm25_context:\n        cnt = cnt + 1\n        if(len(bm25_ans + doc.page_content) > max_length):\n            break\n        bm25_ans = bm25_ans + doc.page_content\n        if(cnt > 6):\n            break\n\n    prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\"，不允许在答案中添加编造成分，答案请使用中文。\n                                已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                1: {emb_ans}\n                                2: {bm25_ans}\n                                问题:\n                                {question}\"\"\".format(emb_ans=emb_ans, bm25_ans = bm25_ans, question = query)\n    return prompt_template\n\n\ndef get_rerank(emb_ans, query):\n\n    prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n                                如果无法从中得到答案，请说 \"无答案\"或\"无答案\" ，不允许在答案中添加编造成分，答案请使用中文。\n                                已知内容为吉利控股集团汽车销售有限公司的吉利用户手册:\n                                1: {emb_ans}\n                                问题:\n                                {question}\"\"\".format(emb_ans=emb_ans, question = query)\n    return prompt_template\n\n\ndef question(text, llm, vector_store, prompt_template):\n\n    chain = get_qa_chain(llm, vector_store, prompt_template)\n\n    response = chain({\"query\": text})\n    return response\n\ndef reRank(rerank, top_k, query, bm25_ans, faiss_ans):\n    items = []\n    max_length = 4000\n    for doc, score in faiss_ans:\n        items.append(doc)\n    items.extend(bm25_ans)\n    rerank_ans = rerank.predict(query, items)\n    rerank_ans = rerank_ans[:top_k]\n    # docs_sort = sorted(rerank_ans, key = lambda x:x.metadata[\"id\"])\n    emb_ans = \"\"\n    for doc in rerank_ans:\n        if(len(emb_ans + doc.page_content) > max_length):\n            break\n        emb_ans = emb_ans + doc.page_content\n    return emb_ans\n\nif __name__ == \"__main__\":\n\n    start = time.time()\n\n    base = \".\"\n    qwen7 = \"/home/lixy/.cache/modelscope/hub/qwen/Qwen-7B-Chatbase\"# + \"/pre_train_model/Qwen-7B-Chat\"\n    m3e =  base + \"/pre_train_model/m3e-large\"\n    bge_reranker_large = base + \"/pre_train_model/bge-reranker-large\"\n\n    # 解析pdf文档，构造数据\n    dp =  DataProcess(pdf_path = base + \"/data/train_a.pdf\")\n    dp.ParseBlock(max_seq = 1024)\n    dp.ParseBlock(max_seq = 512)\n    print(len(dp.data))\n    dp.ParseAllPage(max_seq = 256)\n    dp.ParseAllPage(max_seq = 512)\n    print(len(dp.data))\n    dp.ParseOnePageWithRule(max_seq = 256)\n    dp.ParseOnePageWithRule(max_seq = 512)\n    print(len(dp.data))\n    data = dp.data\n    print(\"data load ok\")\n\n    # Faiss召回\n    faissretriever = FaissRetriever(m3e, data)\n    vector_store = faissretriever.vector_store\n    print(\"faissretriever load ok\")\n\n    # BM25召回\n    bm25 = BM25(data)\n    print(\"bm25 load ok\")\n\n    # LLM大模型\n    llm = ChatLLM(qwen7)\n    print(\"llm qwen load ok\")\n\n    # reRank模型\n    rerank = reRankLLM(bge_reranker_large)\n    print(\"rerank model load ok\")\n\n    # 对每一条测试问题，做答案生成处理\n    with open(base + \"/data/test_question.json\", \"r\") as f:\n        jdata = json.loads(f.read())\n        print(len(jdata))\n        max_length = 4000\n        for idx, line in enumerate(jdata):\n            query = line[\"question\"]\n\n            # faiss召回topk\n            faiss_context = faissretriever.GetTopK(query, 15)\n            faiss_min_score = 0.0\n            if(len(faiss_context) > 0):\n                faiss_min_score = faiss_context[0][1]\n            cnt = 0\n            emb_ans = \"\"\n            for doc, score in faiss_context:\n                cnt =cnt + 1\n                # 最长选择max length\n                if(len(emb_ans + doc.page_content) > max_length):\n                    break\n                emb_ans = emb_ans + doc.page_content\n                # 最多选择6个\n                if(cnt>6):\n                    break\n\n            # bm2.5召回topk\n            bm25_context = bm25.GetBM25TopK(query, 15)\n            bm25_ans = \"\"\n            cnt = 0\n            for doc in bm25_context:\n                cnt = cnt + 1\n                if(len(bm25_ans + doc.page_content) > max_length):\n                    break\n                bm25_ans = bm25_ans + doc.page_content\n                if(cnt > 6):\n                    break\n\n            # 构造合并bm25召回和向量召回的prompt\n            emb_bm25_merge_inputs = get_emb_bm25_merge(faiss_context, bm25_context, query)\n\n            # 构造bm25召回的prompt\n            bm25_inputs = get_rerank(bm25_ans, query)\n\n            # 构造向量召回的prompt\n            emb_inputs = get_rerank(emb_ans, query)\n\n            # rerank召回的候选，并按照相关性得分排序\n            rerank_ans = reRank(rerank, 6, query, bm25_context, faiss_context)\n            # 构造得到rerank后生成答案的prompt\n            rerank_inputs = get_rerank(rerank_ans, query)\n\n            batch_input = []\n            batch_input.append(emb_bm25_merge_inputs)\n            batch_input.append(bm25_inputs)\n            batch_input.append(emb_inputs)\n            batch_input.append(rerank_inputs)\n            # 执行batch推理\n            batch_output = llm.infer(batch_input)\n            line[\"answer_1\"] = batch_output[0] # 合并两路召回的结果\n            line[\"answer_2\"] = batch_output[1] # bm召回的结果\n            line[\"answer_3\"] = batch_output[2] # 向量召回的结果\n            line[\"answer_4\"] = batch_output[3] # 多路召回重排序后的结果\n            line[\"answer_5\"] = emb_ans\n            line[\"answer_6\"] = bm25_ans\n            line[\"answer_7\"] = rerank_ans\n            # 如果faiss检索跟query的距离高于500，输出无答案\n            if(faiss_min_score >500):\n                line[\"answer_5\"] = \"无答案\"\n            else:\n                line[\"answer_5\"] = str(faiss_min_score)\n\n        # 保存结果，生成submission文件\n        json.dump(jdata, open(base + \"/data/result.json\", \"w\", encoding='utf-8'), ensure_ascii=False, indent=2)\n        end = time.time()\n        print(\"cost time: \" + str(int(end-start)/60))\n"
        }
    ]
}